# app.py - Smart IT ì‹ ë¢°ë„ í‰ê°€ Flask API
# --------------------------------------------------------------------------------------------
# Flask ì›¹ APIë¡œ ì‹ ë¢°ë„ í‰ê°€ ê¸°ëŠ¥ ì œê³µ
# ì‚¬ìš©ë²•: python app.py
# API ì—”ë“œí¬ì¸íŠ¸: POST /evaluate
# --------------------------------------------------------------------------------------------

import json
import sys
import traceback
import math
from datetime import datetime, timezone
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging

# ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸
from Veriscope_url import (
    evaluate_url, load_index, configure_http, 
    setup_logging, SESSION, logger
)

app = Flask(__name__)
CORS(app)  # CORS í—ˆìš©

# ë¡œê¹… ì„¤ì •
setup_logging(verbose=False, quiet=True, log_file="api.log")

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ HTTP ì„¸ì…˜ ì„¤ì •
configure_http(http_pool=64, timeout=12)

@app.route('/', methods=['GET'])
def home():
    """API ìƒíƒœ í™•ì¸"""
    return jsonify({
        "status": "ok",
        "service": "Smart IT ì‹ ë¢°ë„ í‰ê°€ API",
        "version": "1.0",
        "timestamp": datetime.now().isoformat(),
        "endpoints": {
            "evaluate": {
                "method": "POST",
                "url": "/evaluate",
                "description": "ë‰´ìŠ¤ ê¸°ì‚¬ ì‹ ë¢°ë„ í‰ê°€"
            },
            "health": {
                "method": "GET", 
                "url": "/health",
                "description": "ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"
            }
        }
    })

@app.route('/health', methods=['GET'])
def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì¸ë±ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
        pack = load_index()
        index_size = pack.matrix.shape[0] if pack else 0
        
        return jsonify({
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "index_loaded": True,
            "index_size": index_size,
            "session_configured": SESSION is not None
        })
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "timestamp": datetime.now().isoformat(),
            "error": str(e),
            "index_loaded": False
        }), 500

@app.route('/evaluate', methods=['POST'])
def evaluate_news():
    """ë‰´ìŠ¤ ê¸°ì‚¬ ì‹ ë¢°ë„ í‰ê°€ API"""
    try:
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = request.get_json()
        
        if not data:
            return jsonify({
                "success": False,
                "error": "JSON ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            }), 400
            
        url = data.get('url')
        if not url:
            return jsonify({
                "success": False,
                "error": "URLì´ í•„ìš”í•©ë‹ˆë‹¤."
            }), 400
            
        # ì˜µì…˜ íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’ ì ìš©)
        nli_batch = data.get('nli_batch', 128)
        use_gpu = data.get('use_gpu', True)
        fp16 = data.get('fp16', True)
        similarity_threshold = data.get('similarity_threshold', 0.6)
        
        logger.info(f"API ìš”ì²­: {url}")
        
        # ì‹ ë¢°ë„ í‰ê°€ ì‹¤í–‰
        result = evaluate_url_api(
            query_url=url,
            nli_batch=nli_batch,
            use_gpu=use_gpu,
            fp16=fp16,
            similarity_threshold=similarity_threshold
        )
        
        return jsonify({
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "url": url,
            "result": result
        })
        
    except Exception as e:
        logger.error(f"API ì˜¤ë¥˜: {str(e)}")
        logger.error(traceback.format_exc())
        
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

def evaluate_url_api(query_url: str, nli_batch: int, use_gpu: bool, fp16: bool, similarity_threshold: float) -> dict:
    """
    APIìš© ì‹ ë¢°ë„ í‰ê°€ í•¨ìˆ˜ - ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜
    """
    from Veriscope_url import (
        domain_of, polite_get, extract_text, make_chunks, 
        get_embedder, load_index, MIN_TEXT_LEN, get_nli,
        summarize_for_nli, TOPK_CANDIDATES, check_keyword_relevance,
        add_url_to_index, save_index, time_weight, source_reputation,
        korean_ratio, FAKE_NEWS_PATTERNS, TOPN_RETURN
    )
    import torch
    from sentence_transformers import util
    import numpy as np
    from collections import defaultdict
    
    if SESSION is None:
        configure_http(http_pool=64, timeout=12)

    pack = load_index()
    embedder, _ = get_embedder(use_gpu=use_gpu, fp16=fp16)

    # URL íŒŒì‹± ë° ì½˜í…ì¸  ì¶”ì¶œ
    host = domain_of(query_url)
    html = None
    if host.endswith("n.news.naver.com") or host.endswith("news.naver.com"):
        html = polite_get(query_url, mobile=True) or polite_get(query_url)
    else:
        html = polite_get(query_url) or polite_get(query_url, mobile=True)

    q_text, q_dt, q_title = extract_text(query_url, html)
    if len(q_text) < 50:
        raise ValueError("ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ")

    # ì‚¬ìš©ì URLì„ ì¸ë±ìŠ¤ì— ì¶”ê°€ (ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš°)
    try:
        if add_url_to_index(query_url, q_text, q_dt, q_title, embedder, pack):
            save_index(pack)
            logger.info("ì‚¬ìš©ì URLì´ ì¸ë±ìŠ¤ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.warning(f"URL ì¸ë±ìŠ¤ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")

    q_chunks = make_chunks(q_text, min_len=max(120, MIN_TEXT_LEN // 2))
    if not q_chunks:
        q_chunks = [q_text]

    q_vecs = embedder.encode(q_chunks, convert_to_numpy=True, normalize_embeddings=True)
    sims = util.cos_sim(torch.tensor(q_vecs), torch.from_numpy(pack.matrix)).cpu().numpy()
    sim_per_idx = sims.max(axis=0)

    # í›„ë³´ ì„ íƒ
    K = min(TOPK_CANDIDATES, pack.matrix.shape[0])
    cand_idx = np.argsort(-sim_per_idx)[:K].tolist()

    tok, mdl, use_fp16 = get_nli(use_gpu=use_gpu, fp16=fp16)
    q_premise = summarize_for_nli(q_text, max_sents=3)

    # ì¦ê±° í‰ê°€
    evidence_list = []
    seen = set()
    url_groups = defaultdict(list)
    
    for idx in cand_idx:
        rec = pack.records[idx]
        sim_score = float(sim_per_idx[idx])
        
        if sim_score < similarity_threshold:
            continue
            
        # í‚¤ì›Œë“œ ê´€ë ¨ì„± ê²€ì¦
        relevance_info = check_keyword_relevance(q_text, rec.chunk)
        if relevance_info['keyword_match_ratio'] == 0.0:
            continue
            
        # NLI ì ìˆ˜ ê³„ì‚°
        if rec.chunk and len(rec.chunk.strip()) > 10:
            hyp = rec.chunk[:500]
            inputs = tok(q_premise, hyp, return_tensors="pt", truncation=True, max_length=512)
            if use_gpu and torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            with torch.no_grad():
                logits = mdl(**inputs).logits
                if use_fp16:
                    logits = logits.float()
                probs = torch.softmax(logits, dim=-1)
                entail_prob = float(probs[0][0])  # ENTAILMENT
        else:
            entail_prob = 0.0
            
        # ê´€ë ¨ì„± ì¡°ì •
        if not relevance_info['has_good_relevance']:
            if relevance_info['keyword_match_ratio'] < 0.15:
                sim_score *= 0.5
            else:
                sim_score *= 0.7
                
        # ì‹œê°„/ì¶œì²˜ ê°€ì¤‘ì¹˜
        dt = datetime.fromtimestamp(rec.published, tz=timezone.utc) if rec.published else None
        time_v = time_weight(dt)
        src_v = source_reputation(rec.url, rec.from_seed)
        
        # ì–¸ì–´ ì •í•© ê°€ì¤‘ì¹˜
        kr_ratio_q = korean_ratio(q_text)
        kr_ratio_r = korean_ratio(rec.chunk)
        if kr_ratio_q >= 0.7 and kr_ratio_r >= 0.7:
            lang_bonus = 1.1
        elif kr_ratio_q >= 0.3 and kr_ratio_r >= 0.3:
            lang_bonus = 1.05
        else:
            lang_bonus = 1.0
            
        final_score = sim_score * entail_prob * time_v * src_v * lang_bonus
        
        if final_score >= 0.3:  # ìµœì¢… ì ìˆ˜ ì„ê³„ê°’
            evidence_list.append((idx, final_score, {
                'url': rec.url,
                'similarity': sim_score,
                'nli_support': entail_prob,
                'time_weight': time_v,
                'source_weight': src_v,
                'final_score': final_score,
                'title': rec.title,
                'published': rec.published
            }))
    
    # Top ì¦ê±° ì„ íƒ ë° ì¤‘ë³µ ì œê±°
    evidence_list.sort(key=lambda x: x[1], reverse=True)
    
    uniq_top = []
    for idx, score, meta in evidence_list:
        canonical_u = meta['url'].lower().rstrip('/')
        if canonical_u in seen:
            continue
            
        domain = domain_of(meta['url'])
        url_groups[domain].append((idx, score, meta))
        uniq_top.append((idx, score, meta))
        seen.add(canonical_u)
        
        if len(uniq_top) >= TOPN_RETURN:
            break
    
    if not uniq_top:
        return {
            "reliability_score": 0,
            "reliability_level": "ë§¤ìš° ë‚®ìŒ",
            "status": "ì—°ê´€ì„± ë†’ì€ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í•¨",
            "evidence": [],
            "factors": {
                "content_consistency": 0,
                "source_diversity": 0,
                "temporal_relevance": 0,
                "evidence_quality": 0
            },
            "recommendation": "í—ˆìœ„ì •ë³´ í˜¹ì€ ì˜¤ë³´ ì˜ì‹¬. ê³µì‹ ì¶œì²˜ë¥¼ í†µí•´ ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
        }
    
    # ì‹ ë¢°ë„ ê³„ì‚°
    total_score = sum(s for _, s, _ in uniq_top)
    base_trust_prob = 1 / (1 + math.exp(-total_score))
    
    # ë‹¤ì–‘ì„± ê³„ì‚°
    unique_domains = len(url_groups)
    diversity_factor = min(unique_domains / 3.0, 1.0)
    
    # ì‹œê°„ì  ê´€ë ¨ì„±
    if q_dt:
        year_diff = datetime.now().year - q_dt.year
        if year_diff >= 10:
            temporal_relevance = 0.1
        elif year_diff >= 5:
            temporal_relevance = 0.3
        else:
            temporal_relevance = 0.9
    else:
        temporal_relevance = 0.5
        
    # ì¦ê±° í’ˆì§ˆ
    avg_nli = sum(meta['nli_support'] for _, _, meta in uniq_top) / len(uniq_top)
    evidence_quality = avg_nli
    
    # ìµœì¢… ì‹ ë¢°ë„ (ê°€ì¤‘í‰ê· )
    reliability_factors = {
        'content_consistency': base_trust_prob,
        'source_diversity': diversity_factor, 
        'temporal_relevance': temporal_relevance,
        'evidence_quality': evidence_quality
    }
    
    weights = {'content_consistency': 0.4, 'source_diversity': 0.25, 'temporal_relevance': 0.2, 'evidence_quality': 0.15}
    final_reliability = sum(reliability_factors[k] * weights[k] for k in weights)
    final_reliability_pct = int(final_reliability * 100)
    
    # ì‹ ë¢°ë„ ë ˆë²¨ ê²°ì •
    if final_reliability_pct >= 80:
        level = "ë§¤ìš° ë†’ìŒ"
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ì‹ ë¢°í•  ë§Œí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì¶œì²˜ì—ì„œ ì¼ê´€ëœ ì •ë³´ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤."
    elif final_reliability_pct >= 65:
        level = "ë†’ìŒ"
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ëŒ€ì²´ë¡œ ì‹ ë¢°í•  ë§Œí•˜ì§€ë§Œ, ì¶”ê°€ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    elif final_reliability_pct >= 50:
        level = "ë³´í†µ"
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ì‹ ì¤‘í•˜ê²Œ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì¶œì²˜ì™€ êµì°¨ í™•ì¸í•˜ì„¸ìš”."
    elif final_reliability_pct >= 35:
        level = "ë‚®ìŒ"
        recommendation = "ì´ ê¸°ì‚¬ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì •ë¶€ ê³µì‹ ë°œí‘œë‚˜ ê¶Œìœ„ ìˆëŠ” ì¶œì²˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."
    else:
        level = "ë§¤ìš° ë‚®ìŒ"
        recommendation = "í—ˆìœ„ì •ë³´ í˜¹ì€ ì˜¤ë³´ ì˜ì‹¬. ê³µì‹ ì¶œì²˜ë¥¼ í†µí•´ ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    # ì¦ê±° ëª©ë¡ êµ¬ì„±
    evidence_data = []
    for idx, score, meta in uniq_top:
        evidence_data.append({
            "url": meta['url'],
            "title": meta['title'],
            "reliability_score": int(score * 100),
            "similarity": round(meta['similarity'], 3),
            "nli_support": round(meta['nli_support'], 3),
            "published": datetime.fromtimestamp(meta['published']).isoformat() if meta['published'] else None
        })
    
    return {
        "reliability_score": final_reliability_pct,
        "reliability_level": level,
        "status": "í‰ê°€ ì™„ë£Œ",
        "evidence": evidence_data,
        "factors": {
            "content_consistency": int(reliability_factors['content_consistency'] * 100),
            "source_diversity": int(reliability_factors['source_diversity'] * 100), 
            "temporal_relevance": int(reliability_factors['temporal_relevance'] * 100),
            "evidence_quality": int(reliability_factors['evidence_quality'] * 100)
        },
        "recommendation": recommendation,
        "article_info": {
            "title": q_title,
            "published": q_dt.isoformat() if q_dt else None,
            "domain": domain_of(query_url)
        }
    }

if __name__ == '__main__':
    print("ğŸš€ Smart IT ì‹ ë¢°ë„ í‰ê°€ API ì„œë²„ ì‹œì‘...")
    print("ğŸ“¡ ì—”ë“œí¬ì¸íŠ¸:")
    print("  - GET  /        : API ì •ë³´")
    print("  - GET  /health  : í—¬ìŠ¤ì²´í¬") 
    print("  - POST /evaluate: ì‹ ë¢°ë„ í‰ê°€")
    print()
    print("ğŸ“– ì‚¬ìš© ì˜ˆì‹œ:")
    print('curl -X POST http://localhost:5000/evaluate \\')
    print('  -H "Content-Type: application/json" \\')
    print('  -d \'{"url": "https://news.example.com/article/123"}\'')
    print()
    
    # ê°œë°œ ì„œë²„ ì‹¤í–‰
    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)