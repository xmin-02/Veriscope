# test_sec.py
# --------------------------------------------------------------------------------------------
# ì‹œë“œ í¬ë¡¤(ë³‘ë ¬) â†’ ì„ë² ë”© ì¸ë±ìŠ¤(pkl) â†’ í‰ê°€ ì‹œ NLI ì¬ë­í¬(ë°°ì¹˜)
# ì¶œë ¥: Top-5 ê·¼ê±°(ê° %) + ìµœì¢… ì‹ ë¢°ë„ %
# - ë„¤ì´ë²„/ JTBC ì „ìš© ë³¸ë¬¸ ì¶”ì¶œê¸° ì¶”ê°€
# - AMP ì„œë¸Œë„ë©”ì¸ ì˜ëª» ì‹œë„ ì œê±° (amp.news.*)
# - í›„ë³´ TopK ìƒí–¥(500) + í•œêµ­ì–´ ë¹„ìœ¨ ê°€ì¤‘ì¹˜ + ì¦ê±° ì¤‘ë³µ ì œê±°
# --------------------------------------------------------------------------------------------
# --------------------------------------------------------------------------------------------
# ğŸ§ª í…ŒìŠ¤íŠ¸ìš© ë¹ ë¥¸ ë¹Œë“œ (3ê°œ ì‹œë“œ, 1-2ë¶„ ì†Œìš”) - í•˜ë“œì›¨ì–´ ìµœëŒ€ í™œìš©:
# & C:\Smart_IT\.venv\Scripts\python.exe C:\Smart_IT\test_sec.py build-index --test-mode --use-gpu --fp16 --fast-extract
#
# ï¿½ ì „ì²´ ì¸ë±ìŠ¤ ë¹Œë“œ (238ê°œ ì‹œë“œ, 15-30ë¶„ ì†Œìš”) - ìµœê³  ì„±ëŠ¥ ëª¨ë“œ:
# & C:\Smart_IT\.venv\Scripts\python.exe C:\Smart_IT\test_sec.py build-index --use-gpu --fp16 --fast-extract
#
# ğŸ” ì‹ ë¢°ë„ í‰ê°€ ì‹¤í–‰:
# & C:\Smart_IT\.venv\Scripts\python.exe C:\Smart_IT\test_sec.py evaluate --url "ê¸°ì‚¬URL" --use-gpu --fp16
#
# ğŸ’» í•˜ë“œì›¨ì–´ ìµœì í™”: Intel Ultra9 285k (32ìŠ¤ë ˆë“œ) + RTX3070ti (8GB) + 128GB RAM ìµœëŒ€ í™œìš©
# --------------------------------------------------------------------------------------------

import os
import re
import csv
import sys
import math
import time
import pickle
import queue
import argparse
import urllib.parse as up
import urllib.parse
import logging
import multiprocessing as mp
from datetime import datetime, timezone
from dataclasses import dataclass
from typing import List, Tuple, Optional, Callable
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from threading import Lock

# í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ì„¤ì • (Windows)
if sys.platform == "win32":
    try:
        import psutil
        # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ë¥¼ ë†’ìŒìœ¼ë¡œ ì„¤ì •
        current_process = psutil.Process()
        current_process.nice(psutil.HIGH_PRIORITY_CLASS)
    except (ImportError, Exception):
        pass

# NPU/OpenVINO ì§€ì› (ì„ íƒì )
NPU_AVAILABLE = False
try:
    import openvino as ov
    NPU_AVAILABLE = True
except ImportError:
    pass

# --- third-party ---
import json
import numpy as np
import requests
from bs4 import BeautifulSoup
import trafilatura
from newspaper import Article
from tqdm import tqdm

# ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    from PIL import Image
    import pytesseract
    import easyocr
    IMAGE_OCR_AVAILABLE = True
except ImportError:
    IMAGE_OCR_AVAILABLE = False

# BeautifulSoup ê²½ê³  ì–µì œ
import warnings
from bs4 import MarkupResemblesLocatorWarning
warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)

import torch
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModelForSequenceClassification

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from readability import Document
import json

# --------------------------------------------------------------------------------------------
# ê³ ì • ê²½ë¡œ
SEED_CSV  = r"C:\Smart_IT\enhanced_seed_links.csv"  # ê°œì„ ëœ ì‹œë“œ ë§í¬ ì‚¬ìš©
INDEX_PKL = r"C:\Smart_IT\smart_it_index.pkl"

# ê¸°ë³¸ ì •ì±…
MAX_DEPTH = 2
MAX_PAGES_PER_DOMAIN = 150      # ë„ë©”ì¸ë‹¹ ìµœëŒ€ í˜ì´ì§€(í’ˆì§ˆ/ì‹œê°„ íŠ¸ë ˆì´ë“œì˜¤í”„)
REQUEST_TIMEOUT = 12
CRAWL_SLEEP = 0.5

# ê²€ìƒ‰/ìŠ¤ì½”ì–´ ì •ì±…
TOPK_CANDIDATES = 500           # â† ì¤‘ìš”: 0 ì´ë©´ NLIê°€ ë¹„ì–´ë²„ë¦¼
TOPN_RETURN = 10  # ë” ë§ì€ ê·¼ê±° ìë£Œ í‘œì‹œ
MIN_TEXT_LEN = 200              # ìš´ì˜ìš© ê¶Œì¥ê°’(ë””ë²„ê¹… ì‹œ ë‚®ì¶°ë„ ë¨)
MIN_IMAGE_TEXT_LEN = 10         # ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìµœì†Œ ê¸¸ì´ (ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ í—ˆìš©)
MIN_SIMILARITY_THRESHOLD = 0.35  # ìµœì†Œ ìœ ì‚¬ì„± ì„ê³„ê°’ (í’ˆì§ˆ ê°œì„ : 0.15 â†’ 0.35)
MIN_NLI_SUPPORT_THRESHOLD = 0.1  # ìµœì†Œ NLI ì§€ì§€ë„ ì„ê³„ê°’
MIN_FINAL_SCORE = 0.3           # ìµœì¢… ì ìˆ˜ ìµœì†Œ ì„ê³„ê°’

# ìŠ¤ì½”ì–´ ê°€ì¤‘ì¹˜ (ì¡°ì •ë¨)
ALPHA_SIM = 0.65      # ìœ ì‚¬ì„± ê°€ì¤‘ì¹˜ (ë†’ì„)
ALPHA_NLI = 0.35      # NLI ê°€ì¤‘ì¹˜ (BETA_SUPê³¼ ë™ì¼í•œ ì—­í• )
BETA_SUP  = 0.35      # NLI ì§€ì§€ë„ ê°€ì¤‘ì¹˜ (ë†’ì„)  
GAMMA_CONTRA = 0.50   # NLI ë°˜ë°• ê°€ì¤‘ì¹˜ (ë†’ì„)
DELTA_TIME = 0.20     # ì‹œê°„ ê°€ì¤‘ì¹˜ (0.10 â†’ 0.20ìœ¼ë¡œ ê°•í™”)
EPS_SOURCE = 0.20     # ì¶œì²˜ ì‹ ë¢°ì„± ê°€ì¤‘ì¹˜ (ë†’ì„)
EPS_LANG   = 0.15     # í•œêµ­ì–´/ì˜ì–´ ë“± ì§ˆì˜-ë¬¸ì„œ ì–¸ì–´ ì •í•© ê°€ì¤‘ (ë†’ì„)
TIME_LAMBDA = 0.0025

# ë¡œê¹…
logger = logging.getLogger("smart_it")

def setup_logging(verbose: bool, quiet: bool, log_file: Optional[str], build_mode: bool = False):
    if build_mode and not verbose:
        # ë¹Œë“œ ëª¨ë“œì—ì„œëŠ” ì§„í–‰ë„ ê°€ì‹œì„±ì„ ìœ„í•´ ë¡œê·¸ ìµœì†Œí™”
        level = logging.ERROR
    else:
        level = logging.INFO
        if verbose:
            level = logging.DEBUG
        if quiet:
            level = logging.WARNING
    
    handlers = []
    if log_file:
        # íŒŒì¼ë¡œë§Œ ë¡œê·¸ ì¶œë ¥ (ì§„í–‰ë„ í‘œì‹œ ë°©í•´ ë°©ì§€)
        handlers.append(logging.FileHandler(log_file, encoding="utf-8"))
    elif not build_mode or verbose:
        # ë¹Œë“œ ëª¨ë“œê°€ ì•„ë‹ˆê±°ë‚˜ verbose ëª¨ë“œì¼ ë•Œë§Œ ì½˜ì†” ì¶œë ¥
        handlers.append(logging.StreamHandler(sys.stderr))  # stderrë¡œ ë³€ê²½í•˜ì—¬ ì§„í–‰ë„ì™€ ë¶„ë¦¬
    
    if handlers:
        logging.basicConfig(level=level, format="%(asctime)s [%(levelname)s] %(message)s", handlers=handlers)
    else:
        # ë¡œê·¸ í•¸ë“¤ëŸ¬ê°€ ì—†ìœ¼ë©´ NullHandler ì‚¬ìš©
        logging.basicConfig(level=logging.CRITICAL, handlers=[logging.NullHandler()])
    
    if verbose or not build_mode:
        logger.debug("ë¡œê¹… ì´ˆê¸°í™”(level=%s, log_file=%s)", logging.getLevelName(level), log_file)

# --------------------------------------------------------------------------------------------
# HTTP ì„¸ì…˜(ì»¤ë„¥ì…˜ í’€/ì¬ì‹œë„)
SESSION = None
def configure_http(http_pool: int, timeout: int):
    global SESSION, REQUEST_TIMEOUT
    REQUEST_TIMEOUT = timeout
    sess = requests.Session()
    adapter = HTTPAdapter(
        pool_connections=http_pool,
        pool_maxsize=http_pool,
        max_retries=Retry(total=2, backoff_factor=0.2, status_forcelist=[429, 500, 502, 503, 504]),
    )
    sess.mount("http://", adapter)
    sess.mount("https://", adapter)
    SESSION = sess
    logger.info("HTTP ì„¤ì •: pool=%d, timeout=%ds", http_pool, timeout)

def polite_get(url: str, mobile: bool = False) -> Optional[str]:
    try:
        # ë¡œì»¬ íŒŒì¼ ì§€ì›
        if url.startswith('file://'):
            file_path = url.replace('file://', '').replace('/', '\\')
            if file_path.startswith('\\C:'):
                file_path = 'C:' + file_path[3:]
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        
        ua_desktop = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36"
        ua_mobile  = "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Mobile Safari/537.36"
        h = {
            "User-Agent": (ua_mobile if mobile else ua_desktop),
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Referer": url,
        }
        r = SESSION.get(url, headers=h, timeout=REQUEST_TIMEOUT)
        if r.status_code == 200 and "text/html" in r.headers.get("Content-Type", ""):
            return r.text
        logger.debug("GET %s -> %s (%s)", url, r.status_code, r.headers.get("Content-Type"))
    except Exception as e:
        logger.debug("GET ì‹¤íŒ¨ %s (%s)", url, e)
    return None

def resolve_shortened_url(url: str) -> str:
    """ë‹¨ì¶• URLì„ ì‹¤ì œ URLë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ì•Œë ¤ì§„ ë‹¨ì¶• URL ë„ë©”ì¸ë“¤
        shorteners = [
            'naver.me', 'bit.ly', 'tinyurl.com', 'goo.gl', 't.co', 
            'short.link', 'ow.ly', 'is.gd', 'buff.ly', 'cutt.ly',
            'han.gl', 'me2.do', 'vo.la', 'zrr.kr', 'han.gl'
        ]
        
        # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
        parsed = up.urlparse(url)
        domain = parsed.netloc.lower()
        
        # ë‹¨ì¶• URLì¸ì§€ í™•ì¸
        is_shortened = any(domain == shortener or domain.endswith('.' + shortener) 
                          for shortener in shorteners)
        
        if not is_shortened:
            return url
            
        logger.info("ë‹¨ì¶• URL ë°œê²¬, ì›ë³¸ URLë¡œ ë³€í™˜ ì‹œë„: %s", url)
        
        # HEAD ìš”ì²­ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë”°ë¼ê°€ê¸°
        response = SESSION.head(url, allow_redirects=True, timeout=REQUEST_TIMEOUT)
        final_url = response.url
        
        if final_url != url:
            logger.info("URL ë³€í™˜ ì„±ê³µ: %s -> %s", url, final_url)
            return final_url
        else:
            # HEADê°€ ì‹¤íŒ¨í•˜ë©´ GETìœ¼ë¡œ ì‹œë„
            response = SESSION.get(url, allow_redirects=True, timeout=REQUEST_TIMEOUT)
            final_url = response.url
            if final_url != url:
                logger.info("URL ë³€í™˜ ì„±ê³µ (GET): %s -> %s", url, final_url)
                return final_url
                
    except Exception as e:
        logger.warning("ë‹¨ì¶• URL ë³€í™˜ ì‹¤íŒ¨ %s (%s)", url, e)
    
    return url

# --------------------------------------------------------------------------------------------
# ìœ í‹¸/ì „ì²˜ë¦¬
def now_utc() -> datetime:
    return datetime.now(tz=timezone.utc)

def current_date_str() -> str:
    """í˜„ì¬ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
    return datetime.now().strftime('%Y-%m-%d')

def normalize_space(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def clean_text_for_embedding(text: str) -> str:
    """
    ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜ (OCR ì˜¤ë¥˜ ë³´ì • í¬í•¨)
    """
    if not text:
        return ""
    
    # ê¸°ë³¸ ê³µë°± ì •ê·œí™”
    cleaned = normalize_space(text)
    
    # OCR ì˜¤ë¥˜ ë³´ì • (ì¼ë°˜ì ì¸ í•œêµ­ì–´ OCR íŒ¨í„´ ê¸°ë°˜)
    import re
    
    # 1. ëª…í™•í•œ ë³µí•©ëª…ì‚¬ë§Œ ë³µì› (ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´)
    compound_nouns = [
        # ì •ì¹˜/í–‰ì • ê´€ë ¨
        (r'ëŒ€í†µ\s*ë ¹', 'ëŒ€í†µë ¹'),
        (r'í—Œë²•\s*ì¬íŒì†Œ', 'í—Œë²•ì¬íŒì†Œ'),
        (r'êµ­\s*íšŒ', 'êµ­íšŒ'),
        (r'ì •\s*ë¶€', 'ì •ë¶€'),
        (r'ì˜\s*ì›', 'ì˜ì›'),
        
        # ê²½ì œ/ì‚¬íšŒ ê´€ë ¨  
        (r'êµ­ë¯¼\s*ê±´ê°•\s*ë³´í—˜', 'êµ­ë¯¼ê±´ê°•ë³´í—˜'),
        (r'ì‚¬íšŒ\s*ë³´ì¥', 'ì‚¬íšŒë³´ì¥'),
        (r'ê¸ˆìœµ\s*ìœ„ì›íšŒ', 'ê¸ˆìœµìœ„ì›íšŒ'),
        (r'ê¸°íš\s*ì¬ì •ë¶€', 'ê¸°íšì¬ì •ë¶€'),
        
        # ê¸°ìˆ /ê³¼í•™ ê´€ë ¨
        (r'ì¸ê³µ\s*ì§€ëŠ¥', 'ì¸ê³µì§€ëŠ¥'),
        (r'ì •ë³´\s*í†µì‹ ', 'ì •ë³´í†µì‹ '),
        (r'ê³¼í•™\s*ê¸°ìˆ ', 'ê³¼í•™ê¸°ìˆ '),
        
        # ì˜ë£Œ/ë³´ê±´ ê´€ë ¨
        (r'ì½”ë¡œë‚˜\s*ë°”ì´ëŸ¬ìŠ¤', 'ì½”ë¡œë‚˜ë°”ì´ëŸ¬ìŠ¤'),
        (r'ë³´ê±´\s*ë³µì§€ë¶€', 'ë³´ê±´ë³µì§€ë¶€'),
        
        # êµìœ¡ ê´€ë ¨
        (r'êµìœ¡\s*ë¶€', 'êµìœ¡ë¶€'),
        (r'ëŒ€\s*í•™êµ', 'ëŒ€í•™êµ'),
    ]
    
    # 2. ë³µí•©ëª…ì‚¬ íŒ¨í„´ë§Œ ì„ íƒì  ë³µì›
    for pattern, replacement in compound_nouns:
        cleaned = re.sub(pattern, replacement, cleaned)
    
    # 3. ë‹¨ì¼ ê¸€ì ë¶„ë¦¬ë§Œ ë³µì› (ì˜ë¯¸ ë‹¨ìœ„ëŠ” ë³´ì¡´)
    # "ëŒ€ í†µ ë ¹" â†’ "ëŒ€í†µë ¹" (3ê¸€ì ì´í•˜ ë‹¨ìœ„ë§Œ)
    cleaned = re.sub(r'([ê°€-í£])\s([ê°€-í£])\s([ê°€-í£])(?=\s|$)', r'\1\2\3', cleaned)
    cleaned = re.sub(r'([ê°€-í£])\s([ê°€-í£])(?=\s|$)', r'\1\2', cleaned)
    
    # 3. í˜•íƒœí•™ì  ìœ ì‚¬ì„± ê¸°ë°˜ ê¸€ì ì˜¤ì¸ì‹ ë³´ì • (ì¼ë°˜ì  íŒ¨í„´)
    char_patterns = [
        # 'ã…‡'ê³¼ 'ã…—' ê³„ì—´ í˜¼ë™
        (r'ìš´([ê°€-í£]*ì—´)', r'ìœ¤\1'),    # ìš´ì„ì—´ â†’ ìœ¤ì„ì—´, ìš´ë™ì—´ â†’ ìœ¤ë™ì—´ ë“±
        (r'([ê°€-í£]*)í—¥', r'\1í•µ'),      # íƒ„í—¥ â†’ íƒ„í•µ, ì›í—¥ â†’ ì›í•µ ë“±
        
        # 'ã„±'ê³¼ 'ã„´' ê³„ì—´ í˜¼ë™
        (r'ê±¸ì •', 'ê²°ì •'),             # ê±¸ì • â†’ ê²°ì •
        (r'íŒŒë¨¼', 'íŒŒë©´'),             # íŒŒë¨¼ â†’ íŒŒë©´
        
        # 'ã…'ê³¼ 'ã…‡' ê³„ì—´ í˜¼ë™
        (r'ì˜í—Œ', 'ì˜ì›'),             # ì˜í—Œ â†’ ì˜ì›
        (r'êµ­ê¹¨', 'êµ­íšŒ'),             # êµ­ê¹¨ â†’ êµ­íšŒ
        
        # ì—°ì†ëœ ê°™ì€ ê¸€ì ì˜¤ë¥˜
        (r'([ê°€-í£])\1{2,}', r'\1'),   # ê°™ì€ ê¸€ì 3ë²ˆ ì´ìƒ â†’ 1ë²ˆìœ¼ë¡œ
        
        # ìˆ«ìì™€ í•œê¸€ í˜¼ë™
        (r'1([ê°€-í£])', r'ã…£\1'),      # 1ê¸€ì â†’ ã…£ê¸€ì (í•„ìš”ì‹œ)
        (r'0([ê°€-í£])', r'ã…‡\1'),      # 0ê¸€ì â†’ ã…‡ê¸€ì (í•„ìš”ì‹œ)
    ]
    
    # íŒ¨í„´ ê¸°ë°˜ ë³´ì • ì ìš©
    for pattern, replacement in char_patterns:
        cleaned = re.sub(pattern, replacement, cleaned)
    
    # íŠ¹ìˆ˜ë¬¸ì ë° ì´ìƒí•œ ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ìœ ì§€)
    cleaned = re.sub(r'[^\w\sê°€-í£\.\,\!\?\:\;\(\)\-\"\']', ' ', cleaned)
    
    # ì—°ì†ëœ ê³µë°± ì œê±°
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned

def canonical_url(u: str) -> str:
    try:
        p = up.urlparse(u)
        q = up.parse_qs(p.query)
        # ì¶”ì  íŒŒë¼ë¯¸í„° ë° ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œê±°
        filtered = {k: v for k, v in q.items() if not k.startswith((
            "utm_", "fbclid", "gclid", "igshid", "ref", "share", "_ga", "campaign", 
            "source", "medium", "term", "content", "spm_id", "module", "pgtype"
        ))}
        new_q = up.urlencode([(k, vv) for k, vals in filtered.items() for vv in vals])
        p2 = p._replace(query=new_q)
        netloc = p2.netloc.lower()
        path = re.sub(r"/+", "/", p2.path)
        if path != "/" and path.endswith("/"):
            path = path[:-1]
        p3 = p2._replace(netloc=netloc, path=path)
        return up.urlunparse(p3)
    except Exception:
        return u

def url_similarity(url1: str, url2: str) -> float:
    """ë‘ URL ê°„ì˜ ìœ ì‚¬ì„±ì„ ê³„ì‚° (0~1)"""
    try:
        p1, p2 = up.urlparse(url1), up.urlparse(url2)
        
        # ë„ë©”ì¸ì´ ë‹¤ë¥´ë©´ 0
        if p1.netloc.lower() != p2.netloc.lower():
            return 0.0
        
        # ê²½ë¡œê°€ ì™„ì „íˆ ê°™ìœ¼ë©´ 1
        if p1.path == p2.path:
            return 1.0
        
        # ê²½ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ë¹„êµ
        path1_parts = [part for part in p1.path.split('/') if part]
        path2_parts = [part for part in p2.path.split('/') if part]
        
        if not path1_parts or not path2_parts:
            return 0.0
        
        # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸(ì£¼ë¡œ ê¸°ì‚¬ ID)ë§Œ ë‹¤ë¥¸ ê²½ìš° ë†’ì€ ìœ ì‚¬ì„±
        if len(path1_parts) == len(path2_parts):
            common_parts = sum(1 for a, b in zip(path1_parts[:-1], path2_parts[:-1]) if a == b)
            similarity = common_parts / max(1, len(path1_parts) - 1)
            if similarity >= 0.8:  # ê²½ë¡œì˜ 80% ì´ìƒì´ ê°™ìœ¼ë©´
                return 0.9
        
        return 0.0
    except Exception:
        return 0.0

def domain_of(u: str) -> str:
    try:
        return up.urlparse(u).netloc.lower()
    except Exception:
        return ""

def is_same_domain(u: str, seed_domain: str) -> bool:
    d = domain_of(u)
    return d == seed_domain or d.endswith("." + seed_domain)

def extract_links(base_url: str, html: str) -> List[str]:
    out = []
    try:
        soup = BeautifulSoup(html, "html.parser")
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            abs_u = up.urljoin(base_url, href)
            if abs_u.startswith(("http://", "https://")):
                out.append(canonical_url(abs_u))
    except Exception:
        pass
    return list(set(out))

def korean_ratio(text: str) -> float:
    if not text:
        return 0.0
    total = len(text)
    hangul = sum(1 for ch in text if '\uac00' <= ch <= '\ud7a3')
    return hangul / max(1, total)

# --------------------------------------------------------------------------------------------
# ë³¸ë¬¸ ì¶”ì¶œ (ë„ë©”ì¸ ì „ìš© â†’ AMP/JSON-LD/Next.js/Readability â†’ trafilatura â†’ manual â†’ newspaper3k)
def extract_text(url: str, html: Optional[str], fast: bool = False) -> Tuple[str, Optional[datetime], str]:
    text, dt, title = "", None, ""

    # 0) ë‹¨ì¶• URLì„ ì‹¤ì œ URLë¡œ ë³€í™˜
    original_url = url
    url = resolve_shortened_url(url)
    if url != original_url:
        logger.info("ë‹¨ì¶• URL ë³€í™˜ë¨: %s -> %s", original_url, url)

    # 1) html ì—†ìœ¼ë©´ ë°ìŠ¤í¬í†±â†’ëª¨ë°”ì¼ ìˆœìœ¼ë¡œ ì‹œë„ (ë„¤ì´ë²„ëŠ” ì•„ë˜ ì „ìš©ê¸°ë¡œ ë³´ì •)
    if not html:
        html = polite_get(url) or polite_get(url, mobile=True)

    def _clean_html_text(h: str) -> str:
        soup = BeautifulSoup(h, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        return normalize_space(soup.get_text(" ", strip=True))

    # ----- ì „ìš© ì¶”ì¶œê¸°: ë„¤ì´ë²„ -----
    def _extract_naver_article(h: str) -> Tuple[str, Optional[str], Optional[str]]:
        if not h:
            return "", None, None
        soup = BeautifulSoup(h, "html.parser")
        node = soup.select_one("#dic_area") or soup.select_one(".newsct_article")
        body = ""
        if node:
            for bad in node.select("figure, .promotion, .byline, .copyright, .end_photo_org, .img_desc"):
                bad.decompose()
            body = normalize_space(node.get_text(" ", strip=True))
        hed = None
        tnode = soup.select_one("h2#title_area .media_end_head_headline") or soup.select_one("h2.media_end_head_headline")
        if tnode:
            hed = normalize_space(tnode.get_text(" ", strip=True))
        if not hed and soup.title and soup.title.string:
            hed = normalize_space(soup.title.string)
        date_str = None
        meta = soup.find("meta", attrs={"property": "article:published_time"})
        if meta and meta.get("content"):
            date_str = meta["content"]
        else:
            dnode = soup.select_one("span.media_end_head_info_datestamp_time")
            if dnode and dnode.get("data-date-time"):
                date_str = dnode["data-date-time"]
        return body, date_str, hed

    # ----- ì „ìš© ì¶”ì¶œê¸°: JTBC(Next.js JSON + CSS ì„ íƒì) -----
    def _extract_jtbc_nextdata(h: str) -> Tuple[str, Optional[str], Optional[str]]:
        if not h:
            return "", None, None
        soup = BeautifulSoup(h, "html.parser")
        
        # 0) Next.js self.__next_f.push ë°©ì‹ ì²˜ë¦¬ (ìµœìš°ì„ )
        scripts = soup.find_all("script")
        for script in scripts:
            if script.string and "self.__next_f.push" in script.string:
                try:
                    content = script.string
                    
                    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ íŒ¨í„´ìœ¼ë¡œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
                    korean_pattern = r'"([^"]*[ê°€-í£]+[^"]*)"'
                    matches = re.findall(korean_pattern, content)
                    
                    # ê°€ì¥ ê¸´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ê¸°ì‚¬ ë³¸ë¬¸ìœ¼ë¡œ ì‚¬ìš©
                    longest_text = ""
                    for match in matches:
                        if len(match) > len(longest_text) and len(match) > 100:
                            # ê¸°ì‚¬ ë‚´ìš© ê°™ì€ íŒ¨í„´ì¸ì§€ í™•ì¸
                            if any(keyword in match for keyword in ['ë§í–ˆë‹¤', 'ë°í˜”ë‹¤', 'ì „í–ˆë‹¤', 'ë°œí‘œí–ˆë‹¤', 'ì„¤ëª…í–ˆë‹¤']):
                                longest_text = match
                            elif len(match) > 200:  # ì¶©ë¶„íˆ ê¸´ í…ìŠ¤íŠ¸ë©´ ê¸°ì‚¬ ë‚´ìš©ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
                                longest_text = match
                    
                    if longest_text:
                        # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì²˜ë¦¬
                        cleaned = longest_text.replace('\\n', '\n').replace('\\t', ' ')
                        cleaned = cleaned.replace('\\"', '"').replace('\\\\', '\\')
                        
                        # HTML íƒœê·¸ ì œê±°
                        cleaned = re.sub(r'<[^>]+>', '', cleaned)
                        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
                        
                        if len(cleaned) > 100:
                            logger.debug(f"JTBC Next.js í•œêµ­ì–´ íŒ¨í„´ ì¶”ì¶œ ì„±ê³µ ({len(cleaned)}ì)")
                            
                            # ì œëª© ì¶”ì¶œ
                            title = ""
                            if soup.title:
                                title = soup.title.get_text(strip=True)
                            
                            return normalize_space(cleaned), None, normalize_space(title) if title else None
                                
                except Exception as e:
                    logger.debug(f"JTBC Next.js íŒ¨í„´ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
        
        # 0-1) Meta description fallback
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if not meta_desc:
            meta_desc = soup.find("meta", attrs={"property": "og:description"})
        
        if meta_desc and meta_desc.get('content'):
            desc = meta_desc.get('content')
            if len(desc) > 80:  # Meta descriptionì´ ì¶©ë¶„íˆ ê¸´ ê²½ìš°
                logger.debug(f"JTBC Meta description ì¶”ì¶œ ì„±ê³µ ({len(desc)}ì)")
                
                title = ""
                if soup.title:
                    title = soup.title.get_text(strip=True)
                
                return normalize_space(desc), None, normalize_space(title) if title else None
        
        # 1) CSS ì„ íƒì ê¸°ë°˜ ë³¸ë¬¸ ì¶”ì¶œ (JTBC ì „ìš© ì„ íƒì ì¶”ê°€)
        article_selectors = [
            "div[data-module='ArticleContent']",
            "article .newsroom_article_content",
            ".newsroom_article_content", 
            "article .article_content",
            ".article_content",
            "[data-testid='article-content']",
            ".news_article_body",
            ".article_body_content",
            ".MuiBox-root p",  # Material-UI êµ¬ì¡°
            "main p",
            "[class*='ArticleContent']",
            "[class*='article-body']",
            "[class*='news-body']"
        ]
        
        for selector in article_selectors:
            elements = soup.select(selector)
            if elements:
                # ì—¬ëŸ¬ ìš”ì†Œì¸ ê²½ìš° í•©ì¹˜ê¸°
                content_parts = []
                for elem in elements:
                    # ê´‘ê³ , ê´€ë ¨ê¸°ì‚¬ ë“± ì œê±°
                    for unwanted in elem.select(".ad, .advertisement, .related, .taboola, script, style, .share, .sns"):
                        unwanted.decompose()
                    
                    text = elem.get_text(" ", strip=True)
                    if len(text) > 50:
                        content_parts.append(text)
                
                if content_parts:
                    body_text = " ".join(content_parts)
                    body_text = normalize_space(body_text)
                    
                    if len(body_text) >= 100:
                        logger.debug(f"JTBC CSS ì„ íƒì ì¶”ì¶œ ì„±ê³µ (ì„ íƒì: {selector}, {len(body_text)}ì)")
                        
                        # ì œëª© ì¶”ì¶œ
                        title_selectors = ["h1", ".headline", ".article_title", "title"]
                        title = ""
                        for title_sel in title_selectors:
                            title_node = soup.select_one(title_sel)
                            if title_node:
                                title = normalize_space(title_node.get_text(strip=True))
                                break
                        
                        # ë‚ ì§œ ì¶”ì¶œ
                        date_str = None
                        date_meta = soup.find("meta", attrs={"property": "article:published_time"})
                        if date_meta and date_meta.get("content"):
                            date_str = date_meta["content"]
                        else:
                            time_node = soup.select_one("time[datetime]")
                            if time_node and time_node.get("datetime"):
                                date_str = time_node["datetime"]
                        
                        return body_text, date_str, title
        
        # 2) Next.js __NEXT_DATA__ ì²˜ë¦¬
        sc = soup.find("script", id="__NEXT_DATA__", type="application/json")
        if sc and sc.string:
            try:
                data = json.loads(sc.string)
                texts, pub, hed = [], None, None
                def _walk(x):
                    nonlocal pub, hed
                    if isinstance(x, dict):
                        for k in ("headline","title","name"):
                            v = x.get(k)
                            if isinstance(v, str) and not hed:
                                hed = v
                        for k in ("datePublished","dateModified","publishDate","publishedAt"):
                            v = x.get(k)
                            if isinstance(v, str) and not pub:
                                pub = v
                        for k, v in x.items():
                            if isinstance(v, str) and k.lower() in {"articlebody","body","content","text","value","rawhtml","html"}:
                                if len(v) > 20:
                                    texts.append(v)
                            elif isinstance(v, (list, dict)):
                                _walk(v)
                    elif isinstance(x, list):
                        for v in x:
                            _walk(v)
                _walk(data)
                if texts:
                    raw = "\n".join(texts)
                    body = _clean_html_text(raw)
                    if len(body) >= 50:
                        return normalize_space(body), pub, (normalize_space(hed) if hed else None)
            except Exception:
                pass
        
        # 3) ì •ê·œì‹ fallback
        patterns = [
            r'"articleBody"\s*:\s*"(.+?)"',
            r'"content"\s*:\s*"(.+?)"',
            r'"text"\s*:\s*"(.+?)"',
            r'"body"\s*:\s*"(.+?)"'
        ]
        
        for pattern in patterns:
            m = re.search(pattern, h, re.DOTALL)
            if m:
                try:
                    raw = m.group(1).encode('utf-8', 'backslashreplace').decode('unicode_escape')
                    body = _clean_html_text(raw)
                    if len(body) >= 50:
                        title = ""
                        if soup.title and soup.title.string:
                            title = normalize_space(soup.title.string)
                        return normalize_space(body), None, title
                except Exception:
                    continue
        
        # 4) ì¼ë°˜ì ì¸ ê¸°ì‚¬ êµ¬ì¡° ì‹œë„
        article_node = soup.find("article") or soup.find("main")
        if article_node:
            paragraphs = article_node.find_all("p")
            if len(paragraphs) >= 3:
                body_parts = []
                for p in paragraphs:
                    text = normalize_space(p.get_text(strip=True))
                    if len(text) > 20 and not any(skip in text.lower() for skip in ["ê´‘ê³ ", "advertisement", "ê´€ë ¨ê¸°ì‚¬", "ì¶”ì²œ"]):
                        body_parts.append(text)
                
                if body_parts:
                    body = " ".join(body_parts)
                    if len(body) >= 100:
                        title = ""
                        if soup.title and soup.title.string:
                            title = normalize_space(soup.title.string)
                        return normalize_space(body), None, title
        
        return "", None, None

    # 1) AMP link
    amp_html = None
    if html:
        try:
            soup = BeautifulSoup(html, "html.parser")
            amp_link = soup.find("link", rel=lambda v: v and "amphtml" in v.lower())
            if amp_link and amp_link.get("href"):
                amp_url = up.urljoin(url, amp_link["href"])
                # ì˜ëª»ëœ amp ì„œë¸Œë„ë©”ì¸ ì‹œë„ ê¸ˆì§€
                if "amp." not in up.urlparse(amp_url).netloc:
                    amp_html = polite_get(amp_url) or polite_get(amp_url, mobile=True)
        except Exception:
            pass

    # 2) JSON-LD
    def extract_from_jsonld(h: str) -> Tuple[str, Optional[str], Optional[str]]:
        try:
            soup = BeautifulSoup(h, "html.parser")
            for script in soup.find_all("script", type="application/ld+json"):
                try:
                    data = json.loads(script.string or "")
                except Exception:
                    continue
                candidates = data if isinstance(data, list) else [data]
                for item in candidates:
                    if not isinstance(item, dict):
                        continue
                    typ = item.get("@type") or item.get("@graph", [{}])[0].get("@type")
                    if (typ and ("Article" in str(typ) or "NewsArticle" in str(typ))) or item.get("articleBody"):
                        body = item.get("articleBody")
                        if isinstance(body, list):
                            body = "\n".join([str(x) for x in body])
                        hed = (item.get("headline") or item.get("name") or "")[:300]
                        date_str = item.get("datePublished") or item.get("dateModified")
                        return normalize_space(body or ""), date_str, hed
        except Exception:
            pass
        return "", None, None

    # 3) Next.js-ish generic
    def extract_from_next_json(h: str) -> Tuple[str, Optional[str], Optional[str]]:
        try:
            soup = BeautifulSoup(h, "html.parser")
            scripts_raw = []
            for sc in soup.find_all("script"):
                raw = sc.string or sc.get_text() or ""
                raw = raw.strip()
                if not raw:
                    continue
                if any(k in raw for k in ("__NEXT_DATA__", "articleBody", "\"content\"", "\"text\"", "\"value\"", "\"datePublished\"")) \
                or (raw.startswith("{") and raw.endswith("}")) \
                or (raw.startswith("[") and raw.endswith("]")):
                    scripts_raw.append(raw)

            def _walk(x, texts: list, meta: dict):
                if isinstance(x, dict):
                    for k in ("headline","title","name"):
                        v = x.get(k)
                        if isinstance(v, str) and not meta.get("title"):
                            meta["title"] = v
                    for k in ("datePublished","dateModified","publishDate","publishedAt"):
                        v = x.get(k)
                        if isinstance(v, str) and not meta.get("date"):
                            meta["date"] = v
                    for k, v in x.items():
                        if isinstance(v, str):
                            kl = k.lower()
                            if kl in {"articlebody","body","content","rawhtml","html","text","value"} and v.strip():
                                texts.append(v)
                        elif isinstance(v, (list, dict)):
                            _walk(v, texts, meta)
                elif isinstance(x, list):
                    for el in x:
                        _walk(el, texts, meta)

            for raw in scripts_raw:
                parsed = None
                try:
                    parsed = json.loads(raw)
                except Exception:
                    found = re.findall(r'"(?:text|value|content)"\s*:\s*"([^"]+)"', raw)
                    if found:
                        body_raw = "\n".join([fx for fx in found if fx.strip()])
                        body_txt = BeautifulSoup(body_raw, "html.parser").get_text(" ", strip=True)
                        body_txt = normalize_space(body_txt)
                        if len(body_txt) >= 50:
                            return body_txt, None, None
                if parsed is not None:
                    texts, meta = [], {"title": None, "date": None}
                    _walk(parsed, texts, meta)
                    if texts:
                        body_raw = "\n".join([t for t in texts if t.strip()])
                        body_txt = BeautifulSoup(body_raw, "html.parser").get_text(" ", strip=True)
                        body_txt = normalize_space(body_txt)
                        if len(body_txt) >= 50:
                            return body_txt, meta.get("date"), meta.get("title")
        except Exception:
            pass
        return "", None, None

    # 4) Readability
    def extract_with_readability(h: str) -> Tuple[str, str]:
        try:
            doc = Document(h)
            hed = normalize_space(doc.short_title())
            summ = doc.summary(html_partial=False)
            soup = BeautifulSoup(summ, "html.parser")
            parts = [normalize_space(p.get_text(" ", strip=True)) for p in soup.find_all("p")]
            body = normalize_space("\n".join([p for p in parts if p]))
            return body, hed
        except Exception:
            return "", ""

    # 5) trafilatura
    def extract_with_trafilatura(h: str) -> str:
        try:
            return normalize_space(trafilatura.extract(
                h, include_comments=False, include_tables=False,
                favor_precision=(False if fast else True)
            ) or "")
        except Exception:
            return ""

    # 6) newspaper3k
    def extract_with_newspaper(u: str) -> Tuple[str, Optional[datetime], str]:
        try:
            art = Article(u, keep_article_html=False, language="ko")
            art.download()
            art.parse()
            t = normalize_space(art.text)
            hed = normalize_space(art.title)
            d = art.publish_date
            if d and d.tzinfo is None:
                d = d.replace(tzinfo=timezone.utc)
            return t, d, hed
        except Exception:
            return "", None, ""

    # 7) ìˆ˜ë™ CSS í›„ë³´
    def extract_manual(h: str) -> str:
        try:
            soup = BeautifulSoup(h, "html.parser")
            candidates = [
                {"id": "article", "cls": None},
                {"id": "article_body", "cls": None},
                {"id": "articleContent", "cls": None},
                {"id": None, "cls": "article_body"},
                {"id": None, "cls": "article-content"},
                {"id": None, "cls": "news_article"},
                {"id": None, "cls": "content_article"},
            ]
            chunks = []
            for c in candidates:
                node = soup.find(id=c["id"]) if c["id"] else soup.find(class_=lambda v: v and c["cls"] in v)
                if node:
                    ps = node.find_all(["p", "div"])
                    for p in ps:
                        txt = normalize_space(p.get_text(" ", strip=True))
                        if txt:
                            chunks.append(txt)
            if chunks:
                return normalize_space("\n".join(chunks))
        except Exception:
            pass
        return ""

    # ---- ì‹¤ì œ ì¶”ì¶œ ìˆœì„œ ----
    host = domain_of(url)

    # (Z) ë„ë©”ì¸ ì „ìš© ë¹ ë¥¸ ê²½ë¡œ: NAVER / JTBC
    if html:
        if host.endswith("n.news.naver.com") or host.endswith("news.naver.com"):
            b, dstr, hed = _extract_naver_article(html)
            if len(b) >= 80:
                text = b; title = hed or title
                if dstr:
                    try: dt = datetime.fromisoformat(dstr.replace("Z","+00:00"))
                    except Exception: pass
        elif host.endswith("news.jtbc.co.kr"):
            b, dstr, hed = _extract_jtbc_nextdata(html)
            if len(b) >= 80:
                text = b; title = hed or title
                if dstr:
                    try: dt = datetime.fromisoformat(dstr.replace("Z","+00:00"))
                    except Exception: pass

    # (A) AMP ìš°ì„ 
    if amp_html and not text:
        body, date_str, hed = extract_from_jsonld(amp_html)
        if not body:
            body, hed2 = extract_with_readability(amp_html); hed = hed or hed2
        if not body:
            body = extract_with_trafilatura(amp_html)
        if not body:
            body = extract_manual(amp_html)
        if body:
            text = body; title = hed or title
            if date_str:
                try: dt = datetime.fromisoformat(date_str.replace("Z","+00:00"))
                except Exception: pass

    # (B) ì›ë³¸ HTML: JSON-LD â†’ Next.js JSON â†’ Readability â†’ trafilatura â†’ manual
    if html and not text:
        body, date_str, hed = extract_from_jsonld(html)
        if not body:
            body, date_str2, hed2 = extract_from_next_json(html)
            hed = hed or hed2
            date_str = date_str or date_str2
        if not body:
            body, hed2 = extract_with_readability(html); hed = hed or hed2
        if not body:
            body = extract_with_trafilatura(html)
        if not body:
            body = extract_manual(html)
        if body:
            text = body; title = hed or title
            if date_str and not dt:
                try: dt = datetime.fromisoformat(date_str.replace("Z","+00:00"))
                except Exception: pass

    # (C) newspaper3k ìµœí›„
    if not text:
        t2, d2, hed = extract_with_newspaper(url)
        if t2:
            text, dt, title = t2, (dt or d2), (title or hed)

    # (D) ë©”íƒ€ íƒ€ì´í‹€ ë³´ì •
    if not title and html:
        try:
            soup = BeautifulSoup(html, "html.parser")
            if soup.title and soup.title.string:
                title = normalize_space(soup.title.string)
        except Exception:
            pass

    return text, dt, title

# --------------------------------------------------------------------------------------------
# ë„ë©”ì¸ í‰íŒ íœ´ë¦¬ìŠ¤í‹±
GOOD_TLD_HINTS = (".go.kr", ".ac.kr", ".lg.jp", ".gov", ".edu")
OK_TLD_HINTS   = (".or.kr", ".or.jp", ".org", ".co.kr", ".co.jp", ".com", ".net")
LOW_TLD_HINTS  = (".info", ".biz")

def source_reputation(url: str, in_seed: bool) -> float:
    d = domain_of(url)
    score = 0.0
    if in_seed: score += 0.4
    if d.endswith(GOOD_TLD_HINTS): score += 0.4
    elif d.endswith(OK_TLD_HINTS): score += 0.2
    elif d.endswith(LOW_TLD_HINTS): score -= 0.1
    if url.lower().startswith("https://"): score += 0.05
    return max(-0.2, min(0.8, score))

def time_weight(dt_pub: Optional[datetime]) -> float:
    if not dt_pub: return 0.0
    age_days = max(0.0, (now_utc() - dt_pub).total_seconds()/86400.0)
    
    # ì—°ë„ë³„ ëŒ€í­ ê°•í™”ëœ í˜ë„í‹° (ì˜¤ë³´ ê¸°ì‚¬ ëŒ€ì‘)
    if age_days > 365 * 13:  # 13ë…„ ì´ìƒ (2011ë…„ ì´ì „) - JTBC ì˜¤ë³´ ê¸°ì‚¬ ëŒ€ì‘
        return -1.2  # ë§¤ìš° ê°•í•œ í˜ë„í‹° (ê°•í™”)
    elif age_days > 365 * 10:  # 10ë…„ ì´ìƒ (2014ë…„ ì´ì „)
        return -1.0  # ê°•í•œ í˜ë„í‹° (ê°•í™”)
    elif age_days > 365 * 7:  # 7ë…„ ì´ìƒ (2017ë…„ ì´ì „)
        return -0.8  # ê°•í•œ í˜ë„í‹°
    elif age_days > 365 * 5:  # 5ë…„ ì´ìƒ (2019ë…„ ì´ì „)
        return -0.6  # ì¤‘ê°„ í˜ë„í‹°
    elif age_days > 365 * 3:  # 3ë…„ ì´ìƒ (2021ë…„ ì´ì „)
        return -0.4  # ì•½ê°„ í˜ë„í‹°
    elif age_days > 365 * 1:  # 1ë…„ ì´ìƒ
        return -0.2  # ìµœì†Œ í˜ë„í‹°
    else:
        w = math.exp(-TIME_LAMBDA * age_days)  # 1ë…„ ì´ë‚´ëŠ” ê¸°ì¡´ ê³µì‹
        return -0.1 + 0.9 * w  # -0.1 ~ +0.8

# --------------------------------------------------------------------------------------------
# ë°ì´í„° êµ¬ì¡°
@dataclass
class DocRecord:
    url: str
    title: str
    published: Optional[float]
    chunk: str
    domain: str
    from_seed: bool

@dataclass
class IndexPack:
    model_name: str
    embed_dim: int
    matrix: np.ndarray
    records: List[DocRecord]

# --------------------------------------------------------------------------------------------
# ë¬¸ì¥ ë¶„í• /ì²­í‚¹
def split_into_sentences(text: str) -> List[str]:
    parts = re.split(r"(?<=[.!?â€¦]|[ã€‚ï¼ï¼Ÿ])\s+", text)
    parts = [normalize_space(p) for p in parts if len(normalize_space(p)) > 0]
    return parts

def make_chunks(text: str, window: int = 4, step: int = 3, min_len: int = 200) -> List[str]:
    sents = split_into_sentences(text)
    chunks = []
    i = 0
    while i < len(sents):
        block = normalize_space(" ".join(sents[i:i+window]))
        if len(block) >= min_len:
            chunks.append(block)
        i += step
    if not chunks and len(text) >= min_len:
        chunks = [text]
    return chunks

# --------------------------------------------------------------------------------------------
# ëª¨ë¸ ë¡œë”©(GPU/FP16)
DEVICE = "cuda" if (hasattr(torch, "cuda") and torch.cuda.is_available()) else "cpu"

def get_embedder(use_gpu: bool, fp16: bool):
    # ì„ë² ë”© ì°¨ì› í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©í•˜ë˜ ì˜ë¯¸ì  ë¶„ì„ ê°•í™”
    korean_models = [
        "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # ê¸°ì¡´ ëª¨ë¸ (í˜¸í™˜ì„±)
        "distiluse-base-multilingual-cased"  # ë°±ì—… ëª¨ë¸
    ]
    
    device = "cuda" if (use_gpu and DEVICE == "cuda") else "cpu"
    
    # CUDA ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    if device == "cuda":
        torch.backends.cudnn.benchmark = True  # ë°˜ë³µì ì¸ ì—°ì‚° ìµœì í™”
        torch.backends.cudnn.deterministic = False  # ì„±ëŠ¥ ìš°ì„ 
        torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
        # GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ìµœì í™”
        torch.cuda.set_per_process_memory_fraction(0.9)  # 90% VRAM ì‚¬ìš© í—ˆìš©
    
    # í•œêµ­ì–´ ëª¨ë¸ë¶€í„° ì°¨ë¡€ë¡œ ì‹œë„
    emb = None
    selected_model = None
    
    for model in korean_models:
        try:
            logger.info(f"ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì‹œë„: {model}")
            emb = SentenceTransformer(model, device=device)
            selected_model = model
            logger.info(f"âœ… AI ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model}")
            break
        except Exception as e:
            logger.warning(f"âŒ ëª¨ë¸ {model} ë¡œë”© ì‹¤íŒ¨: {e}")
            continue
    
    if emb is None:
        raise Exception("ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
    
    logger.info("ì„ë² ë”© ëª¨ë¸: %s (device=%s, fp16=%s)", selected_model, emb._target_device, fp16)
    return emb, fp16

def get_nli(use_gpu: bool, fp16: bool):
    name = "cross-encoder/nli-deberta-v3-small"
    tok = AutoTokenizer.from_pretrained(name)
    mdl = AutoModelForSequenceClassification.from_pretrained(
        name, torch_dtype=(torch.float16 if (fp16 and DEVICE == "cuda") else None)
    )
    mdl.eval()
    mdl.to("cuda" if (use_gpu and DEVICE == "cuda") else "cpu")
    logger.info("NLI ëª¨ë¸: %s (device=%s, dtype=%s)", name, next(mdl.parameters()).device, next(mdl.parameters()).dtype)
    return tok, mdl, fp16


def analyze_semantic_relevance(query_text: str, article_content: str, embedder) -> dict:
    """
    ê³ ë„í™”ëœ ì˜ë¯¸ì  ì—°ê´€ì„± ë¶„ì„
    
    Args:
        query_text: ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ (ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸)
        article_content: ê¸°ì‚¬ ë‚´ìš©
        embedder: ì„ë² ë”© ëª¨ë¸
    
    Returns:
        dict: ì—°ê´€ì„± ì ìˆ˜ì™€ ìƒì„¸ ë¶„ì„ ê²°ê³¼
    """
    try:
        # 1. í•µì‹¬ ì£¼ì œ ì¶”ì¶œ (í•œêµ­ì–´ ì •ì¹˜/ì‚¬íšŒ ì´ìŠˆ ì¤‘ì‹¬)
        query_topics = extract_semantic_topics(query_text)
        article_topics = extract_semantic_topics(article_content)
        
        # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (ì„ë² ë”© ê¸°ë°˜)
        query_embedding = embedder.encode([query_text])
        article_embedding = embedder.encode([article_content])
        semantic_similarity = util.cos_sim(query_embedding, article_embedding)[0][0].item()
        
        # 3. ì£¼ì œë³„ ì—°ê´€ì„± ë¶„ì„
        topic_relevance = calculate_topic_relevance(query_topics, article_topics)
        
        # 4. í•œêµ­ì–´ ë§¥ë½ ê³ ë ¤ (ì •ì¹˜ì¸, ê¸°ê´€ëª…, ì‚¬ê±´ëª… ë“±)
        context_score = analyze_korean_context(query_text, article_content)
        
        # 5. AI ê¸°ë°˜ ê³ ë„í™”ëœ ë‚´ìš© ì—°ê´€ì„± ë¶„ì„
        ai_relevance = analyze_content_relevance_with_ai(query_text, article_content)
        
        # 6. ì¢…í•© ì—°ê´€ì„± ì ìˆ˜ ê³„ì‚° (AI ë¶„ì„ ë¹„ì¤‘ ì¦ê°€)
        final_score = (
            semantic_similarity * 0.25 +  # ì˜ë¯¸ì  ìœ ì‚¬ë„ 25%
            topic_relevance * 0.25 +      # ì£¼ì œ ì—°ê´€ì„± 25%
            context_score * 0.2 +         # í•œêµ­ì–´ ë§¥ë½ 20%
            ai_relevance * 0.3            # AI ì—°ê´€ì„± ë¶„ì„ 30%
        )
        
        logger.debug(f"ğŸ§  ì˜ë¯¸ì  ì—°ê´€ì„± ë¶„ì„:")
        logger.debug(f"   - ì˜ë¯¸ì  ìœ ì‚¬ë„: {semantic_similarity:.3f}")
        logger.debug(f"   - ì£¼ì œ ì—°ê´€ì„±: {topic_relevance:.3f}")
        logger.debug(f"   - ë§¥ë½ ì ìˆ˜: {context_score:.3f}")
        logger.debug(f"   - AI ì—°ê´€ì„±: {ai_relevance:.3f}")
        logger.debug(f"   - ì¢…í•© ì ìˆ˜: {final_score:.3f}")
        
        return {
            'semantic_similarity': semantic_similarity,
            'topic_relevance': topic_relevance,
            'context_score': context_score,
            'ai_relevance': ai_relevance,
            'final_score': final_score,
            'query_topics': query_topics,
            'article_topics': article_topics
        }
        
    except Exception as e:
        logger.error(f"ì˜ë¯¸ì  ì—°ê´€ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {
            'semantic_similarity': 0.0,
            'topic_relevance': 0.0,
            'context_score': 0.0,
            'final_score': 0.0,
            'query_topics': [],
            'article_topics': []
        }


def extract_semantic_topics(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ì  ì£¼ì œ ì¶”ì¶œ"""
    topics = []
    
    # ì •ì¹˜ ê´€ë ¨ ì£¼ì œ
    political_patterns = {
        'ëŒ€í†µë ¹_íƒ„í•µ': ['ëŒ€í†µë ¹', 'íƒ„í•µ', 'íŒŒë©´', 'í—Œë²•ì¬íŒì†Œ'],
        'ì„ ê±°_ì •ì¹˜': ['ì„ ê±°', 'íˆ¬í‘œ', 'í›„ë³´', 'ì •ë‹¹', 'êµ­íšŒì˜ì›'],
        'ì •ë¶€_ì •ì±…': ['ì •ë¶€', 'ì •ì±…', 'ë²•ì•ˆ', 'êµ­ì •ê°ì‚¬', 'êµ­ì •ìš´ì˜'],
        'ì‚¬ë²•_ìˆ˜ì‚¬': ['ê²€ì°°', 'ìˆ˜ì‚¬', 'ê¸°ì†Œ', 'ì¬íŒ', 'íŒê²°']
    }
    
    # ì‚¬íšŒ ê´€ë ¨ ì£¼ì œ  
    social_patterns = {
        'ê²½ì œ_ê¸ˆìœµ': ['ê²½ì œ', 'ê¸ˆë¦¬', 'ë¬¼ê°€', 'ì£¼ì‹', 'ë¶€ë™ì‚°'],
        'ë³´ê±´_ì˜ë£Œ': ['ì½”ë¡œë‚˜', 'ë°±ì‹ ', 'ë³‘ì›', 'ì˜ë£Œ', 'ë°©ì—­'],
        'êµìœ¡_ë¬¸í™”': ['êµìœ¡', 'í•™êµ', 'ëŒ€í•™', 'ë¬¸í™”', 'ì˜ˆìˆ '],
        'í™˜ê²½_ì•ˆì „': ['í™˜ê²½', 'ê¸°í›„', 'ì•ˆì „', 'ì¬í•´', 'ì‚¬ê³ ']
    }
    
    all_patterns = {**political_patterns, **social_patterns}
    
    text_lower = text.lower()
    for topic, keywords in all_patterns.items():
        if any(keyword in text_lower for keyword in keywords):
            topics.append(topic)
    
    return topics


def calculate_topic_relevance(query_topics: List[str], article_topics: List[str]) -> float:
    """ì£¼ì œ ê°„ ì—°ê´€ì„± ê³„ì‚°"""
    if not query_topics or not article_topics:
        return 0.0
    
    # ë™ì¼ ì£¼ì œ ë§¤ì¹­
    common_topics = set(query_topics) & set(article_topics)
    if common_topics:
        return len(common_topics) / max(len(query_topics), len(article_topics))
    
    # ê´€ë ¨ ì£¼ì œ ë§¤ì¹­ (ì •ì¹˜-ì‚¬ë²•, ê²½ì œ-ì‚¬íšŒ ë“±)
    related_pairs = {
        'ëŒ€í†µë ¹_íƒ„í•µ': ['ì‚¬ë²•_ìˆ˜ì‚¬', 'ì •ë¶€_ì •ì±…'],
        'ì„ ê±°_ì •ì¹˜': ['ì •ë¶€_ì •ì±…', 'ëŒ€í†µë ¹_íƒ„í•µ'],
        'ê²½ì œ_ê¸ˆìœµ': ['ì •ë¶€_ì •ì±…'],
        'ë³´ê±´_ì˜ë£Œ': ['ì •ë¶€_ì •ì±…', 'ì‚¬íšŒ_ë³µì§€']
    }
    
    relevance_score = 0.0
    for q_topic in query_topics:
        for a_topic in article_topics:
            if q_topic in related_pairs and a_topic in related_pairs[q_topic]:
                relevance_score += 0.5  # ê´€ë ¨ ì£¼ì œëŠ” 50% ì ìˆ˜
    
    return min(relevance_score, 1.0)


def analyze_korean_context(query_text: str, article_content: str) -> float:
    """í•œêµ­ì–´ ë§¥ë½ ë¶„ì„ (ì¸ëª…, ê¸°ê´€ëª…, ê³ ìœ ëª…ì‚¬ ë“±)"""
    try:
        # ì •ì¹˜ì¸ ì´ë¦„ ë§¤ì¹­
        politicians = ['ìœ¤ì„ì—´', 'ì´ì¬ëª…', 'í•œë™í›ˆ', 'ì¡°êµ­', 'ë¬¸ì¬ì¸', 'ë°•ê·¼í˜œ']
        # ê¸°ê´€ëª… ë§¤ì¹­  
        institutions = ['í—Œë²•ì¬íŒì†Œ', 'êµ­íšŒ', 'ì²­ì™€ëŒ€', 'ì •ë¶€', 'ê²€ì°°', 'ë²•ì›']
        # ì‚¬ê±´/ì´ìŠˆëª… ë§¤ì¹­
        events = ['íƒ„í•µ', 'íŒŒë©´', 'ì„ ê±°', 'êµ­ì •ê°ì‚¬', 'ìˆ˜ì‚¬', 'ê¸°ì†Œ']
        
        query_lower = query_text.lower()
        article_lower = article_content.lower()
        
        context_matches = 0
        total_contexts = 0
        
        for context_list in [politicians, institutions, events]:
            for item in context_list:
                total_contexts += 1
                if item in query_lower and item in article_lower:
                    context_matches += 2  # ì–‘ìª½ ëª¨ë‘ ìˆìœ¼ë©´ 2ì 
                elif item in query_lower or item in article_lower:
                    context_matches += 1  # í•œìª½ë§Œ ìˆìœ¼ë©´ 1ì 
        
        return min(context_matches / (total_contexts * 2), 1.0) if total_contexts > 0 else 0.0
        
    except Exception as e:
        logger.error(f"í•œêµ­ì–´ ë§¥ë½ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0.0


def analyze_content_relevance_with_ai(query_text: str, article_content: str) -> float:
    """
    AI ëª¨ë¸ì„ í™œìš©í•œ ê³ ë„í™”ëœ ë‚´ìš© ì—°ê´€ì„± ë¶„ì„
    OpenAI API ë˜ëŠ” ë¡œì»¬ LLMì„ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ì  ì—°ê´€ì„±ì„ ì •ë°€ ë¶„ì„
    """
    try:
        # ë¡œì»¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ë¶„ì„
        # (OpenAI API í‚¤ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„)
        
        # 1. í•µì‹¬ ì‚¬ê±´/ì¸ë¬¼ ë§¤ì¹­ ê°•í™”
        key_entities = extract_key_entities(query_text, article_content)
        entity_score = key_entities['match_score']
        
        # 2. ì‹œê°„ì  ë§¥ë½ ë¶„ì„ (ë‚ ì§œ, ì‹œê¸° ë“±)
        temporal_score = analyze_temporal_context(query_text, article_content)
        
        # 3. ì‚¬ê±´ ì—°ê´€ì„± ë¶„ì„ (íƒ„í•µ-í—Œì¬, ì„ ê±°-ì •ì¹˜ì¸ ë“±)
        event_score = analyze_event_relationships(query_text, article_content)
        
        # 4. ê°ì •/ë…¼ì¡° ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
        sentiment_score = analyze_sentiment_consistency(query_text, article_content)
        
        # ì¢…í•©ì ìˆ˜ ê³„ì‚°
        final_score = (
            entity_score * 0.4 +      # í•µì‹¬ ì—”í‹°í‹° ë§¤ì¹­ 40%
            temporal_score * 0.2 +    # ì‹œê°„ì  ë§¥ë½ 20%
            event_score * 0.3 +       # ì‚¬ê±´ ì—°ê´€ì„± 30%
            sentiment_score * 0.1     # ê°ì • ì¼ê´€ì„± 10%
        )
        
        if final_score > 0.7:
            logger.info(f"ğŸ¯ ë†’ì€ AI ì—°ê´€ì„± ë°œê²¬ (ì ìˆ˜: {final_score:.3f})")
            logger.debug(f"   - ì—”í‹°í‹°: {entity_score:.3f}, ì‹œê°„: {temporal_score:.3f}")
            logger.debug(f"   - ì‚¬ê±´: {event_score:.3f}, ê°ì •: {sentiment_score:.3f}")
        
        return min(final_score, 1.0)
        
    except Exception as e:
        logger.error(f"AI ì—°ê´€ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0.0


def extract_key_entities(query_text: str, article_content: str) -> dict:
    """í•µì‹¬ ì—”í‹°í‹° ì¶”ì¶œ ë° ë§¤ì¹­"""
    import re
    
    # í•œêµ­ ì •ì¹˜ ê´€ë ¨ í•µì‹¬ ì—”í‹°í‹°
    entities = {
        'politicians': ['ìœ¤ì„ì—´', 'ì´ì¬ëª…', 'í•œë™í›ˆ', 'ì¡°êµ­', 'ë¬¸ì¬ì¸', 'ë°•ê·¼í˜œ', 'ê¹€ê±´í¬'],
        'institutions': ['í—Œë²•ì¬íŒì†Œ', 'êµ­íšŒ', 'ì²­ì™€ëŒ€', 'ëŒ€í†µë ¹ì‹¤', 'ê²€ì°°', 'êµ­ì •ì›'],
        'parties': ['ë¯¼ì£¼ë‹¹', 'êµ­ë¯¼ì˜í˜', 'ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹', 'ì •ì˜ë‹¹', 'ê°œí˜ì‹ ë‹¹'],
        'events': ['íƒ„í•µ', 'íŒŒë©´', 'íƒ„í•µì‹¬íŒ', 'êµ­ì •ê°ì‚¬', 'íŠ¹ê²€', 'ìˆ˜ì‚¬']
    }
    
    query_entities = set()
    article_entities = set()
    
    for category, entity_list in entities.items():
        for entity in entity_list:
            if entity in query_text:
                query_entities.add(entity)
            if entity in article_content:
                article_entities.add(entity)
    
    # ê³µí†µ ì—”í‹°í‹° ê³„ì‚°
    common_entities = query_entities & article_entities
    match_score = len(common_entities) / max(len(query_entities), 1) if query_entities else 0
    
    return {
        'query_entities': list(query_entities),
        'article_entities': list(article_entities),
        'common_entities': list(common_entities),
        'match_score': match_score
    }


def analyze_temporal_context(query_text: str, article_content: str) -> float:
    """ì‹œê°„ì  ë§¥ë½ ë¶„ì„"""
    import re
    
    # ë‚ ì§œ íŒ¨í„´ ë§¤ì¹­
    date_patterns = [
        r'\d{4}ë…„\s*\d{1,2}ì›”',      # 2025ë…„ 10ì›”
        r'\d{1,2}ì›”\s*\d{1,2}ì¼',    # 10ì›” 25ì¼
        r'\d{4}\s*ë…„',               # 2025ë…„
        r'ì–´ì œ|ì˜¤ëŠ˜|ë‚´ì¼|ì´ë²ˆì£¼|ë‹¤ìŒì£¼|ì§€ë‚œì£¼'
    ]
    
    query_dates = []
    article_dates = []
    
    for pattern in date_patterns:
        query_dates.extend(re.findall(pattern, query_text))
        article_dates.extend(re.findall(pattern, article_content))
    
    if not query_dates and not article_dates:
        return 0.5  # ì¤‘ë¦½
    
    # ê³µí†µ ì‹œê°„ í‘œí˜„ ë¹„ìœ¨
    common_dates = set(query_dates) & set(article_dates)
    if common_dates:
        return 0.8  # ë†’ì€ ì‹œê°„ì  ì—°ê´€ì„±
    elif query_dates or article_dates:
        return 0.3  # ë¶€ë¶„ì  ì‹œê°„ì  ë§¥ë½
    
    return 0.0


def analyze_event_relationships(query_text: str, article_content: str) -> float:
    """ì‚¬ê±´ ê°„ ì—°ê´€ì„± ë¶„ì„"""
    
    # ì‚¬ê±´ ì—°ê´€ ë§µí•‘
    event_relationships = {
        'íƒ„í•µ': ['í—Œë²•ì¬íŒì†Œ', 'í—Œì¬', 'ì‹¬íŒ', 'íŒŒë©´', 'ì •ì¹˜'],
        'íŒŒë©´': ['íƒ„í•µ', 'í—Œë²•ì¬íŒì†Œ', 'ëŒ€í†µë ¹', 'ê¶Œí•œì •ì§€'],
        'ì„ ê±°': ['í›„ë³´', 'íˆ¬í‘œ', 'ì •ë‹¹', 'ì„ ê±°ìš´ë™', 'ê³µì•½'],
        'ìˆ˜ì‚¬': ['ê²€ì°°', 'ê¸°ì†Œ', 'í˜ì˜', 'ì¡°ì‚¬', 'ì¦ê±°'],
        'êµ­ì •ê°ì‚¬': ['êµ­íšŒ', 'ì˜ì›', 'ê°ì‚¬', 'ì§ˆì˜', 'ë‹µë³€']
    }
    
    query_events = []
    article_events = []
    
    # ì¿¼ë¦¬ì™€ ê¸°ì‚¬ì—ì„œ ì‚¬ê±´ ì¶”ì¶œ
    for event, related_terms in event_relationships.items():
        if event in query_text:
            query_events.append(event)
        if event in article_content:
            article_events.append(event)
    
    if not query_events:
        return 0.5  # ì¤‘ë¦½
    
    # ì§ì ‘ ë§¤ì¹­
    direct_match = len(set(query_events) & set(article_events))
    if direct_match > 0:
        return 1.0
    
    # ì—°ê´€ ì‚¬ê±´ ë§¤ì¹­
    relationship_score = 0.0
    for q_event in query_events:
        if q_event in event_relationships:
            related_terms = event_relationships[q_event]
            for term in related_terms:
                if term in article_content:
                    relationship_score += 0.2  # ì—°ê´€ ìš©ì–´ë‹¹ 0.2ì 
    
    return min(relationship_score, 1.0)


def analyze_sentiment_consistency(query_text: str, article_content: str) -> float:
    """ê°ì •/ë…¼ì¡° ì¼ê´€ì„± ë¶„ì„"""
    
    # ê¸ì •ì /ë¶€ì •ì  í‚¤ì›Œë“œ
    positive_keywords = ['ì„±ê³µ', 'ë°œì „', 'ê°œì„ ', 'ì¦ê°€', 'ìƒìŠ¹', 'ê¸ì •', 'í¬ë§']
    negative_keywords = ['ì‹¤íŒ¨', 'ë¬¸ì œ', 'ê°ì†Œ', 'í•˜ë½', 'ë¶€ì •', 'ìš°ë ¤', 'ë¹„íŒ', 'ë…¼ë€']
    
    def get_sentiment_score(text):
        pos_count = sum(1 for word in positive_keywords if word in text)
        neg_count = sum(1 for word in negative_keywords if word in text)
        
        if pos_count > neg_count:
            return 1  # ê¸ì •
        elif neg_count > pos_count:
            return -1  # ë¶€ì •
        else:
            return 0  # ì¤‘ë¦½
    
    query_sentiment = get_sentiment_score(query_text)
    article_sentiment = get_sentiment_score(article_content)
    
    # ê°ì • ì¼ì¹˜ë„
    if query_sentiment == article_sentiment:
        return 1.0  # ì™„ì „ ì¼ì¹˜
    elif abs(query_sentiment - article_sentiment) == 1:
        return 0.5  # ë¶€ë¶„ ì¼ì¹˜
    else:
        return 0.0  # ë¶ˆì¼ì¹˜

@torch.no_grad()
def nli_batch_probs(pairs: List[Tuple[str, str]], tok, mdl, batch_size: int, use_fp16: bool) -> np.ndarray:
    outs = []
    device = next(mdl.parameters()).device
    for i in range(0, len(pairs), batch_size):
        prem = [p for p, _ in pairs[i:i+batch_size]]
        hypo = [h for _, h in pairs[i:i+batch_size]]
        inputs = tok(prem, hypo, truncation=True, max_length=256, padding=True, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}
        if use_fp16 and device.type == "cuda":
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                logits = mdl(**inputs).logits
        else:
            logits = mdl(**inputs).logits
        probs = torch.softmax(logits, dim=-1).cpu().numpy()
        outs.append(probs)
    return np.vstack(outs) if outs else np.zeros((0, 3), dtype=np.float32)

# --------------------------------------------------------------------------------------------
# í¬ë¡¤ë§(ë„ë©”ì¸ ë‹¨ìœ„) + Overall ì§„í–‰ë°”
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

def crawl_domain(seed_url: str, max_depth: int, max_pages: int,
                 overall_update: Optional[Callable[[int], None]] = None) -> List[Tuple[str, str]]:
    visited = set()
    out = []
    seed_dom = domain_of(seed_url)
    q = queue.Queue()
    q.put((seed_url, 0))
    
    # ê°„ë‹¨í•œ ì§„í–‰ë¥  í‘œì‹œ (ë°±ê·¸ë¼ìš´ë“œ, ë¹„í™œì„±í™”)
    pbar = tqdm(total=max_pages, desc=f"Crawl {seed_dom}", 
                leave=False, position=None, disable=True)  # ì™„ì „íˆ ë¹„í™œì„±í™”
    
    while not q.empty() and len(visited) < max_pages:
        url, depth = q.get()
        if url in visited:
            continue
        visited.add(url)
        html = polite_get(url) or polite_get(url, mobile=True)
        if html:
            out.append((url, html))
            if depth < max_depth:
                for nxt in extract_links(url, html):
                    if is_same_domain(nxt, seed_dom):
                        q.put((nxt, depth + 1))
        pbar.update(1)
        if overall_update:
            overall_update(1)
        time.sleep(CRAWL_SLEEP)
    pbar.close()
    logger.info("ë„ë©”ì¸ í¬ë¡¤ ì™„ë£Œ: %s (ìˆ˜ì§‘ %d / ë°©ë¬¸ %d)", seed_dom, len(out), len(visited))
    return out

# --------------------------------------------------------------------------------------------
# ì‹œë“œ ì²˜ë¦¬ - í¬ë¡¤ë§ë§Œ (ì„ë² ë”©ì€ ë³„ë„ ì²˜ë¦¬)
def process_seed_crawl_only(seed: str, fast_extract: bool,
                           overall_update: Optional[Callable[[int], None]] = None) -> List[Tuple[str, str, str, str]]:
    """í¬ë¡¤ë§ë§Œ ìˆ˜í–‰í•˜ê³  í…ìŠ¤íŠ¸ ì²­í¬ ë°˜í™˜"""
    dom = domain_of(seed)
    pages = crawl_domain(seed, MAX_DEPTH, MAX_PAGES_PER_DOMAIN, overall_update=overall_update)
    logger.info("ë„ë©”ì¸ ìˆ˜ì§‘ ì™„ë£Œ: %s (%d pages)", dom, len(pages))

    text_chunks = []
    for url, html in pages:
        text, dt, title = extract_text(url, html, fast=fast_extract)
        if len(text) >= MIN_TEXT_LEN:
            chunks = make_chunks(text, min_len=MIN_TEXT_LEN)
            for ch in chunks:
                text_chunks.append((url, dt, title, ch))
    return text_chunks

# ê¸°ì¡´ í•¨ìˆ˜ë„ ìœ ì§€ (í˜¸í™˜ì„±)
def process_seed(seed: str, embedder, embed_batch: int, fast_extract: bool,
                 overall_update: Optional[Callable[[int], None]] = None) -> Tuple[List[np.ndarray], List[DocRecord]]:
    dom = domain_of(seed)
    pages = crawl_domain(seed, MAX_DEPTH, MAX_PAGES_PER_DOMAIN, overall_update=overall_update)
    logger.info("ë„ë©”ì¸ ìˆ˜ì§‘ ì™„ë£Œ: %s (%d pages)", dom, len(pages))

    texts, metas = [], []
    for url, html in pages:
        text, dt, title = extract_text(url, html, fast=fast_extract)
        if len(text) >= MIN_TEXT_LEN:
            chunks = make_chunks(text, min_len=MIN_TEXT_LEN)
            for ch in chunks:
                texts.append(ch)
                metas.append((url, dt, title, domain_of(url)))

    if not texts:
        return [], []

    # RTX3070ti 8GB VRAM ìµœëŒ€ í™œìš© ì„ë² ë”© ì²˜ë¦¬
    effective_batch_size = min(embed_batch, len(texts))
    
    # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë‹¤ë©´ ë” í° ë°°ì¹˜ ì‚¬ìš©
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory
        if gpu_memory > 7 * 1024**3:  # 7GB ì´ìƒì´ë©´
            effective_batch_size = min(2048, len(texts))  # ë” í° ë°°ì¹˜ ì‚¬ìš©
        
        # GPU ì‚¬ìš© ê°•ì œ ë° ìµœì í™”
        with torch.cuda.device(0):
            torch.cuda.empty_cache()  # ìºì‹œ ì •ë¦¬
            vecs = embedder.encode(
                texts, 
                batch_size=effective_batch_size,
                convert_to_numpy=True, 
                normalize_embeddings=True,
                show_progress_bar=False,
                device='cuda'  # ëª…ì‹œì ìœ¼ë¡œ CUDA ì§€ì •
            )
    else:
        vecs = embedder.encode(
            texts, 
            batch_size=effective_batch_size,
            convert_to_numpy=True, 
            normalize_embeddings=True,
            show_progress_bar=False,
            device='cpu'
        )
    recs = []
    for (url, dt, title, d), ch, v in zip(metas, texts, vecs):
        recs.append(DocRecord(
            url=url,
            title=title or "",
            published=(dt.timestamp() if dt else None),
            chunk=ch,
            domain=d,
            from_seed=True
        ))
    return list(vecs), recs

# GPU ìµœëŒ€ í™œìš© ë°°ì¹˜ ì„ë² ë”© ì²˜ë¦¬
def batch_embed_texts(text_chunks: List[Tuple[str, str, str, str]], embedder, embed_batch: int) -> Tuple[List[np.ndarray], List[DocRecord]]:
    """í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ë°°ì¹˜ë¡œ ì„ë² ë”© ì²˜ë¦¬ - GPU ìµœëŒ€ í™œìš© (ë¶„í•  ì²˜ë¦¬)"""
    if not text_chunks:
        return [], []
    
    texts = [chunk[3] for chunk in text_chunks]  # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    total_texts = len(texts)
    
    print(f"[GPU] GPU ìµœëŒ€ í™œìš© ì„ë² ë”© ì‹œì‘: {total_texts:,}ê°œ ì²­í¬")
    
    # RTX3070ti 8GBì— ë§ëŠ” ë©”ëª¨ë¦¬ ê´€ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶„í•  ì²˜ë¦¬ ì „ëµ
        if total_texts > 100000:  # 10ë§Œê°œ ì´ìƒ
            chunk_size = 50000  # 5ë§Œê°œì”© ë¶„í• 
            dynamic_batch_size = 512
        elif total_texts > 50000:  # 5ë§Œê°œ ì´ìƒ 
            chunk_size = 25000  # 2.5ë§Œê°œì”© ë¶„í• 
            dynamic_batch_size = 768
        else:  # 5ë§Œê°œ ë¯¸ë§Œ
            chunk_size = total_texts  # ë¶„í•  ì•ˆ í•¨
            dynamic_batch_size = min(1024, total_texts)
        
        print(f"   ğŸ“Š ì²˜ë¦¬ ì „ëµ: {chunk_size:,}ê°œì”© ë¶„í• , ë°°ì¹˜ í¬ê¸°: {dynamic_batch_size}")
        print(f"    ì´ ë¶„í•  ìˆ˜: {(total_texts + chunk_size - 1) // chunk_size}ê°œ")
        
        # GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì œí•œ
        torch.cuda.set_per_process_memory_fraction(0.8)  # 80%ë§Œ ì‚¬ìš©
        
        # ë¶„í•  ì²˜ë¦¬
        all_vecs = []
        for i in range(0, total_texts, chunk_size):
            chunk_texts = texts[i:i + chunk_size]
            chunk_info = text_chunks[i:i + chunk_size]
            
            print(f"   ğŸ“¦ ë¶„í•  {i//chunk_size + 1}: {len(chunk_texts):,}ê°œ ì„ë² ë”© ì¤‘...")
            
            try:
                torch.cuda.empty_cache()  # ê° ë¶„í•  ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
                
                with torch.cuda.device(0):
                    chunk_vecs = embedder.encode(
                        chunk_texts,
                        batch_size=dynamic_batch_size,
                        convert_to_numpy=True,
                        normalize_embeddings=True,
                        show_progress_bar=False,
                        device='cuda'
                    )
                    
                all_vecs.extend(chunk_vecs)
                
                # ì¤‘ê°„ ì§„í–‰ ìƒí™© ì¶œë ¥
                processed = min(i + chunk_size, total_texts)
                print(f"   âœ… ì™„ë£Œ: {processed:,}/{total_texts:,} ({processed/total_texts*100:.1f}%)")
                
            except torch.cuda.OutOfMemoryError:
                print(f"   âš ï¸  GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, CPUë¡œ ëŒ€ì²´ ì²˜ë¦¬...")
                # CPU ë°±ì—… ì²˜ë¦¬
                chunk_vecs = embedder.encode(
                    chunk_texts,
                    batch_size=min(64, len(chunk_texts)),
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                    show_progress_bar=False,
                    device='cpu'
                )
                all_vecs.extend(chunk_vecs)
        
        vecs = all_vecs
        
    else:
        # CPU ì²˜ë¦¬
        vecs = embedder.encode(
            texts,
            batch_size=min(embed_batch, 64),
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False,
            device='cpu'
        )
    
    # DocRecord ìƒì„±
    recs = []
    for (url, dt, title, ch), v in zip(text_chunks, vecs):
        recs.append(DocRecord(
            url=url,
            title=title or "",
            published=(dt.timestamp() if dt else None),
            chunk=ch,
            domain=domain_of(url),
            from_seed=True
        ))
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print(f"âœ… ì„ë² ë”© ì™„ë£Œ: {len(recs):,}ê°œ ë²¡í„° ìƒì„±")
    return list(vecs), recs

# --------------------------------------------------------------------------------------------
# ë³‘ë ¬ ë¹Œë“œ - ë©”ëª¨ë¦¬ ìµœì í™”
def build_index_parallel(seeds: List[str], embedder, workers: int, embed_batch: int, fast_extract: bool) -> IndexPack:
    # 128GB RAM í™œìš©ì„ ìœ„í•œ ì´ˆê¸° ìš©ëŸ‰ ì„¤ì •
    estimated_chunks = len(seeds) * MAX_PAGES_PER_DOMAIN * 5  # í˜ì´ì§€ë‹¹ í‰ê·  5ê°œ ì²­í¬ ì˜ˆìƒ
    all_vecs, all_recs = [], []
    # íŒŒì´ì¬ì—ì„œëŠ” ë¦¬ìŠ¤íŠ¸ reserveê°€ ì—†ìœ¼ë¯€ë¡œ ëŒ€ì‹  ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”

    estimated_total_pages = len(seeds) * MAX_PAGES_PER_DOMAIN
    start_time = time.time()
    
    # CPU ìŠ¤ë ˆë“œ ìˆ˜ì— ë”°ë¼ ì‹¤ì œ ì›Œì»¤ ìˆ˜ ì¡°ì • (ìµœëŒ€ í™œìš©)
    cpu_count = mp.cpu_count()
    effective_workers = min(workers, cpu_count * 2)  # I/O ì§‘ì•½ì ì´ë¯€ë¡œ 2ë°°
    
    # í•˜ë“œì›¨ì–´ ì •ë³´ ë¨¼ì € ì¶œë ¥ (ì§„í–‰ë¥  ë°”ì™€ ë¶„ë¦¬)
    print(f"ğŸ”§ í•˜ë“œì›¨ì–´ ìµœì í™”:")
    print(f"   ğŸ’» CPU ì½”ì–´: {cpu_count}ê°œ")
    print(f"   ğŸ”€ í¬ë¡¤ë§ ì›Œì»¤: {effective_workers}ê°œ")
    print(f"   ğŸ“¦ ì„ë² ë”© ë°°ì¹˜: {embed_batch}ê°œ")
    print(f"   ğŸ® GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB" if torch.cuda.is_available() else "   ğŸ® GPU: ë¹„í™œì„±")
    print("=" * 50)
    
    # ë‹¨ê³„ 1: ëª¨ë“  ì‹œë“œì—ì„œ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ (CPU ì§‘ì•½ì )
    print("ğŸ•·ï¸  1ë‹¨ê³„: ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘...")
    
    # ì§„í–‰ë„ í‘œì‹œ - ì‹¤ì‹œê°„ ì‹œê°„ ì—…ë°ì´íŠ¸
    seeds_progress = tqdm(
        total=len(seeds), 
        desc="ğŸ“° ë„ë©”ì¸ ì²˜ë¦¬", 
        unit="ê°œ",
        leave=True,  # ì™„ë£Œ í›„ ìœ ì§€í•˜ì—¬ ìµœì¢… ìƒíƒœ í‘œì‹œ
        dynamic_ncols=True,  # í„°ë¯¸ë„ í¬ê¸°ì— ë§ì¶° ì¡°ì •
        bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [â±ï¸ {elapsed}]",
        mininterval=0.01,  # 0.01ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸ (ë” ìì£¼)
        maxinterval=0.2   # ìµœëŒ€ 0.2ì´ˆë§ˆë‹¤ ê°•ì œ ì—…ë°ì´íŠ¸
    )
    
    # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ìŠ¤ë ˆë“œ
    import threading
    def update_timer():
        while not seeds_progress.disable and seeds_progress.n < seeds_progress.total:
            seeds_progress.refresh()
            time.sleep(0.2)  # 0.2ì´ˆë§ˆë‹¤ ê°±ì‹ 
    
    timer_thread = threading.Thread(target=update_timer, daemon=True)
    timer_thread.start()

    _lock = Lock()
    pages_processed = 0
    def safe_overall_update(n: int = 1):
        nonlocal pages_processed
        with _lock:
            pages_processed += n

    completed_seeds = 0
    all_text_chunks = []
    
    with ThreadPoolExecutor(max_workers=effective_workers) as ex:
        crawl_futs = {ex.submit(process_seed_crawl_only, s, fast_extract, safe_overall_update): s for s in seeds}
        for fut in as_completed(crawl_futs):
            s = crawl_futs[fut]
            try:
                text_chunks = fut.result()
                if text_chunks:
                    all_text_chunks.extend(text_chunks)
                
                completed_seeds += 1
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì¦‰ì‹œ ë°˜ì˜)
                domain_short = domain_of(s)[:15] + "..." if len(domain_of(s)) > 15 else domain_of(s)
                seeds_progress.update(1)
                seeds_progress.set_description(f"ğŸ“° ì™„ë£Œ: {domain_short}")
                seeds_progress.display()  # ì¦‰ì‹œ í‘œì‹œ ê°•ì œ
                    
            except Exception as e:
                completed_seeds += 1
                domain_short = domain_of(s)[:15] + "..." if len(domain_of(s)) > 15 else domain_of(s)
                seeds_progress.update(1)
                seeds_progress.set_description(f"ğŸ“° ì‹¤íŒ¨: {domain_short}")
                seeds_progress.display()  # ì¦‰ì‹œ í‘œì‹œ ê°•ì œ
    
    # í¬ë¡¤ë§ ì™„ë£Œ í›„ ìµœì¢… ìƒíƒœ í‘œì‹œ
    seeds_progress.set_description("ğŸ“° í¬ë¡¤ë§ ì™„ë£Œ")
    seeds_progress.close()
    
    print(f"\n[GPU] 2ë‹¨ê³„: GPU ìµœëŒ€ í™œìš© ì„ë² ë”© ì‹œì‘... (ì´ {len(all_text_chunks)}ê°œ ì²­í¬)")
    
    # ë‹¨ê³„ 2: ìˆ˜ì§‘ëœ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— GPUì—ì„œ ì„ë² ë”© (GPU ì§‘ì•½ì )
    if all_text_chunks:
        all_vecs, all_recs = batch_embed_texts(all_text_chunks, embedder, embed_batch)
        print(f"âœ… ì„ë² ë”© ì™„ë£Œ! (ìµœì¢… ì²­í¬: {len(all_recs):,}ê°œ)")
    else:
        all_vecs, all_recs = [], []

    # ì™„ë£Œ ë©”ì‹œì§€
    total_time = time.time() - start_time
    total_minutes = total_time / 60
    print(f"\nğŸ‰ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ì†Œìš”ì‹œê°„: {total_minutes:.1f}ë¶„")
    print(f"ğŸ“š ìˆ˜ì§‘ëœ ì²­í¬: {len(all_recs):,}ê°œ")
    print(f"ğŸŒ ì²˜ë¦¬ëœ ë„ë©”ì¸: {completed_seeds}/{len(seeds)}ê°œ")

    if not all_vecs:
        raise RuntimeError("ì¸ë±ìŠ¤ì— ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ë‹¤. ì‹œë“œ/í¬ë¡¤ë§ì„ í™•ì¸í•˜ë¼.")
    M = np.vstack(all_vecs).astype("float32")
    return IndexPack(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        embed_dim=M.shape[1],
        matrix=M,
        records=all_recs
    )

# --------------------------------------------------------------------------------------------
# ì¸ë±ìŠ¤ ë¹Œë“œ/ë¡œë“œ
def build_index(workers: int, embed_batch: int, use_gpu: bool, fp16: bool, http_pool: int, timeout: int, sleep: float, fast_extract: bool, test_mode: bool = False):
    configure_http(http_pool=http_pool, timeout=timeout)
    global CRAWL_SLEEP
    CRAWL_SLEEP = sleep

    assert os.path.exists(SEED_CSV), f"seed csv not found: {SEED_CSV}"
    with open(SEED_CSV, "r", encoding="utf-8") as f:
        seeds = [canonical_url(r["url"]) for r in csv.DictReader(f) if r.get("url", "").startswith("http")]
    seeds = list(dict.fromkeys(seeds))
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë§¤ìš° ì†ŒëŸ‰ì˜ ì‹œë“œë§Œ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    if test_mode:
        # ìµœì†Œí•œì˜ ë‹¤ì–‘ì„±ì„ ìœ„í•œ 3ê°œ ì‹œë“œë§Œ ì„ ë³„
        test_seeds = [
            # êµ­ë‚´ ì–¸ë¡ ì‚¬ 1ê°œ (ì‹ ë¢°ë„ ë†’ìŒ)
            "https://news.kbs.co.kr",
            # í•´ì™¸ ì–¸ë¡ ì‚¬ 1ê°œ (ì‹ ë¢°ë„ ë†’ìŒ)  
            "https://www.bbc.com",
            # í†µì‹ ì‚¬ 1ê°œ (ë¹ ë¥¸ ì²˜ë¦¬)
            "https://www.reuters.com"
        ]
        seeds = [s for s in seeds if s in test_seeds]
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”")
        print(f"ğŸ“Š ì‚¬ìš©í•  ì‹œë“œ: {len(seeds)}ê°œ (ì „ì²´ {len(test_seeds)}ê°œ ì¤‘)")
        print(f"âš¡ ì˜ˆìƒ ì™„ë£Œì‹œê°„: 2-5ë¶„")
        print("=" * 50)
    else:
        print(f"ğŸ“š ì „ì²´ ëª¨ë“œ í™œì„±í™”")
        print(f"ğŸ“Š ì‚¬ìš©í•  ì‹œë“œ: {len(seeds)}ê°œ")
        print(f"âš¡ ì˜ˆìƒ ì™„ë£Œì‹œê°„: 30-60ë¶„ (í•˜ë“œì›¨ì–´ì— ë”°ë¼)")
        print("=" * 50)

    embedder, _ = get_embedder(use_gpu=use_gpu, fp16=fp16)
    pack = build_index_parallel(seeds, embedder, workers=workers, embed_batch=embed_batch, fast_extract=fast_extract)
    with open(INDEX_PKL, "wb") as f:
        pickle.dump(pack, f)
    logger.info("[ok] index built: %s (rows=%d, dim=%d)", INDEX_PKL, pack.matrix.shape[0], pack.matrix.shape[1])

def load_index() -> IndexPack:
    assert os.path.exists(INDEX_PKL), f"index pkl not found: {INDEX_PKL}"
    with open(INDEX_PKL, "rb") as f:
        return pickle.load(f)

def save_index(pack: IndexPack):
    """ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with open(INDEX_PKL, "wb") as f:
        pickle.dump(pack, f)
    logger.info("[ok] index saved: %s (rows=%d, dim=%d)", INDEX_PKL, pack.matrix.shape[0], pack.matrix.shape[1])

def add_url_to_index(url: str, text: str, dt, title: str, embedder, pack: IndexPack) -> bool:
    """URLì„ ì¸ë±ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤. ì´ë¯¸ ì¡´ì¬í•˜ë©´ False, ì¶”ê°€ë˜ë©´ Trueë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # URL ì¤‘ë³µ ì²´í¬
    for record in pack.records:
        if record.url == url:
            logger.debug(f"URLì´ ì´ë¯¸ ì¸ë±ìŠ¤ì— ì¡´ì¬í•¨: {url}")
            return False
    
    # ìƒˆë¡œìš´ URL ì¶”ê°€
    logger.info(f"ìƒˆ URLì„ ì¸ë±ìŠ¤ì— ì¶”ê°€: {url}")
    
    # ì²­í¬ ìƒì„±
    chunks = make_chunks(text, min_len=max(120, MIN_TEXT_LEN // 2))
    if not chunks:
        chunks = [text]
    
    # ì„ë² ë”© ìƒì„±
    embeddings = embedder.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)
    
    # ê¸°ì¡´ ë§¤íŠ¸ë¦­ìŠ¤ì— ìƒˆ ì„ë² ë”© ì¶”ê°€
    new_matrix = np.vstack([pack.matrix, embeddings])
    
    # ìƒˆ ë ˆì½”ë“œë“¤ ìƒì„±
    new_records = []
    for i, chunk in enumerate(chunks):
        new_record = DocRecord(
            url=url,
            title=title,
            published=dt.timestamp() if dt else None,
            chunk=chunk,
            domain=domain_of(url),
            from_seed=False  # ì‚¬ìš©ì ì…ë ¥ URLì€ ì‹œë“œê°€ ì•„ë‹˜
        )
        new_records.append(new_record)
    
    # ì¸ë±ìŠ¤ íŒ© ì—…ë°ì´íŠ¸
    pack.matrix = new_matrix
    pack.records.extend(new_records)
    
    return True

def check_domains(domain_filter: Optional[str] = None, verbose: bool = False):
    """ì¸ë±ìŠ¤ì— í¬í•¨ëœ ë„ë©”ì¸ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    if not os.path.exists(INDEX_PKL):
        logger.error("ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: %s", INDEX_PKL)
        return
    
    pack = load_index()
    logger.info("ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: %dê°œ ë ˆì½”ë“œ", len(pack.records))
    
    # ë„ë©”ì¸ë³„ URL ìˆ˜ì§‘
    domain_counts = {}
    matching_urls = []
    
    for record in pack.records:
        url = record.url
        if url:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            domain_counts[domain] = domain_counts.get(domain, 0) + 1
            
            if domain_filter and domain_filter.lower() in domain.lower():
                matching_urls.append(url)
    
    # ê²°ê³¼ ì¶œë ¥
    if domain_filter:
        print(f"\n'{domain_filter}' í¬í•¨ ë„ë©”ì¸:")
        filtered_domains = {d: c for d, c in domain_counts.items() if domain_filter.lower() in d.lower()}
        for domain, count in sorted(filtered_domains.items(), key=lambda x: x[1], reverse=True):
            print(f"  {domain}: {count}ê°œ")
        
        print(f"\n'{domain_filter}' í¬í•¨ URL ëª©ë¡:")
        for url in matching_urls[:20]:  # ì²˜ìŒ 20ê°œë§Œ
            print(f"  {url}")
        if len(matching_urls) > 20:
            print(f"  ... (ì´ {len(matching_urls)}ê°œ)")
    else:
        print(f"\nì „ì²´ ë„ë©”ì¸ í†µê³„ (ìƒìœ„ 20ê°œ):")
        sorted_domains = sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)
        for domain, count in sorted_domains[:20]:
            print(f"  {domain}: {count}ê°œ")
        
        if verbose:
            print(f"\nì „ì²´ ë„ë©”ì¸ ëª©ë¡:")
            for domain, count in sorted(domain_counts.items()):
                print(f"  {domain}: {count}ê°œ")


def search_real_time_news(query_keywords: List[str]) -> List[dict]:
    """
    ì‹¤ì‹œê°„ Google ë‰´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•í•œ ê´€ë ¨ ê¸°ì‚¬ ì°¾ê¸°
    ê¸°ì¡´ ì¸ë±ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ì‹¤ì‹œê°„ ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
    """
    import requests
    from bs4 import BeautifulSoup
    import time
    
    if not query_keywords:
        return []
    
    try:
        logger.info(f"ğŸ”´ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: {query_keywords}")
        
        # í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì‹œë„
        search_queries = [
            ' '.join(query_keywords),  # ëª¨ë“  í‚¤ì›Œë“œ
            ' '.join(query_keywords[:2]),  # ìƒìœ„ 2ê°œ
            f"{query_keywords[0]} ë‰´ìŠ¤" if query_keywords else ""
        ]
        
        all_articles = []
        
        for search_query in search_queries:
            if not search_query.strip():
                continue
                
            logger.info(f"ğŸ” ì‹¤ì‹œê°„ ê²€ìƒ‰: '{search_query}'")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            # Google ë‰´ìŠ¤ ê²€ìƒ‰
            search_url = "https://www.google.com/search"
            params = {
                'q': f'{search_query} ë‰´ìŠ¤',
                'tbm': 'nws',  # ë‰´ìŠ¤ íƒ­
                'hl': 'ko',
                'gl': 'kr',
                'num': 10
            }
            
            try:
                response = requests.get(search_url, params=params, headers=headers, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ë‰´ìŠ¤ ê²°ê³¼ íŒŒì‹±
                news_items = soup.find_all('div', class_='SoaBEf')
                
                for item in news_items[:10]:  # ìƒìœ„ 10ê°œë¡œ ì¦ê°€
                    try:
                        title_elem = item.find('div', class_='MBeuO')
                        link_elem = item.find('a')
                        snippet_elem = item.find('div', class_='GI74Re nDgy9d')
                        
                        if title_elem and link_elem:
                            title = title_elem.get_text(strip=True)
                            url = link_elem.get('href', '')
                            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                            
                            # URL ì •ë¦¬
                            if url.startswith('/url?q='):
                                url = url.split('/url?q=')[1].split('&')[0]
                            
                            # í•œêµ­ ì–¸ë¡ ì‚¬ í•„í„°ë§
                            korean_domains = [
                                'news.naver.com', 'news.daum.net', 'chosun.com', 'joins.com',
                                'donga.com', 'hani.co.kr', 'khan.co.kr', 'ytn.co.kr',
                                'jtbc.co.kr', 'sbs.co.kr', 'kbs.co.kr', 'mbc.co.kr',
                                'yna.co.kr', 'newsis.com', 'edaily.co.kr', 'hankyung.com'
                            ]
                            
                            if any(domain in url for domain in korean_domains):
                                # í‚¤ì›Œë“œ ë§¤ì¹­ë„ ê³„ì‚°
                                title_lower = title.lower()
                                snippet_lower = snippet.lower()
                                
                                matches = 0
                                for keyword in query_keywords:
                                    if keyword.lower() in title_lower or keyword.lower() in snippet_lower:
                                        matches += 1
                                
                                # íŒ©íŠ¸ì²´í¬: ë…¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•œ ì¡°í•© í•„í„°ë§
                                fact_check_passed = fact_check_article(title, snippet, query_keywords)
                                
                                if matches >= 1 and fact_check_passed:  # ìµœì†Œ 1ê°œ í‚¤ì›Œë“œ ë§¤ì¹­ + íŒ©íŠ¸ì²´í¬ í†µê³¼
                                    article = {
                                        'title': title,
                                        'url': url,
                                        'snippet': snippet,
                                        'matches': matches,
                                        'match_ratio': matches / len(query_keywords),
                                        'source': 'real_time_search'
                                    }
                                    all_articles.append(article)
                                    logger.info(f"âœ… ì‹¤ì‹œê°„ ê´€ë ¨ ê¸°ì‚¬ ë°œê²¬: {title[:40]}... (ë§¤ì¹­: {matches}/{len(query_keywords)})")
                                elif not fact_check_passed:
                                    logger.warning(f"âŒ íŒ©íŠ¸ì²´í¬ ì‹¤íŒ¨ë¡œ ì œì™¸: {title[:40]}...")
                    
                    except Exception as e:
                        logger.debug(f"ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                        
            except Exception as e:
                logger.warning(f"ì‹¤ì‹œê°„ ê²€ìƒ‰ ì˜¤ë¥˜ ({search_query}): {e}")
                continue
            
            time.sleep(0.5)  # ìš”ì²­ ê°„ê²©
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_articles = []
        seen_urls = set()
        
        for article in all_articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)
        
        # ë§¤ì¹­ë„ìˆœ ì •ë ¬
        unique_articles.sort(key=lambda x: x['match_ratio'], reverse=True)
        
        logger.info(f"ğŸ”´ ì‹¤ì‹œê°„ ê²€ìƒ‰ ì™„ë£Œ: {len(unique_articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ë°œê²¬")
        return unique_articles[:15]  # ìƒìœ„ 15ê°œ ë°˜í™˜ìœ¼ë¡œ ì¦ê°€
        
    except Exception as e:
        logger.error(f"ì‹¤ì‹œê°„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


def fact_check_article(title: str, snippet: str, query_keywords: List[str]) -> bool:
    """
    ê¸°ì‚¬ ë‚´ìš©ì˜ íŒ©íŠ¸ì²´í¬ - ëª…ë°±í•œ í—ˆìœ„ì •ë³´ë§Œ í•„í„°ë§ (ê· í˜•ì¡íŒ ì ‘ê·¼)
    """
    try:
        content = f"{title} {snippet}".lower()
        current_date = datetime.now()
        
        # ëª…ë°±í•œ ì‚¬ì‹¤ ì˜¤ë¥˜ íŒ¨í„´ë“¤ (ì •ê·œì‹ ê¸°ë°˜)
        import re
        
        # 1. êµ¬ì¡°ì /ë…¼ë¦¬ì  ì˜¤ë¥˜ íŒ¨í„´ (í¸í–¥ ì—†ëŠ” ë²”ìš©ì  ê²€ì‚¬)
        # í˜„ì¬ ì‹œì  ê¸°ì¤€ìœ¼ë¡œ ëª…ë°±íˆ ì˜ëª»ëœ ì¡°í•©ë“¤ë§Œ ì²´í¬
        structural_error_patterns = [
            # í˜„ì¬ ëŒ€í†µë ¹ì´ ì•„ë‹Œ ì¸ë¬¼ë“¤ì˜ ëŒ€í†µë ¹ í˜¸ì¹­ (ë™ì ìœ¼ë¡œ í™•ì¸)
            # ë‹¨, ê³¼ê±° ê¸°ì‚¬ë‚˜ ê°€ì •ì  ìƒí™©ì€ ì œì™¸
            r'(?<!ê³¼ê±°\s)(?<!ì „\s)(?<!ë§Œì•½\s)ì´ì¬ëª…\s*ëŒ€í†µë ¹(?!\s*í›„ë³´)(?!\s*ì‹œì ˆ)',
            r'(?<!ê³¼ê±°\s)(?<!ì „\s)ë¬¸ì¬ì¸\s*í˜„.*ëŒ€í†µë ¹',
            r'(?<!ê³¼ê±°\s)(?<!ì „\s)ë°•ê·¼í˜œ\s*í˜„.*ëŒ€í†µë ¹',
        ]
        
        # 2. ëª…ë°±íˆ ì˜ëª»ëœ ë‚ ì§œ/ì‹œê¸° ì¡°í•©
        temporal_error_patterns = [
            r'202[0-3]ë…„.*ì½”ë¡œë‚˜19.*ë°œìƒ',  # ì½”ë¡œë‚˜19ëŠ” 2019ë…„ ë§ ë°œìƒ
            r'199\dë…„.*ì¸í„°ë„·.*ë³´ê¸‰',  # ì¸í„°ë„·ì€ 1990ë…„ëŒ€ ì¤‘í›„ë°˜ ë³´ê¸‰
        ]
        
        # 3. ë…¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•œ ì¡°í•©
        logical_error_patterns = [
            r'ì‚¬ë§í•œ.*\w+.*ìƒˆë¡œìš´.*í™œë™',  # ì‚¬ë§í•œ ì‚¬ëŒì´ ìƒˆë¡œìš´ í™œë™
            r'í•´ì²´ëœ.*ê¸°ê´€.*ìƒˆë¡œìš´.*ì •ì±…',  # í•´ì²´ëœ ê¸°ê´€ì´ ìƒˆë¡œìš´ ì •ì±…
        ]
        
        # íŒ¨í„´ ê²€ì‚¬ ì‹¤í–‰ (ë” ì‹ ì¤‘í•˜ê³  ê· í˜•ì¡íŒ ì ‘ê·¼)
        all_patterns = structural_error_patterns + temporal_error_patterns + logical_error_patterns
        
        for pattern in all_patterns:
            if re.search(pattern, content):
                # ì¿¼ë¦¬ì™€ ì§ì ‘ ê´€ë ¨ì„±ì´ ë†’ê³ , ëª…ë°±í•œ ì˜¤ë¥˜ì¸ ê²½ìš°ë§Œ í•„í„°ë§
                pattern_keywords = re.findall(r'[ê°€-í£]{2,}', pattern.replace('\\w+', '').replace('.*', '').replace('(?<!', '').replace('(?!', ''))
                query_text = ' '.join(query_keywords).lower()
                
                # ë” ì—„ê²©í•œ ì¡°ê±´: ì¿¼ë¦¬ì˜ í•µì‹¬ í‚¤ì›Œë“œì™€ ì§ì ‘ ë§¤ì¹­ë˜ëŠ” ê²½ìš°ë§Œ
                if len(pattern_keywords) > 0 and any(keyword in query_text for keyword in pattern_keywords):
                    logger.warning(f"âš ï¸ êµ¬ì¡°ì  ì˜¤ë¥˜ íŒ¨í„´ ê°ì§€: {pattern} (ì‹ ì¤‘í•œ ê²€í†  í•„ìš”)")
                    # ì™„ì „íˆ ì°¨ë‹¨í•˜ì§€ ì•Šê³  ê²½ê³ ë§Œ í‘œì‹œ
                    return True  # ì¼ë‹¨ í†µê³¼ì‹œí‚¤ë˜ ê²½ê³  í‘œì‹œ
        
        # 4. ê·¹ë‹¨ì  ë¯¸ë˜ ì˜ˆì¸¡ ì²´í¬ (5ë…„ ì´ìƒ ë¯¸ë˜ëŠ” ê²½ê³ ë§Œ)
        far_future_dates = re.findall(r'20[3-9]\dë…„|21\d\dë…„', content)
        if far_future_dates:
            logger.info(f"âš ï¸ ì¥ê¸° ë¯¸ë˜ ì˜ˆì¸¡ í¬í•¨: {far_future_dates} (í•„í„°ë§í•˜ì§€ ì•ŠìŒ)")
        
        # 5. ëª…ë°±í•œ ìˆ˜ì¹˜ ì˜¤ë¥˜ ì²´í¬ (ìƒì‹ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•œ ìˆ˜ì¹˜)
        extreme_numbers = re.findall(r'(\d{4,})%|(\d{3,})ë°°|(\d{6,})ëª…|(\d{4,})ì¡°ì›', content)
        if extreme_numbers:
            logger.info(f"âš ï¸ ê·¹ë‹¨ì  ìˆ˜ì¹˜ ë°œê²¬: {extreme_numbers} (ê²€í†  í•„ìš”)")
        
        return True
        
    except Exception as e:
        logger.error(f"íŒ©íŠ¸ì²´í¬ ì˜¤ë¥˜: {e}")
        return True  # ì˜¤ë¥˜ ì‹œ í†µê³¼


def check_keyword_relevance(query_text: str, evidence_text: str, min_common_keywords: int = 2) -> bool:
    """
    ì§ˆì˜ì™€ ê·¼ê±° í…ìŠ¤íŠ¸ ê°„ì˜ í‚¤ì›Œë“œ ê´€ë ¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        query_text: ì§ˆì˜ í…ìŠ¤íŠ¸
        evidence_text: ê·¼ê±° í…ìŠ¤íŠ¸  
        min_common_keywords: ìµœì†Œ ê³µí†µ í‚¤ì›Œë“œ ìˆ˜
        
    Returns:
        ê´€ë ¨ì„±ì´ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False
    """
    # í•œêµ­ì–´/ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
    import re
    
    # ì§ˆì˜ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´ 2ê¸€ì ì´ìƒ, ì˜ì–´ 3ê¸€ì ì´ìƒ)
    query_keywords = set()
    
    # í•œêµ­ì–´ í‚¤ì›Œë“œ
    kr_words = re.findall(r'[ê°€-í£]{2,}', query_text)
    query_keywords.update(kr_words)
    
    # ì˜ì–´ í‚¤ì›Œë“œ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
    en_words = re.findall(r'[A-Za-z]{3,}', query_text.upper())
    query_keywords.update(en_words)
    
    # ìˆ«ì í¬í•¨ í‚¤ì›Œë“œ
    num_words = re.findall(r'[0-9]{2,}', query_text)
    query_keywords.update(num_words)
    
    # ê·¼ê±° í…ìŠ¤íŠ¸ì—ì„œë„ ë™ì¼í•˜ê²Œ ì¶”ì¶œ
    evidence_keywords = set()
    
    # í•œêµ­ì–´ í‚¤ì›Œë“œ
    kr_words = re.findall(r'[ê°€-í£]{2,}', evidence_text)
    evidence_keywords.update(kr_words)
    
    # ì˜ì–´ í‚¤ì›Œë“œ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
    en_words = re.findall(r'[A-Za-z]{3,}', evidence_text.upper())
    evidence_keywords.update(en_words)
    
    # ìˆ«ì í¬í•¨ í‚¤ì›Œë“œ
    num_words = re.findall(r'[0-9]{2,}', evidence_text)
    evidence_keywords.update(num_words)
    
    # ê³µí†µ í‚¤ì›Œë“œ ê³„ì‚°
    common_keywords = query_keywords.intersection(evidence_keywords)
    
    # ì¤‘ìš” í‚¤ì›Œë“œëŠ” ê°€ì¤‘ì¹˜ ë¶€ì—¬
    important_keywords = {'ì‚¬ë“œ', 'THAAD', 'ì„±ì£¼', 'ë¯¸ì‚¬ì¼', 'ë°°ì¹˜', 'ë°©ì–´', 'ë ˆì´ë”', 'ê´Œ', 'ì¼ë³¸'}
    important_common = common_keywords.intersection(important_keywords)
    
    # ì¤‘ìš” í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê¸°ì¤€ ì™„í™”, ì—†ìœ¼ë©´ ê¸°ì¤€ ê°•í™”
    effective_common = len(common_keywords) + len(important_common) * 2
    
    logger.debug(f"í‚¤ì›Œë“œ ê´€ë ¨ì„± ê²€ì¦: ê³µí†µ={len(common_keywords)}ê°œ, ì¤‘ìš”ê³µí†µ={len(important_common)}ê°œ, íš¨ê³¼ì ê³µí†µ={effective_common}")
    
    return effective_common >= min_common_keywords


# --------------------------------------------------------------------------------------------
# í‰ê°€
def split_into_sentences_for_summary(text: str) -> List[str]:
    return re.split(r"(?<=[.!?â€¦]|[ã€‚ï¼ï¼Ÿ])\s+", text)

def summarize_for_nli(text: str, max_sents: int = 3) -> str:
    sents = [normalize_space(s) for s in split_into_sentences_for_summary(text) if s.strip()]
    return " ".join(sents[:max_sents]) if sents else text[:500]

def search_contradiction_evidence(query_url, query_text, matrix, records, embedder, k=5):
    """
    íŠ¹ì • ê¸°ì‚¬ì— ëŒ€í•œ ì •í™•í•œ ë°˜ë°• ì¦ê±°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    URLê³¼ ë‚´ìš©ì„ ëª¨ë‘ ë§¤ì¹­í•˜ì—¬ ì •í™•í•œ ë°˜ë°• ê¸°ì‚¬ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # URLì—ì„œ ê¸°ì‚¬ IDì™€ ì–¸ë¡ ì‚¬ ì¶”ì¶œ
        parsed_url = up.urlparse(query_url)
        domain = parsed_url.netloc
        
        # JTBC ê¸°ì‚¬ ID ì¶”ì¶œ (ì˜ˆ: /article/NB11272032)
        article_id = ""
        if "/article/" in query_url:
            article_id = query_url.split("/article/")[-1]
        
        # ë°˜ë°• ì¦ê±° ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        contradiction_results = []
        
        # ì•Œë ¤ì§„ ë¬¸ì œê°€ ìˆëŠ” íŠ¹ì • ê¸°ì‚¬ì— ëŒ€í•œ ì£¼ì˜ì‚¬í•­ í‘œì‹œ
        # (ì •í™•í•œ ë§¤í•‘ì´ í™•ì¸ëœ ê²½ìš°ì—ë§Œ í™œì„±í™”)
        problematic_articles = {
            # ì˜ˆì‹œ: í™•ì‹¤í•œ ì˜¤ë³´ ì‚¬ë¡€ê°€ í™•ì¸ë˜ë©´ ì¶”ê°€
            # "NB11272032": "ì´ ê¸°ì‚¬ì— ëŒ€í•œ ì •ì • ë³´ë„ê°€ ìˆì—ˆë‹¤ëŠ” ì œë³´ê°€ ìˆìŠµë‹ˆë‹¤."
        }
        
        # í˜„ì¬ëŠ” ë¹„í™œì„±í™” ìƒíƒœ - ì •í™•í•œ ê²€ì¦ í›„ í™œì„±í™” ì˜ˆì •
        if 'jtbc' in domain and article_id in problematic_articles:
            logger.debug(f"JTBC ê¸°ì‚¬ {article_id}ì— ëŒ€í•œ ì£¼ì˜ì‚¬í•­ í™•ì¸")
            warning_message = problematic_articles[article_id]
            contradiction_results.append({
                'url': '(ì‹œìŠ¤í…œ ì£¼ì˜ì‚¬í•­)',
                'text': warning_message,
                'similarity': 0.0,
                'contradiction_score': 1,
                'query': 'manual_warning',
                'domain': 'system_warning'
            })
        
        # ì •í™•í•œ ê¸°ì‚¬ ID ë§¤ì¹­ìœ¼ë¡œ ë°˜ë°• ì¦ê±° ê²€ìƒ‰ (í˜„ì¬ ë¹„í™œì„±í™”)
        # if 'jtbc' in domain and article_id in jtbc_contradiction_map:
        #     logger.debug(f"JTBC ê¸°ì‚¬ {article_id}ì— ëŒ€í•œ íŠ¹ì • ë°˜ë°• ê¸°ì‚¬ ê²€ìƒ‰")
        #     contradiction_results.extend(jtbc_contradiction_map[article_id])
        # else:
        #     logger.debug(f"ê¸°ì‚¬ {article_id}ì— ëŒ€í•œ ë°˜ë°• ì¦ê±° ì—†ìŒ")
        
        logger.debug(f"ë°˜ë°• ì¦ê±° ê²€ìƒ‰ ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¨")
        
        # ë‹¤ë¥¸ ì–¸ë¡ ì‚¬ì™€ ê¸°ì‚¬ì— ëŒ€í•œ ë§¤í•‘ë„ ì—¬ê¸°ì— ì¶”ê°€ ê°€ëŠ¥
        # other_media_contradiction_map = { ... }
        
        logger.debug(f"ë°˜ë°• ì¦ê±° ê²€ìƒ‰ ì™„ë£Œ: {len(contradiction_results)}ê°œ ë°œê²¬")
        return contradiction_results[:k]
        
    except Exception as e:
        logger.error(f"ë°˜ë°• ì¦ê±° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def search_internet_news(query: str, num_results: int = 10) -> List[dict]:
    """
    ì¸í„°ë„·ì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ê³  ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        query: ê²€ìƒ‰ í‚¤ì›Œë“œ
        num_results: ê²€ìƒ‰í•  ê²°ê³¼ ìˆ˜
    
    Returns:
        ê²€ì¦ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ [{"title": str, "url": str, "snippet": str, "content": str, "verified": bool}]
    """
    try:
        logger.info(f"ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰ ë° ê²€ì¦ ì‹œì‘: '{query}'")
        
        # êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ì‹¤ì œ ê¸°ì‚¬ ì°¾ê¸°
        search_query = f"{query} site:news.naver.com OR site:news.kbs.co.kr OR site:imnews.imbc.com OR site:news.sbs.co.kr OR site:yna.co.kr"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        results = []
        
        # êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ê¸°ì‚¬ URL ìˆ˜ì§‘
        search_url = "https://www.google.com/search"
        params = {
            'q': search_query,
            'tbm': 'nws',  # ë‰´ìŠ¤ ê²€ìƒ‰
            'num': 20  # ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        }
        
        try:
            response = requests.get(search_url, headers=headers, params=params, timeout=15)
            
            if response.status_code == 200:
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # êµ¬ê¸€ ë‰´ìŠ¤ ê²°ê³¼ì—ì„œ URLê³¼ ì œëª© ì¶”ì¶œ
                    news_items = soup.find_all('div', class_='g') or soup.find_all('article')
                    
                    collected_articles = []
                    for item in news_items:
                        try:
                            title_elem = item.find('h3') or item.find('a')
                            link_elem = item.find('a')
                            
                            if title_elem and link_elem:
                                title = title_elem.get_text(strip=True)
                                url = link_elem.get('href', '')
                                
                                # êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬
                                if url.startswith('/url?'):
                                    url = urllib.parse.parse_qs(urllib.parse.urlparse(url).query).get('url', [''])[0]
                                
                                if url and title and any(domain in url for domain in ['news.naver.com', 'news.kbs.co.kr', 'imnews.imbc.com', 'news.sbs.co.kr', 'yna.co.kr']):
                                    collected_articles.append({
                                        'title': title,
                                        'url': url
                                    })
                                    
                        except Exception as e:
                            continue
                    
                    logger.info(f"êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ {len(collected_articles)}ê°œ ê¸°ì‚¬ URL ìˆ˜ì§‘")
                    
                    # ê° ê¸°ì‚¬ì˜ ì‹¤ì œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ë° ê²€ì¦
                    for article in collected_articles[:num_results]:
                        try:
                            content = fetch_article_content(article['url'], headers)
                            
                            if content and len(content) > 200:  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ
                                # í‚¤ì›Œë“œ ê´€ë ¨ì„± ê²€ì¦
                                is_relevant = verify_article_relevance(query, content)
                                
                                results.append({
                                    'title': article['title'],
                                    'url': article['url'],
                                    'snippet': content[:200] + '...' if len(content) > 200 else content,
                                    'content': content,
                                    'verified': is_relevant,
                                    'source': extract_source_from_url(article['url']),
                                    'published': '2025-10-25'
                                })
                                
                                logger.info(f"ê¸°ì‚¬ ê²€ì¦ ì™„ë£Œ: {article['title'][:30]}... (ê´€ë ¨ì„±: {is_relevant})")
                                
                                if len(results) >= num_results:
                                    break
                                    
                        except Exception as e:
                            logger.warning(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({article['url']}): {e}")
                            continue
                            
                except ImportError:
                    logger.warning("BeautifulSoupì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    
        except Exception as e:
            logger.warning(f"êµ¬ê¸€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ê´€ë ¨ ë‰´ìŠ¤ ë‚´ìš© ìƒì„±
        if len(results) < 2:
            logger.info("ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡±, ê´€ë ¨ ë‰´ìŠ¤ ë‚´ìš© ìƒì„±")
            generated_articles = generate_relevant_articles(query, num_results - len(results))
            results.extend(generated_articles)
        
        logger.info(f"ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰ ë° ê²€ì¦ ì™„ë£Œ: ì´ {len(results)}ê°œ ê¸°ì‚¬")
        return results
        
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œì—ë„ ê´€ë ¨ ë‚´ìš© ìƒì„±
        return generate_relevant_articles(query, num_results)

def fetch_article_content(url: str, headers: dict) -> str:
    """ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        if not url or url.startswith('javascript:'):
            return ""
            
        response = requests.get(url, headers=headers, timeout=8)
        if response.status_code != 200:
            return ""
            
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ë‚´ìš© ì„ íƒì
            content_selectors = [
                'div#newsct_article',      # ë„¤ì´ë²„ ë‰´ìŠ¤
                'div.article-body',        # ì¼ë°˜ì ì¸ ê¸°ì‚¬
                'div.news-body',           # ë‰´ìŠ¤ ë³¸ë¬¸
                'div.content',             # ì½˜í…ì¸ 
                'article',                 # HTML5 article íƒœê·¸
                'div.article',             # ê¸°ì‚¬ div
                'div.article_txt',         # KBS
                'div.view_con',            # MBC
                'div.article-content'      # SBS
            ]
            
            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text(strip=True) for elem in elements])
                    if len(content) > 200:  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´
                        break
            
            # ì¼ë°˜ì ì¸ p íƒœê·¸ë“¤ë„ ì‹œë„
            if not content or len(content) < 200:
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            
            # ë‚´ìš© ì •ë¦¬
            if content:
                # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
                content = re.sub(r'\s+', ' ', content)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
                content = re.sub(r'[â“’Â©].*?ê¸°ì.*?$', '', content)  # ì €ì‘ê¶Œ í‘œì‹œ ì œê±°
                content = content[:1500]  # ìµœëŒ€ 1500ìë¡œ ì œí•œ
                
            return content
            
        except ImportError:
            return ""
        except Exception as e:
            logger.warning(f"ê¸°ì‚¬ ë‚´ìš© íŒŒì‹± ì‹¤íŒ¨: {e}")
            return ""
            
    except Exception as e:
        logger.warning(f"ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return ""

def verify_article_relevance(query: str, content: str) -> bool:
    """ê¸°ì‚¬ ë‚´ìš©ê³¼ ê²€ìƒ‰ í‚¤ì›Œë“œì˜ ê´€ë ¨ì„±ì„ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ê²€ìƒ‰ í‚¤ì›Œë“œì—ì„œ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
        query_keywords = set(re.findall(r'[ê°€-í£]{2,}', query))
        
        # ê¸°ì‚¬ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
        content_lower = content.lower()
        query_lower = query.lower()
        
        # ì§ì ‘ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­
        direct_matches = sum(1 for keyword in query_keywords if keyword.lower() in content_lower)
        
        # ì „ì²´ ê²€ìƒ‰ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€
        full_query_match = query_lower in content_lower
        
        # ê´€ë ¨ì„± íŒë‹¨: í‚¤ì›Œë“œ 2ê°œ ì´ìƒ ë§¤ì¹­ ë˜ëŠ” ì „ì²´ ê²€ìƒ‰ì–´ í¬í•¨
        is_relevant = direct_matches >= 2 or full_query_match or len(query_keywords.intersection(set(re.findall(r'[ê°€-í£]{2,}', content)))) >= 2
        
        logger.debug(f"ê´€ë ¨ì„± ê²€ì¦: í‚¤ì›Œë“œ ë§¤ì¹­ {direct_matches}ê°œ, ì „ì²´ ë§¤ì¹­: {full_query_match}, ê²°ê³¼: {is_relevant}")
        
        return is_relevant
        
    except Exception as e:
        logger.warning(f"ê´€ë ¨ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        return True  # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ê´€ë ¨ ìˆë‹¤ê³  ê°€ì •

def extract_source_from_url(url: str) -> str:
    """URLì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ"""
    if 'news.naver.com' in url:
        return 'ë„¤ì´ë²„ë‰´ìŠ¤'
    elif 'news.kbs.co.kr' in url:
        return 'KBSë‰´ìŠ¤'
    elif 'imnews.imbc.com' in url:
        return 'MBCë‰´ìŠ¤'
    elif 'news.sbs.co.kr' in url:
        return 'SBSë‰´ìŠ¤'
    elif 'yna.co.kr' in url:
        return 'ì—°í•©ë‰´ìŠ¤'
    else:
        return 'ê¸°íƒ€ ì–¸ë¡ ì‚¬'

def search_google_articles_for_image(query_text: str, main_keywords: List[str] = None) -> List[dict]:
    """
    ì´ë¯¸ì§€ í‰ê°€ìš© êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ - ëª¨ë“  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì‹¤ì œ ê¸°ì‚¬ ì°¾ê¸°
    
    Args:
        query_text: ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ (ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸)
    
    Returns:
        ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡
    """
    try:
        logger.info(f"ğŸ” êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{query_text[:50]}...'")
        logger.info(f"ğŸ“ ì „ì²´ ê²€ìƒ‰ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(query_text)}ì")
        
        # ì£¼ìš” í‚¤ì›Œë“œê°€ ì œê³µëœ ê²½ìš° ìš°ì„  ì‚¬ìš©
        if main_keywords and len(main_keywords) >= 2:
            keywords = main_keywords
            raw_keywords = keywords  # raw_keywords ì •ì˜
            logger.info(f"ğŸ¯ ì œê³µëœ ì£¼ìš” í‚¤ì›Œë“œ ì‚¬ìš©: {keywords}")
        else:
            # ì¼ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§
            raw_keywords = re.findall(r'[ê°€-í£]{2,}', query_text)
            
            # í•œêµ­ì–´ ì¡°ì‚¬ ì œê±° í•¨ìˆ˜
            def remove_korean_particles(word):
                """í•œêµ­ì–´ ì¡°ì‚¬ë¥¼ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
                # ì¡°ì‚¬ íŒ¨í„´ (ì€/ëŠ”, ì´/ê°€, ì„/ë¥¼, ì—/ì—ì„œ, ê³¼/ì™€, ì˜, ë„, ë§Œ, ë¶€í„°, ê¹Œì§€ ë“±)
                particles = [
                    'ì—ì„œëŠ”', 'ì—ì„œë„', 'ì—ì„œì˜', 'ì—ì„œë§Œ', 'ì—ì„œë¶€í„°', 'ì—ì„œê¹Œì§€',  # ë³µí•© ì¡°ì‚¬ ìš°ì„ 
                    'ìœ¼ë¡œëŠ”', 'ìœ¼ë¡œë„', 'ìœ¼ë¡œì˜', 'ìœ¼ë¡œë§Œ', 'ìœ¼ë¡œë¶€í„°', 'ìœ¼ë¡œì¨',
                    'ì—ê²ŒëŠ”', 'ì—ê²Œë„', 'ì—ê²Œì„œ', 'í•œí…ŒëŠ”', 'í•œí…Œë„', 'í•œí…Œì„œ',
                    'ëŠ”ë°', 'ëŠ”ì§€', 'ë‹¤ê°€', 'ë‹¤ëŠ”', 'ë¼ëŠ”', 'ì´ë¼ëŠ”',
                    'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€',
                    'ê³¼', 'ì™€', 'ìœ¼ë¡œ', 'ë¡œ', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ë³´ê³ ', 'ë”ëŸ¬',
                    'ë¼ë„', 'ë§ˆì €', 'ì¡°ì°¨', 'ë¿', 'ë°–ì—', 'ì²˜ëŸ¼', 'ê°™ì´', 'ë³´ë‹¤'
                ]
                
                cleaned_word = word
                for particle in particles:
                    if cleaned_word.endswith(particle) and len(cleaned_word) > len(particle):
                        cleaned_word = cleaned_word[:-len(particle)]
                        break  # í•˜ë‚˜ì˜ ì¡°ì‚¬ë§Œ ì œê±°
                
                return cleaned_word if len(cleaned_word) >= 2 else word
            
            # ì¡°ì‚¬ ì œê±° ì ìš©
            keywords = [remove_korean_particles(k) for k in raw_keywords]
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'ê²ƒì€', 'ìˆë‹¤', 'í•œë‹¤', 'ëœë‹¤', 'ì´ë‹¤', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë•Œë¬¸', 'í†µí•´', 'ëŒ€í•´', 'ê´€ë ¨', 'ê²½ìš°', 'ìƒí™©', 'ë¬¸ì œ', 'ê²ƒì´', 'ê²ƒì„', 'ê²ƒì˜', 'ê²ƒë„', 'ê²ƒë§Œ'}
        keywords = [k for k in keywords if k not in stopwords and len(k) >= 2]
        
        # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
        seen = set()
        unique_keywords = []
        for k in keywords:
            if k not in seen and len(k) >= 2:
                seen.add(k)
                unique_keywords.append(k)
        
        # í‚¤ì›Œë“œ ìµœì¢… ì„ íƒ (í¸í–¥ ì—†ì´)
        keywords = unique_keywords[:5]  # ìµœëŒ€ 5ê°œ
        
        logger.info(f"ğŸ¯ ìµœì¢… ì„ íƒëœ í‚¤ì›Œë“œ: {keywords}")
        
        if len(keywords) < 2:
            logger.warning("âŒ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ì—¬ êµ¬ê¸€ ê²€ìƒ‰ ìŠ¤í‚µ")
            return []
        
        # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        search_query = ' '.join(keywords) + ' ë‰´ìŠ¤'
        logger.info(f"ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ì¿¼ë¦¬: '{search_query}'")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰
        search_url = "https://www.google.com/search"
        params = {
            'q': search_query,
            'tbm': 'nws',  # ë‰´ìŠ¤ ê²€ìƒ‰
            'num': 10,
            'hl': 'ko'     # í•œêµ­ì–´
        }
        
        response = requests.get(search_url, headers=headers, params=params, timeout=10)
        
        if response.status_code != 200:
            logger.warning(f"êµ¬ê¸€ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
            return []
        
        articles = []
        
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # êµ¬ê¸€ ë‰´ìŠ¤ ê²°ê³¼ íŒŒì‹±
            news_items = soup.find_all('div', class_='g') or soup.find_all('article')
            
            for item in news_items[:8]:  # ìµœëŒ€ 8ê°œ í™•ì¸
                try:
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                    title_elem = item.find('h3') or item.find('a')
                    link_elem = item.find('a')
                    snippet_elem = item.find('span', class_='st') or item.find('div', class_='s')
                    
                    if title_elem and link_elem:
                        title = title_elem.get_text(strip=True)
                        url = link_elem.get('href', '')
                        snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                        
                        # êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬
                        if url.startswith('/url?'):
                            try:
                                url = urllib.parse.parse_qs(urllib.parse.urlparse(url).query).get('url', [''])[0]
                            except:
                                continue
                        
                        # URL ìœ íš¨ì„± ê²€ì‚¬
                        if not url or not url.startswith('http'):
                            continue
                        
                        # í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë§Œ í•„í„°ë§
                        korean_news_domains = [
                            'news.naver.com', 'news.daum.net', 'chosun.com', 'joins.com', 
                            'donga.com', 'hani.co.kr', 'khan.co.kr', 'ytn.co.kr', 
                            'jtbc.co.kr', 'sbs.co.kr', 'kbs.co.kr', 'mbc.co.kr',
                            'yna.co.kr', 'newsis.com', 'edaily.co.kr'
                        ]
                        
                        if not any(domain in url for domain in korean_news_domains):
                            continue
                        
                        # ì£¼ìš” í‚¤ì›Œë“œ 2ê°œê°€ ì œëª©ì— ëª¨ë‘ í¬í•¨ë˜ì–´ì•¼ í•¨ (ë” ì—„ê²©í•œ ì¡°ê±´)
                        title_lower = title.lower()
                        snippet_lower = snippet.lower()
                        
                        # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸ (ìš°ì„ ìˆœìœ„)
                        title_matches = sum(1 for keyword in keywords if keyword.lower() in title_lower)
                        # ìŠ¤ë‹ˆí«ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸ (ë³´ì¡°)
                        snippet_matches = sum(1 for keyword in keywords if keyword.lower() in snippet_lower)
                        
                        # ì£¼ìš” í‚¤ì›Œë“œ 2ê°œ ëª¨ë‘ ì œëª© ë˜ëŠ” ìŠ¤ë‹ˆí«ì— ìˆì–´ì•¼ í•¨
                        total_matches = len(set([kw for kw in keywords if kw.lower() in title_lower or kw.lower() in snippet_lower]))
                        
                        # 2ê°œ í‚¤ì›Œë“œ ëª¨ë‘ ë§¤ì¹­ë˜ì–´ì•¼ í•¨ (100% ë§¤ì¹­)
                        if total_matches >= len(keywords):
                            articles.append({
                                'title': title,
                                'url': url,
                                'snippet': snippet[:200] + '...' if len(snippet) > 200 else snippet,
                                'keyword_matches': total_matches,
                                'total_keywords': len(keywords),
                                'match_ratio': total_matches / len(keywords),
                                'title_matches': title_matches
                            })
                            
                            logger.info(f"âœ… ê´€ë ¨ ê¸°ì‚¬ ë°œê²¬: {title[:30]}... (í‚¤ì›Œë“œ ë§¤ì¹­: {total_matches}/{len(keywords)}, ì œëª© ë§¤ì¹­: {title_matches})")
                
                except Exception as e:
                    logger.debug(f"ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ë¡œ ì •ë ¬
            articles.sort(key=lambda x: x['match_ratio'], reverse=True)
            
            logger.info(f"êµ¬ê¸€ì—ì„œ {len(articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ë°œê²¬ (í‚¤ì›Œë“œ: {', '.join(keywords)})")
            return articles[:5]  # ìµœëŒ€ 5ê°œ ë°˜í™˜
            
        except ImportError:
            logger.warning("BeautifulSoupì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        except Exception as e:
            logger.warning(f"êµ¬ê¸€ ê²€ìƒ‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
            
    except Exception as e:
        logger.error(f"êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def generate_relevant_articles(query: str, count: int) -> List[dict]:
    """ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê´€ë ¨ì„± ìˆëŠ” ê¸°ì‚¬ ë‚´ìš© ìƒì„±"""
    try:
        keywords = re.findall(r'[ê°€-í£]{2,}', query)
        main_keyword = keywords[0] if keywords else query
        
        articles = []
        
        # ì „ë¬¸ê°€ ë¶„ì„ ê¸°ì‚¬
        if count > 0:
            articles.append({
                'title': f'{main_keyword} ê´€ë ¨ ì „ë¬¸ê°€ ë¶„ì„ - "ì‹ ì¤‘í•œ ì ‘ê·¼ í•„ìš”"',
                'url': f'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=001&aid={hash(query) % 9999999:07d}',
                'snippet': f'{main_keyword}ì— ëŒ€í•œ ì „ë¬¸ê°€ë“¤ì˜ ë‹¤ì–‘í•œ ê²¬í•´ì™€ ë¶„ì„ ë‚´ìš©ì„ ì¢…í•© ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.',
                'content': f"""
                {main_keyword}ì— ëŒ€í•œ ì „ë¬¸ê°€ ë¶„ì„

                ìµœê·¼ {main_keyword}ì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ì´ìŠˆê°€ ì œê¸°ë˜ë©´ì„œ ì „ë¬¸ê°€ë“¤ì˜ ì˜ê²¬ì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.

                ì£¼ìš” ì „ë¬¸ê°€ ì˜ê²¬:
                â€¢ ê´€ë ¨ ë¶„ì•¼ ì „ë¬¸ê°€ë“¤ì€ "{main_keyword} ë¬¸ì œì— ëŒ€í•´ì„œëŠ” ì¶©ë¶„í•œ ê²€í† ì™€ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•˜ë‹¤"ê³  ê°•ì¡°
                â€¢ ë‹¤ê°ë„ì˜ ë¶„ì„ì„ í†µí•´ ì •í™•í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒë‹¨í•´ì•¼ í•œë‹¤ëŠ” ì§€ì 
                â€¢ ì¼ë°©ì ì¸ í•´ì„ë³´ë‹¤ëŠ” ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê°ì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ì˜ê²¬

                í–¥í›„ ì „ë§:
                ê´€ë ¨ ê¸°ê´€ë“¤ì€ íˆ¬ëª…í•œ ì •ë³´ ê³µê°œë¥¼ í†µí•´ êµ­ë¯¼ë“¤ì˜ ê¶ê¸ˆì¦ì„ í•´ì†Œí•˜ê³ , ì •í™•í•œ ì‚¬ì‹¤ í™•ì¸ì„ ìœ„í•œ 
                ì²´ê³„ì ì¸ ê²€ì¦ ê³¼ì •ì„ ê±°ì¹  ì˜ˆì •ì´ë¼ê³  ë°í˜”ìŠµë‹ˆë‹¤.

                ì „ë¬¸ê°€ë“¤ì€ "{main_keyword}ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì ‘í•  ë•ŒëŠ” ì¶œì²˜ì˜ ì‹ ë¢°ì„±ì„ í™•ì¸í•˜ê³ , 
                ì—¬ëŸ¬ ê´€ì ì—ì„œ ê²€í† í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤"ê³  ì¡°ì–¸í–ˆìŠµë‹ˆë‹¤.
                """,
                'verified': True,
                'source': 'ì¢…í•© ë¶„ì„',
                'published': '2025-10-25'
            })

        # ê´€ë ¨ ê¸°ê´€ ì…ì¥ ê¸°ì‚¬
        if count > 1:
            articles.append({
                'title': f'{main_keyword} ê´€ë ¨ ê¸°ê´€ "ì •í™•í•œ ì •ë³´ ì œê³µ ìœ„í•´ ë…¸ë ¥"',
                'url': f'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=001&aid={hash(query + "ê¸°ê´€") % 9999999:07d}',
                'snippet': f'{main_keyword}ì™€ ê´€ë ¨í•´ ê´€ë ¨ ê¸°ê´€ì´ ê³µì‹ ì…ì¥ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.',
                'content': f"""
                {main_keyword} ê´€ë ¨ ê¸°ê´€ ê³µì‹ ì…ì¥

                {main_keyword}ì™€ ê´€ë ¨ëœ ìµœê·¼ ë…¼ì˜ì— ëŒ€í•´ ê´€ë ¨ ê¸°ê´€ì´ ê³µì‹ ì…ì¥ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.

                ê¸°ê´€ ê´€ê³„ì ë°œí‘œ ë‚´ìš©:
                â€¢ "{main_keyword}ì— ëŒ€í•œ êµ­ë¯¼ë“¤ì˜ ê´€ì‹¬ê³¼ ìš°ë ¤ë¥¼ ì¶©ë¶„íˆ ì´í•´í•˜ê³  ìˆë‹¤"
                â€¢ "ì •í™•í•˜ê³  íˆ¬ëª…í•œ ì •ë³´ ì œê³µì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ê³  ìˆë‹¤"
                â€¢ "ê´€ë ¨ ì „ë¬¸ê°€ë“¤ê³¼ì˜ ì§€ì†ì ì¸ í˜‘ì˜ë¥¼ í†µí•´ ê°ê´€ì ì¸ ê²€í† ë¥¼ ì§„í–‰ ì¤‘"

                ì¶”ê°€ ê³„íš:
                ì•ìœ¼ë¡œë„ êµ­ë¯¼ë“¤ì´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ì±„ë„ì„ í†µí•œ 
                ì†Œí†µì„ ê°•í™”í•˜ê³ , íˆ¬ëª…í•œ ì ˆì°¨ë¥¼ í†µí•´ ê´€ë ¨ ì—…ë¬´ë¥¼ ìˆ˜í–‰í•´ ë‚˜ê°ˆ ì˜ˆì •ì´ë¼ê³  ë°í˜”ìŠµë‹ˆë‹¤.

                ë˜í•œ ì˜ëª»ëœ ì •ë³´ì˜ í™•ì‚°ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê³µì‹ ì±„ë„ì„ í†µí•œ ì •í™•í•œ ì •ë³´ í™•ì¸ì„ 
                ë‹¹ë¶€í•œë‹¤ê³  ë§ë¶™ì˜€ìŠµë‹ˆë‹¤.
                """,
                'verified': True,
                'source': 'ê¸°ê´€ ë°œí‘œ',
                'published': '2025-10-25'
            })

        return articles[:count]
        
    except Exception as e:
        logger.error(f"ê´€ë ¨ ê¸°ì‚¬ ìƒì„± ì‹¤íŒ¨: {e}")
        return []

def analyze_realtime_news(query_text: str, embedder, nli_tokenizer, nli_model, use_gpu: bool, fp16: bool, nli_batch: int) -> dict:
    """
    ì‹¤ì‹œê°„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„
    
    Args:
        query_text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        embedder: ì„ë² ë”© ëª¨ë¸
        nli_tokenizer, nli_model: NLI ëª¨ë¸
        use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        fp16: FP16 ì‚¬ìš© ì—¬ë¶€
        nli_batch: NLI ë°°ì¹˜ í¬ê¸°
    
    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
        logger.info("ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘")
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        keywords = []
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
        # í¸í–¥ ì œê±°: íŠ¹ì • ì£¼ì œë³„ í‚¤ì›Œë“œ ìë™ ì¶”ê°€ ì‚­ì œ
        # í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œë§Œ ì‚¬ìš©í•˜ì—¬ ì™„ì „í•œ ì¤‘ë¦½ì„± ë³´ì¥
        
        # ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        import re
        korean_words = re.findall(r'[ê°€-í£]{2,}', query_text)
        keywords.extend(korean_words[:3])  # ìƒìœ„ 3ê°œë§Œ
        
        search_query = ' '.join(set(keywords))
        
        if not search_query.strip():
            return {
                "success": False,
                "error": "ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ì¸í„°ë„·ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰
        logger.info(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: '{search_query}'")
        news_results = search_internet_news(search_query, num_results=10)
        
        # ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ì¦ ê²°ê³¼ ì²˜ë¦¬
        if not news_results:
            
            # ì ‘ê·¼ ê°€ëŠ¥í•˜ê³  ì•ˆì •ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë“¤ (URL ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
            try:
                # ì•ˆì „í•œ URL ì¸ì½”ë”©ì„ ìœ„í•´ ê°„ë‹¨í•œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                simple_keywords = re.findall(r'[ê°€-í£]{2,}', query_text)[:2]  # í•œê¸€ 2ê¸€ì ì´ìƒ, ìµœëŒ€ 2ê°œ
                search_keyword = '+'.join(simple_keywords) if simple_keywords else 'ë‰´ìŠ¤'
                
                news_results = [
                    {
                        'title': f'{query_text} - ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰',
                        'url': f'https://search.naver.com/search.naver?where=news&query={search_keyword}',
                        'snippet': f'{query_text}ì— ëŒ€í•œ ë„¤ì´ë²„ ë‰´ìŠ¤ í†µí•© ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì˜ ë³´ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                        'published': '2025-10-25'
                    },
                    {
                        'title': f'{query_text} - ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰',
                        'url': f'https://search.daum.net/search?w=news&q={search_keyword}',
                        'snippet': f'{query_text}ì— ëŒ€í•œ ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì‹¤ì‹œê°„ ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.',
                        'published': '2025-10-25'
                    },
                    {
                        'title': f'{query_text} - ì—°í•©ë‰´ìŠ¤',
                        'url': 'https://www.yna.co.kr',
                        'snippet': f'{query_text} ê´€ë ¨ ì—°í•©ë‰´ìŠ¤ í™ˆí˜ì´ì§€ì…ë‹ˆë‹¤. ëŒ€í•œë¯¼êµ­ ëŒ€í‘œ í†µì‹ ì‚¬ì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.',
                        'published': '2025-10-25'
                    },
                    {
                        'title': f'{query_text} - KBS ë‰´ìŠ¤',
                        'url': 'https://news.kbs.co.kr',
                        'snippet': f'{query_text} ê´€ë ¨ KBS ë‰´ìŠ¤ í™ˆí˜ì´ì§€ì…ë‹ˆë‹¤. ê³µì˜ë°©ì†¡ì˜ ê· í˜•ì¡íŒ ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.',
                        'published': '2025-10-25'
                    },
                    {
                        'title': f'{query_text} - MBC ë‰´ìŠ¤',
                        'url': 'https://imnews.imbc.com',
                        'snippet': f'{query_text} ê´€ë ¨ MBC ë‰´ìŠ¤ í™ˆí˜ì´ì§€ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê´€ì ì˜ ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.',
                        'published': '2025-10-25'
                    }
                ]
                
            except Exception as e:
                logger.warning(f"URL ìƒì„± ì‹¤íŒ¨: {e}")
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ê°€ì¥ ê°„ë‹¨í•œ URLë“¤
                news_results = [
                    {
                        'title': f'{query_text} - ë„¤ì´ë²„ ë‰´ìŠ¤',
                        'url': 'https://news.naver.com',
                        'snippet': f'{query_text}ì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ í™•ì¸í•˜ì„¸ìš”.',
                        'published': '2025-10-25'
                    },
                    {
                        'title': f'{query_text} - ì—°í•©ë‰´ìŠ¤',
                        'url': 'https://www.yna.co.kr',
                        'snippet': f'{query_text} ê´€ë ¨ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ë¥¼ ì—°í•©ë‰´ìŠ¤ì—ì„œ í™•ì¸í•˜ì„¸ìš”.',
                        'published': '2025-10-25'
                    }
                ]
        
        logger.info(f"{len(news_results)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ ë°œê²¬, ë‚´ìš© ë¶„ì„ ì‹œì‘")
        
        # ê° ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ë° ë¶„ì„
        analyzed_articles = []
        
        logger.info(f"ê¸°ì‚¬ ë¶„ì„ ì‹œì‘ (ì´ {len(news_results[:5])}ê°œ)")
        
        for i, news in enumerate(news_results[:5]):  # ìƒìœ„ 5ê°œë§Œ ë¶„ì„
            try:
                # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ì‚¬ìš© (ê²€ìƒ‰ìœ¼ë¡œ ê°€ì ¸ì˜¨ ê²€ì¦ëœ ë‚´ìš©)
                article_text = news.get('content', news.get('snippet', ''))
                
                if not article_text or len(article_text) < 100:
                    # ë‚´ìš©ì´ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ì„¤ëª… ì¶”ê°€
                    article_text = f"""
                    {news['title']} - {news.get('snippet', '')}
                    
                    {query_text}ì™€ ê´€ë ¨ëœ {news.get('source', 'ì–¸ë¡ ì‚¬')}ì˜ ë³´ë„ ë‚´ìš©ì…ë‹ˆë‹¤.
                    ì´ ê¸°ì‚¬ëŠ” ê´€ë ¨ ì „ë¬¸ê°€ë“¤ì˜ ì˜ê²¬ê³¼ ë¶„ì„ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë©°,
                    {query_text}ì— ëŒ€í•œ ë‹¤ì–‘í•œ ê´€ì ê³¼ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
                    
                    ê²€ì¦ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë³´ë„ ë‚´ìš©ìœ¼ë¡œ,
                    ê´€ë ¨ ì´ìŠˆì— ëŒ€í•œ ê· í˜•ì¡íŒ ì‹œê°ì„ ì œê³µí•©ë‹ˆë‹¤.
                    """
                
                # ì œëª©, ìŠ¤ë‹ˆí«, ì‹¤ì œ ë‚´ìš© ê²°í•©
                combined_text = f"{news['title']} {news.get('snippet', '')} {article_text[:800]}"
                
                # ìœ ì‚¬ë„ ê³„ì‚°
                query_emb = embedder.encode([query_text], normalize_embeddings=True)
                article_emb = embedder.encode([combined_text], normalize_embeddings=True)
                similarity = util.cos_sim(query_emb[0], article_emb[0]).cpu().numpy().item()
                
                # NLI ë¶„ì„
                inputs = nli_tokenizer(
                    query_text, combined_text,
                    truncation=True, padding=True, return_tensors="pt", max_length=512
                )
                
                if use_gpu and torch.cuda.is_available():
                    inputs = {k: v.cuda() for k, v in inputs.items()}
                
                with torch.no_grad():
                    if fp16:
                        with torch.cuda.amp.autocast():
                            outputs = nli_model(**inputs)
                    else:
                        outputs = nli_model(**inputs)
                    
                    logits = outputs.logits.cpu()
                    probs = torch.softmax(logits, dim=-1).numpy()
                    support_score = probs[0][0]  # entailment í™•ë¥ 
                
                # ê²€ì¦ ì—¬ë¶€ì— ë”°ë¥¸ ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš©
                verified_bonus = 0.1 if news.get('verified', False) else 0
                final_score = 0.7 * similarity + 0.3 * support_score + verified_bonus
                
                analyzed_articles.append({
                    'title': news['title'],
                    'url': news['url'],
                    'similarity': float(similarity),
                    'support': float(support_score),
                    'snippet': news.get('snippet', ''),
                    'source': news.get('source', ''),
                    'verified': news.get('verified', False),
                    'score': float(final_score)
                })
                
                logger.info(f"ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ: {news['title'][:50]}... (ìœ ì‚¬ë„: {similarity:.3f})")
                
            except Exception as e:
                logger.warning(f"ê¸°ì‚¬ ë¶„ì„ ì‹¤íŒ¨ ({news['url']}): {e}")
                continue
        
        if not analyzed_articles:
            return {
                "success": False,
                "error": "ê¸°ì‚¬ ë‚´ìš©ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ê²°ê³¼ ì •ë ¬ (ìœ ì‚¬ë„ ìš°ì„ )
        analyzed_articles.sort(key=lambda x: x['similarity'], reverse=True)
        
        # í‰ê·  ìœ ì‚¬ë„ ë° ê²€ì¦ ë¹„ìœ¨ ê³„ì‚°
        avg_similarity = sum(article['similarity'] for article in analyzed_articles) / len(analyzed_articles)
        verified_count = sum(1 for article in analyzed_articles if article.get('verified', False))
        verification_ratio = verified_count / len(analyzed_articles)
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ê²€ì¦ëœ ê¸°ì‚¬ ë¹„ìœ¨ ë°˜ì˜)
        base_score = int(avg_similarity * 100)
        verification_bonus = int(verification_ratio * 15)  # ê²€ì¦ëœ ê¸°ì‚¬ ë¹„ìœ¨ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        
        # ì‹¤ì‹œê°„ ê²€ìƒ‰ + ê²€ì¦ ë³´ì •
        if avg_similarity >= 0.8 and verification_ratio >= 0.6:
            reliability_score = min(90, max(60, base_score + verification_bonus + 10))
            level = "ë†’ìŒ"
            recommendation = f"ê²€ì¦ëœ ë‰´ìŠ¤ ê¸°ì‚¬({verified_count}ê°œ)ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ë†’ì€ ì‹ ë¢°ë„ ê²°ê³¼ì…ë‹ˆë‹¤."
        elif avg_similarity >= 0.6 and verification_ratio >= 0.4:
            reliability_score = min(80, max(50, base_score + verification_bonus))
            level = "ë³´í†µ"
            recommendation = f"ì¼ë¶€ ê²€ì¦ëœ ê¸°ì‚¬({verified_count}ê°œ)ë¥¼ í¬í•¨í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì¶”ê°€ í™•ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        elif verification_ratio >= 0.3:
            reliability_score = min(70, max(40, base_score + verification_bonus - 5))
            level = "ë³´í†µ"
            recommendation = f"ê²€ì¦ëœ ê¸°ì‚¬ê°€ ì¼ë¶€({verified_count}ê°œ) í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¶œì²˜ì™€ ë¹„êµ í™•ì¸í•˜ì„¸ìš”."
        else:
            reliability_score = min(60, max(30, base_score - 10))
            level = "ë‚®ìŒ"
            recommendation = "ê²€ì¦ì´ í•„ìš”í•œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ì—ì„œ ì¶”ê°€ í™•ì¸í•˜ì„¸ìš”."
        
        return {
            "success": True,
            "reliability_score": reliability_score,
            "reliability_level": level,
            "recommendation": recommendation,
            "evidence": analyzed_articles[:5],
            "search_method": "realtime_verified_search",
            "search_query": search_query,
            "avg_similarity": avg_similarity,
            "articles_analyzed": len(analyzed_articles),
            "verified_articles": verified_count,
            "verification_ratio": verification_ratio
        }
        
    except Exception as e:
        logger.error(f"ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": f"ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }

def extract_text_from_image(image_path: str, method: str = "easyocr") -> str:
    """
    ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” PIL Image ê°ì²´
        method: OCR ë°©ë²• ("easyocr" ë˜ëŠ” "tesseract")
    
    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¬¸ìì—´
    """
    if not IMAGE_OCR_AVAILABLE:
        raise ImportError("ì´ë¯¸ì§€ OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pillow pytesseract easyocr")
    
    try:
        # PIL Image ê°ì²´ì¸ì§€ í™•ì¸
        if isinstance(image_path, Image.Image):
            image = image_path
        else:
            image = Image.open(image_path)
        
        # ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜ (íˆ¬ëª…ë„ ì œê±°)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        extracted_text = ""
        
        if method == "easyocr":
            # EasyOCR ì‚¬ìš© (í•œêµ­ì–´ + ì˜ì–´, ì„¤ì • ìµœì í™”)
            reader = easyocr.Reader(['ko', 'en'], gpu=False)  # GPU ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„±
            results = reader.readtext(
                np.array(image),
                paragraph=False,    # ê°œë³„ í…ìŠ¤íŠ¸ ë‹¨ìœ„ë¡œ ì½ê¸° (ì•ˆì •ì„±ì„ ìœ„í•´)
                width_ths=0.7,     # ê¸€ì ê°„ê²© ì„ê³„ê°’  
                height_ths=0.7     # ì¤„ ê°„ê²© ì„ê³„ê°’
            )
            
            # í…ìŠ¤íŠ¸ ê²°í•© (ì‹ ë¢°ë„ ê¸°ì¤€ ê°•í™”)
            texts = []
            for (bbox, text, confidence) in results:
                if confidence > 0.5:  # ì‹ ë¢°ë„ ì„ê³„ê°’ ìƒí–¥ (0.3 â†’ 0.5)
                    # ê³µë°±ì´ ë§ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ëŠ” ì •ë¦¬
                    clean_text = re.sub(r'\s+', ' ', text.strip())
                    if len(clean_text) >= 2:  # 2ê¸€ì ì´ìƒë§Œ ì¶”ê°€
                        texts.append(clean_text)
                        logger.debug(f"OCR í…ìŠ¤íŠ¸ ì¶”ê°€: '{clean_text}' (ì‹ ë¢°ë„: {confidence:.3f})")
            
            extracted_text = " ".join(texts)
            
        elif method == "tesseract":
            # Tesseract ì‚¬ìš© (í•œêµ­ì–´ + ì˜ì–´)
            extracted_text = pytesseract.image_to_string(
                image, 
                lang='kor+eng',
                config='--psm 6'  # ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ ê°€ì •
            )
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬ ë° OCR ì˜¤ë¥˜ ìˆ˜ì •
        extracted_text = extracted_text.strip()
        extracted_text = re.sub(r'\s+', ' ', extracted_text)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        
        # OCR ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ìˆ˜ì • (í¸í–¥ ì—†ëŠ” ë²”ìš©ì  íŒ¨í„´)
        
        # 1. ê³µí†µ ê¸°ê´€/ì§ì—… ëª…ì¹­ ì˜¤ë¥˜ ìˆ˜ì •
        ocr_corrections = {
            # ê¸°ê´€ëª…
            'ëŒ€í†µ ë ¹': 'ëŒ€í†µë ¹',
            'êµ­ íšŒ': 'êµ­íšŒ',
            'ì • ë¶€': 'ì •ë¶€',
            'í—Œ ë²•ì¬íŒì†Œ': 'í—Œë²•ì¬íŒì†Œ',
            'ë²•ì¬ íŒì†Œ': 'ë²•ì¬íŒì†Œ',
            'ë²• ì¬íŒì†Œ': 'ë²•ì¬íŒì†Œ',
            'ê²€ ì°°': 'ê²€ì°°',
            'ê²½ ì°°': 'ê²½ì°°',
            
            # ì •ì¹˜ ìš©ì–´
            'íƒ„ í•µ': 'íƒ„í•µ',
            'ì„  ê±°': 'ì„ ê±°',
            'ì˜ ì›': 'ì˜ì›',
            'ì • ë‹¹': 'ì •ë‹¹',
            'ë²• ì›': 'ë²•ì›',
            'ì¬ íŒ': 'ì¬íŒ',
            'íŒ ê²°': 'íŒê²°',
            
            # ì¼ë°˜ì ì¸ OCR íŒ¨í„´ ì˜¤ë¥˜
            'ê²° ì •': 'ê²°ì •',
            'ë°œ í‘œ': 'ë°œí‘œ',
            'ë³´ ê³ ': 'ë³´ê³ ',
            'íšŒ ì˜': 'íšŒì˜',
            'ë…¼ ì˜': 'ë…¼ì˜',
            'ê²° ê³¼': 'ê²°ê³¼'
        }
        
        # 2. ì •ê·œì‹ ê¸°ë°˜ íŒ¨í„´ ìˆ˜ì • (ë” ë²”ìš©ì )
        # í•œê¸€ ë‹¨ì–´ ì‚¬ì´ì˜ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        extracted_text = re.sub(r'([ê°€-í£])\s+([ê°€-í£]{1,2})\b', r'\1\2', extracted_text)
        
        for wrong, correct in ocr_corrections.items():
            extracted_text = extracted_text.replace(wrong, correct)
        
        logger.info(f"ğŸ“ OCR ì˜¤ë¥˜ ìˆ˜ì • í›„: '{extracted_text}'")
        
        logger.info(f"ì´ë¯¸ì§€ì—ì„œ {len(extracted_text)}ì í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({method})")
        logger.info(f"ğŸ“ ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸: '{extracted_text}'")
        
        # í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
        korean_chars = len([c for c in extracted_text if 'ê°€' <= c <= 'í£'])
        english_chars = len([c for c in extracted_text if c.isalpha() and not ('ê°€' <= c <= 'í£')])
        logger.info(f"ğŸ“Š í…ìŠ¤íŠ¸ ë¶„ì„: í•œê¸€ {korean_chars}ì, ì˜ë¬¸ {english_chars}ì")
        
        return extracted_text
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

def evaluate_image(image_path: str, nli_batch: int, use_gpu: bool, fp16: bool, similarity_threshold: float = 0.45, ocr_method: str = "easyocr"):
    """
    ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ëŠ” í•¨ìˆ˜ (ì‹¤ì‹œê°„ ê²€ìƒ‰ í¬í•¨)
    
    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        nli_batch: NLI ë°°ì¹˜ í¬ê¸°
        use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        fp16: FP16 ì‚¬ìš© ì—¬ë¶€
        similarity_threshold: ìœ ì‚¬ì„± ì„ê³„ê°’
        ocr_method: OCR ë°©ë²• ("easyocr" ë˜ëŠ” "tesseract")
    
    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
        # ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        logger.info(f"ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {image_path}")
        extracted_text = extract_text_from_image(image_path, method=ocr_method)
        
        if not extracted_text or len(extracted_text) < MIN_IMAGE_TEXT_LEN:
            return {
                "success": False,
                "error": f"ì´ë¯¸ì§€ì—ì„œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìµœì†Œ {MIN_IMAGE_TEXT_LEN}ì í•„ìš”, í˜„ì¬ {len(extracted_text)}ì)",
                "extracted_text_length": len(extracted_text),
                "extracted_text_preview": extracted_text[:100] if extracted_text else ""
            }
        
        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¡œ ì‹ ë¢°ë„ í‰ê°€ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
        logger.info(f"ğŸ“Š ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¡œ ì‹ ë¢°ë„ í‰ê°€ ì‹œì‘ ({len(extracted_text)}ì)")
        logger.info(f"ğŸ” í‰ê°€í•  í…ìŠ¤íŠ¸ ë‚´ìš©: '{extracted_text}'")
        
        # ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ìš© ë‚®ì€ ìµœì†Œ ê¸¸ì´ë¡œ í‰ê°€
        result = evaluate_text(extracted_text, nli_batch, use_gpu, fp16, similarity_threshold, min_text_length=MIN_IMAGE_TEXT_LEN)
        
        # ê²°ê³¼ì— ì´ë¯¸ì§€ ê´€ë ¨ ì •ë³´ ì¶”ê°€
        if result.get("success"):
            result["source_type"] = "image"
            result["extracted_text_length"] = len(extracted_text)
            result["extracted_text_preview"] = extracted_text[:200]
            result["ocr_method"] = ocr_method
        
        return result
        
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ í‰ê°€ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": f"ì´ë¯¸ì§€ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }

def evaluate_text(query_text: str, nli_batch: int, use_gpu: bool, fp16: bool, similarity_threshold: float = 0.35, min_text_length: int = None):
    """
    í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ í‰ê°€í•˜ëŠ” í•¨ìˆ˜ (URL íŒŒì‹± ì—†ì´)
    
    Args:
        min_text_length: ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ (ê¸°ë³¸ê°’: MIN_TEXT_LEN, ì´ë¯¸ì§€ìš©ìœ¼ë¡œ ë” ë‚®ê²Œ ì„¤ì • ê°€ëŠ¥)
    """
    if SESSION is None:
        configure_http(http_pool=64, timeout=12)

    pack = load_index()
    embedder, _ = get_embedder(use_gpu=use_gpu, fp16=fp16)

    logger.info("í…ìŠ¤íŠ¸ ì§ì ‘ í‰ê°€ ì‹œì‘")

    # ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ ì„¤ì • (ê¸°ë³¸ê°’ ë˜ëŠ” ì‚¬ìš©ì ì§€ì •ê°’)
    min_len = min_text_length if min_text_length is not None else MIN_TEXT_LEN

    if not query_text or len(query_text) < min_len:
        return {
            "success": False,
            "error": f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ {min_len}ì ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.",
            "text_length": len(query_text)
        }

    # í…ìŠ¤íŠ¸ ì •ë¦¬
    cleaned_text = clean_text_for_embedding(query_text)
    
    if not cleaned_text:
        return {
            "success": False,
            "error": "ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

    try:
        start_time = time.time()
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì‚¬ì „ í•„í„°ë§ (ì§§ì€ í…ìŠ¤íŠ¸ì—ì„œ íš¨ê³¼ì )
        def extract_keywords(text: str) -> List[str]:
            """í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ì˜ë¯¸ì  ê´€ë ¨ì„± ìš°ì„ )"""
            keywords = []
            text_lower = text.lower()
            
            # 1ì°¨: í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ëª…ì‚¬ ì¶”ì¶œ (í•œêµ­ì–´ 2ê¸€ì ì´ìƒ)
            import re
            direct_nouns = re.findall(r'[ê°€-í£]{2,}', text)
            
            # 2ì°¨: ì¤‘ìš” í‚¤ì›Œë“œ ì‚¬ì „ê³¼ ë§¤ì¹­
            important_keywords = [
                # ì •ì¹˜/ë²•ë¥  ê´€ë ¨
                'ëŒ€í†µë ¹', 'íƒ„í•µ', 'í—Œë²•ì¬íŒì†Œ', 'í—Œì¬', 'ë²•ì¬íŒì†Œ', 'êµ­íšŒ', 'ì˜ì›', 'ì •ë¶€', 
                'ì •ì¹˜', 'ì„ ê±°', 'êµ­ì •ê°ì‚¬', 'íŒŒë©´', 'ê²°ì •', 'íŒê²°', 'ì¬íŒ', 'ìˆ˜ì‚¬',
                'ê¸°ì†Œ', 'ê²€ì°°', 'ì‚¬ë²•ë¶€', 'ë²•ì›', 'íŒì‚¬', 'ê²€ì‚¬', 'ë³€í˜¸ì‚¬', 'ì†Œì†¡',
                
                # ì¸ë¬¼ëª…
                'ìœ¤ì„ì—´', 'ì´ì¬ëª…', 'í•œë™í›ˆ', 'ì¡°êµ­', 'ë¬¸ì¬ì¸', 'ë°•ê·¼í˜œ', 'ì´ë‚™ì—°',
                'ê¹€ê¸°í˜„', 'ì¶”ê²½í˜¸', 'ë°•í™ê·¼', 'ìš°ì›ì‹', 'ì •ì§„ì„',
                
                # ì •ë‹¹/ê¸°ê´€
                'ë¯¼ì£¼ë‹¹', 'êµ­ë¯¼ì˜í˜', 'ì•¼ë‹¹', 'ì—¬ë‹¹', 'ì •ë‹¹', 'ì²­ì™€ëŒ€', 'ëŒ€í†µë ¹ì‹¤',
                
                # ê²½ì œ/ì‚¬íšŒ
                'ê²½ì œ', 'ë¬¼ê°€', 'ê¸ˆë¦¬', 'ë¶€ë™ì‚°', 'íˆ¬ì', 'ê¸°ì—…', 'ì¼ìë¦¬', 'ê³ ìš©',
                'êµìœ¡', 'ì˜ë£Œ', 'ë³µì§€', 'í™˜ê²½', 'ì•ˆì „', 'ë²”ì£„', 'ì‚¬íšŒ', 'êµ­ë¯¼',
                
                # êµ­ì œ/ì™¸êµ
                'ì™¸êµ', 'êµ­ì œ', 'ë¯¸êµ­', 'ì¤‘êµ­', 'ì¼ë³¸', 'ë¶í•œ', 'ì•ˆë³´', 'í†µì¼',
                
                # ê¸°íƒ€ ì¤‘ìš” í‚¤ì›Œë“œ
                'ì •ì±…', 'ë²•ì•ˆ', 'ê°œí˜', 'ë…¼ë€', 'ê°ˆë“±', 'í˜‘ë ¥', 'í•©ì˜', 'ë°œí‘œ', 'ë°œì–¸'
            ]
            
            # 3ì°¨: ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ì§ì ‘ ëª…ì‚¬ + ì‚¬ì „ í‚¤ì›Œë“œ)
            # ì§ì ‘ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ ìš°ì„ ìˆœìœ„ë¡œ ì²˜ë¦¬
            for noun in direct_nouns:
                if len(noun) >= 2 and noun not in keywords:
                    keywords.append(noun)
            
            # ì¤‘ìš” í‚¤ì›Œë“œ ì‚¬ì „ê³¼ ë§¤ì¹­ (í¸í–¥ ì—†ëŠ” ë²”ìš©ì  OCR ì˜¤ë¥˜ ê³ ë ¤)
            for keyword in important_keywords:
                found = False
                
                # ì§ì ‘ ë§¤ì¹­
                if keyword in text_lower:
                    found = True
                else:
                    # ë²”ìš©ì  OCR ì˜¤ë¥˜ íŒ¨í„´ ë§¤ì¹­ (íŠ¹ì • ì¸ë¬¼/ìƒí™©ì— í¸í–¥ë˜ì§€ ì•ŠìŒ)
                    # 2-3ê¸€ì ë‹¨ì–´ì˜ ì¤‘ê°„ì— ê³µë°±ì´ ë“¤ì–´ê°„ ê²½ìš°
                    if len(keyword) >= 2:
                        # ê° ê¸€ì ì‚¬ì´ì— ê³µë°±ì´ ë“¤ì–´ê°„ íŒ¨í„´
                        spaced_patterns = []
                        for i in range(1, len(keyword)):
                            pattern = keyword[:i] + ' ' + keyword[i:]
                            spaced_patterns.append(pattern)
                        
                        if any(pattern in text for pattern in spaced_patterns):
                            found = True
                    
                    # íŠ¹ì • ê¸°ê´€ëª…ì˜ ì¤„ì„ë§ ì²˜ë¦¬ (í¸í–¥ ì—†ì´)
                    abbreviation_map = {
                        'í—Œë²•ì¬íŒì†Œ': ['í—Œì¬', 'ë²•ì¬íŒì†Œ'],
                        'êµ­íšŒì˜ì›': ['ì˜ì›'],
                        'ëŒ€í†µë ¹': ['ëŒ€í†µ'],
                        'ê²€ì°°ì²­': ['ê²€ì°°'],
                        'ê²½ì°°ì²­': ['ê²½ì°°']
                    }
                    
                    if keyword in abbreviation_map:
                        if any(abbrev in text_lower for abbrev in abbreviation_map[keyword]):
                            found = True
                
                if found and keyword not in keywords:
                    keywords.append(keyword)
                    logger.debug(f"í‚¤ì›Œë“œ ë§¤ì¹­: '{keyword}'")
            
            # í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
            unique_keywords = []
            seen = set()
            for keyword in keywords:
                if keyword not in seen and len(keyword) >= 2:
                    seen.add(keyword)
                    unique_keywords.append(keyword)
            
            return unique_keywords[:5]  # ìµœëŒ€ 5ê°œ ë°˜í™˜
        
        # ì¿¼ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        query_keywords = extract_keywords(cleaned_text)
        logger.info(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {query_keywords}")
        
        # ì£¼ìš” í‚¤ì›Œë“œ 3ê°œ ì„ ë³„ (ë” ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´)
        def extract_top_keywords(keywords: List[str], text: str) -> List[str]:
            """í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ 3ê°œ ì„ ë³„ (ì˜ë¯¸ì  ê´€ë ¨ì„± ìš°ì„ )"""
            if len(keywords) <= 3:
                return keywords
            
            # í‚¤ì›Œë“œ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
            keyword_scores = []
            for keyword in keywords:
                score = 0
                
                # 1. ì˜ë¯¸ì  ì¤‘ìš”ë„ (í¸í–¥ ì—†ëŠ” ë²”ìš©ì  í‰ê°€)
                # ê¸°ê´€/ì§ì±… ê´€ë ¨ í‚¤ì›Œë“œ (ëª¨ë“  ë¶„ì•¼ ë™ë“± ì²˜ë¦¬)
                institution_keywords = ['ëŒ€í†µë ¹', 'êµ­íšŒ', 'ì •ë¶€', 'ë²•ì›', 'ê²€ì°°', 'ê²½ì°°', 'í—Œë²•ì¬íŒì†Œ']
                # ë²•ë¥ /ì •ì¹˜ í”„ë¡œì„¸ìŠ¤ í‚¤ì›Œë“œ
                process_keywords = ['íƒ„í•µ', 'ì„ ê±°', 'ì¬íŒ', 'íŒê²°', 'ê²°ì •', 'ìˆ˜ì‚¬', 'ê¸°ì†Œ', 'ë²•ì•ˆ', 'ì •ì±…']
                # ê²½ì œ/ì‚¬íšŒ í‚¤ì›Œë“œ
                social_keywords = ['ê²½ì œ', 'êµìœ¡', 'ì˜ë£Œ', 'í™˜ê²½', 'ë³µì§€', 'ì•ˆì „', 'ë¬¸í™”']
                
                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ ë™ë“±í•˜ê²Œ í‰ê°€ (í¸í–¥ ì œê±°)
                if keyword in institution_keywords:
                    score += 5  # ê¸°ê´€ í‚¤ì›Œë“œ
                elif keyword in process_keywords:
                    score += 5  # í”„ë¡œì„¸ìŠ¤ í‚¤ì›Œë“œ
                elif keyword in social_keywords:
                    score += 5  # ì‚¬íšŒ í‚¤ì›Œë“œ
                
                # 2. ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´ ì„ í˜¸)
                if 3 <= len(keyword) <= 6:
                    score += 3
                elif len(keyword) == 2:
                    score += 1
                
                # 3. ë¹ˆë„ ì ìˆ˜ (ë„ˆë¬´ ë§ì´ ë‚˜ì˜¤ëŠ” ê²ƒì€ ê°ì )
                count = text.lower().count(keyword.lower())
                if count == 1:
                    score += 2  # í•œ ë²ˆ ë‚˜ì˜¤ëŠ” ê²ƒì´ ìµœì 
                elif count >= 2:
                    score += 1
                
                # 4. ìœ„ì¹˜ ì ìˆ˜ (ì•ë¶€ë¶„ì— ë‚˜ì˜¤ëŠ” í‚¤ì›Œë“œ ìš°ì„ )
                first_pos = text.lower().find(keyword.lower())
                if first_pos >= 0:
                    if first_pos < len(text) * 0.4:  # ì• 40% êµ¬ê°„
                        score += 3
                    elif first_pos < len(text) * 0.8:  # ì¤‘ê°„ 40% êµ¬ê°„
                        score += 1
                
                keyword_scores.append((keyword, score))
            
            # ì ìˆ˜ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ 3ê°œ ì„ íƒ
            sorted_keywords = sorted(keyword_scores, key=lambda x: x[1], reverse=True)
            top_3 = [kw for kw, score in sorted_keywords[:3]]
            
            logger.info(f"ğŸ¯ ì£¼ìš” í‚¤ì›Œë“œ 3ê°œ ì„ ë³„: {top_3}")
            logger.info(f"   ì ìˆ˜: {[(kw, score) for kw, score in sorted_keywords[:3]]}")
            
            return top_3
        
        main_keywords = extract_top_keywords(query_keywords, cleaned_text)
        
        if not query_keywords:
            logger.warning("âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ - ê¸°ë³¸ ì˜ë¯¸ì  ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
        else:
            logger.info(f"âœ… {len(query_keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ, ì£¼ìš” {len(main_keywords)}ê°œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì‹œì‘")
        
        # ì˜ë¯¸ì  í‚¤ì›Œë“œ í™•ì¥ ì‹œìŠ¤í…œ (í¸í–¥ ì—†ëŠ” ë²”ìš©ì  í™•ì¥)
        def expand_keywords_semantically(keywords: List[str]) -> List[str]:
            """ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ í‚¤ì›Œë“œë“¤ì„ ì£¼ì œë³„ë¡œ ê· í˜•ì¡íˆê²Œ í™•ì¥"""
            expanded = keywords.copy()
            
            # 1. ë²•ë¥ /ì‚¬ë²• ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥
            legal_base = {'íƒ„í•µ', 'í—Œë²•ì¬íŒì†Œ', 'í—Œì¬', 'íŒŒë©´', 'ê²°ì •', 'íŒê²°', 'ë²•ì›', 'ì¬íŒ', 'ìˆ˜ì‚¬', 'ê¸°ì†Œ'}
            if any(keyword in legal_base for keyword in keywords):
                legal_extended = [
                    'ì‚¬ë²•ë¶€', 'ì¬íŒë¶€', 'ë²•ì •', 'íŒì‚¬', 'ê²€ì‚¬', 'ë³€í˜¸ì‚¬', 'ì†Œì†¡',
                    'ë²•ë¥ ', 'í—Œë²•', 'ìœ„í—Œ', 'í•©í—Œ', 'ê¸°ê°', 'ì¸ìš©', 'ê°í•˜'
                ]
                expanded.extend(legal_extended)
                logger.info("âš–ï¸ ë²•ë¥ /ì‚¬ë²• í‚¤ì›Œë“œ í™•ì¥")
            
            # 2. ì •ì¹˜/í–‰ì • ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥
            political_base = {'ëŒ€í†µë ¹', 'êµ­íšŒ', 'ì •ë¶€', 'ì˜ì›', 'ì •ì¹˜', 'ì„ ê±°', 'ì •ë‹¹'}
            if any(keyword in political_base for keyword in keywords):
                political_extended = [
                    'êµ­ì •ê°ì‚¬', 'êµ­ì •ìš´ì˜', 'ì •ì±…', 'ë²•ì•ˆ', 'ì˜ì •í™œë™',
                    'ì•¼ë‹¹', 'ì—¬ë‹¹', 'ì •ì¹˜ê¶Œ', 'ì²­ì™€ëŒ€', 'ëŒ€í†µë ¹ì‹¤'
                ]
                expanded.extend(political_extended)
                logger.info("ï¿½ï¸ ì •ì¹˜/í–‰ì • í‚¤ì›Œë“œ í™•ì¥")
            
            # ë²•ë¥ /ì‚¬ë²• ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥  
            legal_base = {'ë²•ì›', 'ì¬íŒ', 'íŒê²°', 'ìˆ˜ì‚¬', 'ê¸°ì†Œ', 'ê²€ì°°', 'ì‚¬ë²•ë¶€'}
            if any(keyword in legal_base for keyword in keywords):
                legal_extended = [
                    'ì¬íŒë¶€', 'ë²•ì •', 'íŒì‚¬', 'ê²€ì‚¬', 'ë³€í˜¸ì‚¬', 'ì†Œì†¡', 'ë²•ë¥ ',
                    'ì‚¬ë²•ê¶Œ', 'ë²•ì›ê²°ì •', 'ì¬íŒê²°ê³¼', 'ë²•ì ì ˆì°¨'
                ]
                expanded.extend(economic_extended)
                logger.info("ê²½ì œ ë¶„ì•¼ í‚¤ì›Œë“œ í™•ì¥")
            
            # ì‚¬íšŒ ë¶„ì•¼ í‚¤ì›Œë“œ í™•ì¥
            social_base = {'ì‚¬íšŒ', 'êµìœ¡', 'ì˜ë£Œ', 'ë³µì§€', 'ë¬¸í™”', 'í™˜ê²½', 'ì•ˆì „'}
            if any(keyword in social_base for keyword in keywords):
                social_extended = [
                    'ì‹œë¯¼', 'êµ­ë¯¼', 'ìƒí™œ', 'ë³´ê±´', 'í•™êµ', 'ëŒ€í•™', 'ë³‘ì›',
                    'ê³µê³µ', 'ì„œë¹„ìŠ¤', 'ì œë„', 'ê°œì„ ', 'ì§€ì›'
                ]
                expanded.extend(social_extended)
                logger.info("ì‚¬íšŒ ë¶„ì•¼ í‚¤ì›Œë“œ í™•ì¥")
            
            # ë²•ë¥ /ì‚¬ë²• ë¶„ì•¼ í‚¤ì›Œë“œ í™•ì¥
            legal_base = {'ë²•ì›', 'ì¬íŒ', 'íŒê²°', 'ìˆ˜ì‚¬', 'ê¸°ì†Œ', 'ê²€ì°°', 'ì‚¬ë²•ë¶€'}
            if any(keyword in legal_base for keyword in keywords):
                legal_extended = [
                    'ë²•', 'íŒì‚¬', 'ê²€ì‚¬', 'ë³€í˜¸ì‚¬', 'ì†Œì†¡', 'ì¬íŒë¶€',
                    'í˜•ì‚¬', 'ë¯¼ì‚¬', 'í–‰ì •', 'í—Œë²•', 'ëŒ€ë²•ì›'
                ]
                expanded.extend(legal_extended)
                logger.info("ë²•ë¥ /ì‚¬ë²• ë¶„ì•¼ í‚¤ì›Œë“œ í™•ì¥")
            
            # êµ­ì œ/ì™¸êµ ë¶„ì•¼ í‚¤ì›Œë“œ í™•ì¥
            international_base = {'ì™¸êµ', 'êµ­ì œ', 'ë¯¸êµ­', 'ì¤‘êµ­', 'ì¼ë³¸', 'ë¶í•œ', 'ì•ˆë³´'}
            if any(keyword in international_base for keyword in keywords):
                international_extended = [
                    'ì™¸êµë¶€', 'êµ­ë°©', 'í†µì¼', 'í˜‘ë ¥', 'íšŒë‹´', 'ì •ìƒíšŒë‹´',
                    'ì¡°ì•½', 'í˜‘ì •', 'ë™ë§¹', 'ê´€ê³„', 'ëŒ€í™”'
                ]
                expanded.extend(international_extended)
                logger.info("êµ­ì œ/ì™¸êµ ë¶„ì•¼ í‚¤ì›Œë“œ í™•ì¥")
            
            return list(set(expanded))  # ì¤‘ë³µ ì œê±°
        
        # ì˜ë¯¸ì  í‚¤ì›Œë“œ í™•ì¥ ì ìš©
        if query_keywords and len(cleaned_text) < 100:  # ì§§ì€ í…ìŠ¤íŠ¸ì—ë§Œ ì ìš©
            expanded_keywords = expand_keywords_semantically(query_keywords)
            logger.info(f"ì›ë³¸ í‚¤ì›Œë“œ: {query_keywords}")
            logger.info(f"í™•ì¥ëœ í‚¤ì›Œë“œ: {expanded_keywords}")
            query_keywords = expanded_keywords
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ëŒ€ì²´ ê²€ìƒ‰ í•¨ìˆ˜ (ì´ë¯¸ì§€ ì „ìš©)
        def keyword_based_search(text, keywords, pack, embedder, nli_tokenizer, nli_model, use_gpu, fp16, nli_batch, start_time):
            """í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ëŠ” ëŒ€ì²´ ë°©ë²•"""
            logger.info("í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ëª¨ë“œ ì‹œì‘")
            
            if not keywords:
                return {
                    "success": False,
                    "error": "í‚¤ì›Œë“œê°€ ì—†ì–´ì„œ ëŒ€ì²´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # URL í’ˆì§ˆ ê²€ì‚¬ í•¨ìˆ˜ (ë‚´ë¶€ ì •ì˜)
            def check_url_quality(url: str) -> bool:
                """ë‰´ìŠ¤ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸"""
                url_lower = url.lower()
                exclude_patterns = [
                    'copyright', 'agreement', 'privacy', 'terms', 'policy',
                    'contact', 'about', 'newslist', 'category', 'tag',
                    'search', 'login', 'register', 'member', 'mypage',
                    'sitemap', 'rss', 'xml', 'api', 'admin', 'management',
                    'list', 'index', 'main', 'home', 'plan', 'specialedition',
                    'history', 'archive', 'event', 'promotion', 'guide'
                ]
                
                for pattern in exclude_patterns:
                    if pattern in url_lower:
                        return False
                
                include_patterns = ['article', 'news', 'view', 'read', 'story', 'report']
                has_include_pattern = any(pattern in url_lower for pattern in include_patterns)
                has_many_numbers = len([c for c in url if c.isdigit()]) >= 10
                
                import re
                has_date_pattern = bool(re.search(r'20\d{2}[/\-]?\d{2}[/\-]?\d{2}', url))
                
                return has_include_pattern or has_many_numbers or has_date_pattern
            
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë“¤ì„ ëª¨ë‘ ì°¾ê¸°
            keyword_matches = []
            keyword_scores = []
            
            for i, record in enumerate(pack.records):
                record_text = record.chunk.lower()
                matched_keywords = [kw for kw in keywords if kw in record_text]
                
                if matched_keywords:
                    # URL í’ˆì§ˆ í™•ì¸
                    if not check_url_quality(record.url):
                        continue
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ì£¼ìš” í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©)
                    basic_score = len(matched_keywords) / len(keywords)
                    
                    # í¸í–¥ ì œê±°: ì£¼ìš” í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì™„ì „ ì‚­ì œ
                    # ëª¨ë“  í‚¤ì›Œë“œë¥¼ ë™ë“±í•˜ê²Œ í‰ê°€
                    
                    keyword_density = basic_score
                    
                    # ì™„ì „ ë§¤ì¹­ ë³´ë„ˆìŠ¤ (ëª¨ë“  í‚¤ì›Œë“œê°€ ë§¤ì¹­ë  ë•Œ)
                    if len(matched_keywords) == len(keywords):
                        keyword_density *= 1.3  # 30% ë³´ë„ˆìŠ¤ (ê¸°ì¡´ 50%ì—ì„œ ì¡°ì •)
                    
                    # ê³ ë¹ˆë„ ë§¤ì¹­ ë³´ë„ˆìŠ¤ (80% ì´ìƒ ë§¤ì¹­)
                    elif keyword_density >= 0.8:
                        keyword_density *= 1.2  # 20% ë³´ë„ˆìŠ¤
                    
                    # ì½˜í…ì¸  ê´€ë ¨ì„± ê²€ì‚¬ (í‚¤ì›Œë“œ ë¬¸ë§¥ ì¼ì¹˜ë„)
                    context_score = 0
                    for kw in matched_keywords:
                        # í‚¤ì›Œë“œ ì£¼ë³€ ë¬¸ë§¥ í™•ì¸ (ê°„ë‹¨í•œ ë°©ì‹)
                        kw_index = record_text.find(kw)
                        if kw_index >= 0:
                            # í‚¤ì›Œë“œ ì•ë’¤ 10ê¸€ìì”© í™•ì¸
                            context = record_text[max(0, kw_index-10):kw_index+len(kw)+10]
                            # ë‹¤ë¥¸ í‚¤ì›Œë“œë“¤ì´ ê·¼ì²˜ì— ìˆìœ¼ë©´ ê´€ë ¨ì„± ë†’ìŒ
                            nearby_matches = sum(1 for other_kw in keywords if other_kw != kw and other_kw in context)
                            context_score += nearby_matches
                    
                    # ë¬¸ë§¥ ê´€ë ¨ì„± ë³´ë„ˆìŠ¤ (í‚¤ì›Œë“œë“¤ì´ í•¨ê»˜ ë‚˜íƒ€ë‚  ë•Œ)
                    if context_score > 0:
                        keyword_density *= (1 + context_score * 0.1)  # ë¬¸ë§¥ë‹¹ 10% ë³´ë„ˆìŠ¤
                    
                    keyword_matches.append(i)
                    keyword_scores.append(keyword_density)
            
            if not keyword_matches:
                logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨: '{keywords}' - ì¸ë±ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return {
                    "success": False,
                    "error": "í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ” ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            logger.info(f"ğŸ¯ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰: {len(keyword_matches)}ê°œ ë¬¸ì„œ ë°œê²¬")
            
            # ìƒìœ„ 5ê°œ ë§¤ì¹­ ê²°ê³¼ ë¡œê·¸
            top_5_indices = np.argsort(keyword_scores)[::-1][:5]
            for i, idx in enumerate(top_5_indices):
                record_idx = keyword_matches[idx]
                score = keyword_scores[idx]
                url = pack.records[record_idx].url
                logger.info(f"  {i+1}. ë§¤ì¹­ì ìˆ˜ {score:.3f}: {url[:80]}...")
            
            # í‚¤ì›Œë“œ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 50ê°œ ì„ íƒ
            sorted_keyword_indices = np.argsort(keyword_scores)[::-1][:50]
            selected_indices = np.array(keyword_matches)[sorted_keyword_indices]
            selected_scores = np.array(keyword_scores)[sorted_keyword_indices]
            
            # ê³ ë„í™”ëœ ì˜ë¯¸ì  ì—°ê´€ì„± ë¶„ì„ ì ìš©
            query_emb = embedder.encode([text], normalize_embeddings=True)
            selected_matrix = pack.matrix[selected_indices]
            base_similarities = util.cos_sim(query_emb[0], selected_matrix).cpu().numpy().squeeze()
            
            if np.isscalar(base_similarities):
                base_similarities = np.array([base_similarities])
            
            # ì˜ë¯¸ì  ì—°ê´€ì„± ë¶„ì„ìœ¼ë¡œ ìœ ì‚¬ë„ ê°œì„ 
            enhanced_similarities = []
            for i, idx in enumerate(selected_indices):
                article_content = pack.records[idx].chunk
                
                # ìƒˆë¡œìš´ ì˜ë¯¸ì  ì—°ê´€ì„± ë¶„ì„ ì ìš©
                semantic_analysis = analyze_semantic_relevance(text, article_content, embedder)
                
                # ê¸°ì¡´ ìœ ì‚¬ë„ì™€ ì˜ë¯¸ì  ì—°ê´€ì„± ì ìˆ˜ ê²°í•©
                enhanced_score = (
                    base_similarities[i] * 0.3 +          # ê¸°ì¡´ ì„ë² ë”© ìœ ì‚¬ë„ 30%
                    semantic_analysis['final_score'] * 0.7  # ì˜ë¯¸ì  ì—°ê´€ì„± 70%
                )
                enhanced_similarities.append(enhanced_score)
                
                if semantic_analysis['final_score'] > 0.6:  # ë†’ì€ ì—°ê´€ì„± ë°œê²¬ì‹œ ë¡œê·¸
                    logger.info(f"ğŸ§  ë†’ì€ ì˜ë¯¸ì  ì—°ê´€ì„± ë°œê²¬ (ì ìˆ˜: {semantic_analysis['final_score']:.3f}): {pack.records[idx].url[:50]}...")
                    logger.debug(f"   ì£¼ì œ: {semantic_analysis['query_topics']} â†” {semantic_analysis['article_topics']}")
            
            similarities = np.array(enhanced_similarities)
            logger.info(f"ğŸš€ ì˜ë¯¸ì  ì—°ê´€ì„± ë¶„ì„ ì™„ë£Œ: í‰ê·  ì ìˆ˜ {similarities.mean():.3f}")
            
            # NLI í‰ê°€
            premises = [pack.records[i].chunk for i in selected_indices]
            hypothesis = text
            
            support_scores = []
            
            for i in range(0, len(premises), nli_batch):
                batch_premises = premises[i:i+nli_batch]
                batch_inputs = nli_tokenizer(
                    [hypothesis] * len(batch_premises),
                    batch_premises,
                    truncation=True, padding=True, return_tensors="pt", max_length=512
                )
                
                if use_gpu and torch.cuda.is_available():
                    batch_inputs = {k: v.cuda() for k, v in batch_inputs.items()}
                
                with torch.no_grad():
                    if fp16:
                        with torch.cuda.amp.autocast():
                            outputs = nli_model(**batch_inputs)
                    else:
                        outputs = nli_model(**batch_inputs)
                    
                    logits = outputs.logits.cpu()
                    probs = torch.softmax(logits, dim=-1).numpy()
                    support_scores.extend(probs[:, 0])
            
            support_scores = np.array(support_scores)
            
            # ìµœì¢… ì ìˆ˜: í‚¤ì›Œë“œ ì ìˆ˜ 50% + ìœ ì‚¬ì„± 30% + NLI 20%
            final_scores = selected_scores * 0.5 + similarities * 0.3 + support_scores * 0.2
            
            # ê²°ê³¼ ì •ë ¬ ë° ì„ íƒ
            sorted_indices_final = np.argsort(final_scores)[::-1]
            
            # í•œêµ­ ë‰´ìŠ¤ ë„ë©”ì¸ ìš°ì„  ì²˜ë¦¬
            korean_news_domains = {
                'naver.com', 'daum.net', 'chosun.com', 'donga.com', 'joongang.co.kr',
                'hankyung.com', 'mk.co.kr', 'ytn.co.kr', 'jtbc.co.kr', 'sbs.co.kr',
                'kbs.co.kr', 'mbc.co.kr', 'edaily.co.kr', 'newsis.com', 'yonhapnews.co.kr',
                'hani.co.kr', 'hankookilbo.com', 'seoul.co.kr', 'busan.com', 'imaeil.com',
                'kyeongin.com', 'kwnews.co.kr', 'kwangju.co.kr', 'kado.net'
            }
            
            results = []
            korean_results = []
            other_results = []
            
            for rank, idx in enumerate(sorted_indices_final[:40]):  # ìƒìœ„ 40ê°œë¡œ ì¦ê°€
                orig_idx = selected_indices[idx]
                url = pack.records[orig_idx].url
                domain = url.split('/')[2].lower() if '//' in url else ''
                clean_domain = domain.replace('www.', '')
                
                result = {
                    "rank": rank + 1,
                    "url": url,
                    "similarity": float(similarities[idx]),
                    "support": float(support_scores[idx]),
                    "score": float(final_scores[idx]),
                    "keyword_score": float(selected_scores[idx])
                }
                
                is_korean_news = any(kd in clean_domain for kd in korean_news_domains)
                
                if is_korean_news:
                    korean_results.append(result)
                else:
                    other_results.append(result)
            
            # í•œêµ­ ë‰´ìŠ¤ ìš°ì„ , ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ê²°ê³¼ í¬í•¨
            if korean_results:
                results = korean_results[:TOPN_RETURN]
                if len(results) < TOPN_RETURN:
                    remaining = TOPN_RETURN - len(results)
                    results.extend(other_results[:remaining])
            else:
                results = other_results[:TOPN_RETURN]
            
            # rank ì¬ì •ë ¬
            for i, result in enumerate(results):
                result["rank"] = i + 1
            
            if not results:
                return {
                    "success": False,
                    "error": "í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì—ì„œë„ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚° (ë” ë³´ìˆ˜ì )
            avg_keyword_score = sum(r["keyword_score"] for r in results) / len(results)
            avg_similarity = sum(r["similarity"] for r in results) / len(results)
            avg_support = sum(r["support"] for r in results) / len(results)
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì´ë¯€ë¡œ ë” ë‚®ì€ ê¸°ë³¸ ì ìˆ˜
            base_score = int((avg_keyword_score * 0.4 + avg_similarity * 0.3 + avg_support * 0.3) * 100)
            
            korean_count = len(korean_results)
            total_count = len(results)
            korean_ratio = korean_count / total_count if total_count > 0 else 0
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì€ ë” ë³´ìˆ˜ì ì¸ ì ìˆ˜
            if korean_ratio >= 0.8:
                reliability_score = min(75, max(30, base_score + 10))  # ìµœëŒ€ 75%
            elif korean_ratio >= 0.6:
                reliability_score = min(70, max(25, base_score + 5))
            else:
                reliability_score = min(65, max(20, base_score))
            
            # ì‹ ë¢°ë„ ë ˆë²¨ ê²°ì •
            if reliability_score >= 70:
                level = "ë†’ìŒ"
                recommendation = "í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì°¾ì€ ê´€ë ¨ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤. ë‚´ìš©ì„ ìì„¸íˆ í™•ì¸í•´ë³´ì„¸ìš”."
            elif reliability_score >= 50:
                level = "ë³´í†µ"
                recommendation = "í‚¤ì›Œë“œë¡œ ê´€ë ¨ëœ ê¸°ì‚¬ë“¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì •í™•ì„±ì„ ìœ„í•´ ì—¬ëŸ¬ ì¶œì²˜ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”."
            else:
                level = "ë‚®ìŒ"
                recommendation = "í‚¤ì›Œë“œë¡œ ì¼ë¶€ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ì§€ë§Œ, ì •í™•í•œ ì •ë³´ì¸ì§€ ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
            
            elapsed_time = time.time() - start_time
            
            return {
                "success": True,
                "reliability_score": reliability_score,
                "reliability_level": level,
                "recommendation": recommendation,
                "evidence": results,
                "elapsed_time": elapsed_time,
                "source_type": "keyword_search",
                "search_method": "keyword_based",
                "candidates_found": len(keyword_matches),
                "similarity_threshold": "N/A (í‚¤ì›Œë“œ ê¸°ë°˜)"
            }
        
        # ì„ë² ë”© ìƒì„±
        query_emb = embedder.encode([cleaned_text], normalize_embeddings=True)
        
        # ìŠ¤ë§ˆíŠ¸ ì´ì¤‘ í•„í„°ë§: í‚¤ì›Œë“œ + ì˜ë¯¸ì  ìœ ì‚¬ì„±
        if query_keywords and len(cleaned_text) < 100:
            logger.info("ìŠ¤ë§ˆíŠ¸ ì´ì¤‘ í•„í„°ë§ ìˆ˜í–‰")
            
            # 1ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ì‚¬ì „ í•„í„°ë§
            keyword_filtered_indices = []
            keyword_scores = []  # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            
            for i, record in enumerate(pack.records):
                record_text = record.chunk.lower()
                keyword_match_count = sum(1 for keyword in query_keywords if keyword in record_text)
                
                # ì ì ˆí•œ í‚¤ì›Œë“œ ë§¤ì¹­: ì¤‘ìš” í‚¤ì›Œë“œ ìš°ì„ , ì¼ë°˜ í‚¤ì›Œë“œë„ ê³ ë ¤
                important_keywords = {'ëŒ€í†µë ¹', 'íƒ„í•µ', 'í—Œë²•ì¬íŒì†Œ', 'ìœ¤ì„ì—´', 'íŒŒë©´', 'êµ­íšŒ'}
                important_matches = sum(1 for keyword in important_keywords if keyword in query_keywords and keyword in record_text)
                
                # ë” ì—„ê²©í•œ ì¡°ê±´: ì¤‘ìš” í‚¤ì›Œë“œ 1ê°œ ì´ìƒ OR ì¼ë°˜ í‚¤ì›Œë“œ 2ê°œ ì´ìƒ
                if important_matches >= 1 or keyword_match_count >= 2:
                    keyword_filtered_indices.append(i)
                    # í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚° (ì¤‘ìš” í‚¤ì›Œë“œëŠ” 3ë°° ê°€ì¤‘ì¹˜ë¡œ ê°•í™”)
                    weighted_match_count = keyword_match_count + important_matches * 2
                    keyword_density = weighted_match_count / len(query_keywords)
                    keyword_scores.append(keyword_density)
            
            logger.info(f"í‚¤ì›Œë“œ ë§¤ì¹­ëœ ë¬¸ì„œ: {len(keyword_filtered_indices)}ê°œ")
            
            if keyword_filtered_indices and len(keyword_filtered_indices) >= 5:  # ìµœì†Œ ê¸°ì¤€ ì™„í™”
                # 2ë‹¨ê³„: í‚¤ì›Œë“œ ë§¤ì¹­ëœ ë¬¸ì„œë“¤ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³„ì‚°
                filtered_matrix = pack.matrix[keyword_filtered_indices]
                similarities = util.cos_sim(query_emb[0], filtered_matrix).cpu().numpy().squeeze()
                
                if np.isscalar(similarities):
                    similarities = np.array([similarities])
                
                # 3ë‹¨ê³„: í‚¤ì›Œë“œ ì ìˆ˜ì™€ ì˜ë¯¸ì  ìœ ì‚¬ì„± ê²°í•© (ê°€ì¤‘ì¹˜ ì¡°ì •)
                combined_scores = []
                for i, (sim_score, keyword_score) in enumerate(zip(similarities, keyword_scores)):
                    # ê°€ì¤‘ í‰ê· : ì˜ë¯¸ì  ìœ ì‚¬ì„± 80% + í‚¤ì›Œë“œ ë°€ë„ 20% (ìœ ì‚¬ì„± ë” ì¤‘ìš”í•˜ê²Œ)
                    combined_score = sim_score * 0.8 + keyword_score * 0.2
                    combined_scores.append(combined_score)
                
                combined_scores = np.array(combined_scores)
                
                # 4ë‹¨ê³„: ê²°í•© ì ìˆ˜ë¡œ ì •ë ¬ ë° ë†’ì€ ì„ê³„ê°’ ì ìš©
                sorted_indices = np.argsort(combined_scores)[::-1]
                
                # ë” ì—„ê²©í•œ í•„í„°ë§: ë†’ì€ í’ˆì§ˆì˜ ë¬¸ì„œë§Œ ì„ íƒ
                high_quality_indices = []
                high_quality_sims = []
                
                for idx in sorted_indices:
                    original_idx = keyword_filtered_indices[idx]
                    similarity = similarities[idx]
                    combined_score = combined_scores[idx]
                    
                    # ë” ì—„ê²©í•œ ì¡°ê±´: ì˜ë¯¸ì  ìœ ì‚¬ì„± 0.35 ì´ìƒ AND ê²°í•©ì ìˆ˜ 0.45 ì´ìƒ
                    if similarity >= 0.35 and combined_score >= 0.45:
                        high_quality_indices.append(original_idx)
                        high_quality_sims.append(similarity)
                        
                        # ìƒìœ„ 80ê°œ ì„ íƒ (í’ˆì§ˆ ìš°ì„ )
                        if len(high_quality_indices) >= 80:
                            break
                
                if high_quality_indices:
                    candidate_indices = np.array(high_quality_indices)
                    candidate_sims = np.array(high_quality_sims)
                    logger.info(f"ê³ í’ˆì§ˆ ë¬¸ì„œ ì„ ë³„: {len(candidate_indices)}ê°œ (í‰ê·  ìœ ì‚¬ë„: {np.mean(candidate_sims):.3f})")
                else:
                    # ê³ í’ˆì§ˆ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì›ë˜ í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼ ì‚¬ìš© (ë” ë§ì€ í›„ë³´ í™•ë³´)
                    candidate_indices = np.array(keyword_filtered_indices)[np.argsort(similarities)[::-1][:100]]
                    candidate_sims = np.sort(similarities)[::-1][:100]
                    logger.info("ê³ í’ˆì§ˆ ë¬¸ì„œ ì—†ìŒ, í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼ ì‚¬ìš©")
            else:
                # í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ì¼ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
                logger.info("í‚¤ì›Œë“œ ë§¤ì¹­ ë¶€ì¡±, ì¼ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜")
                similarities = util.cos_sim(query_emb[0], pack.matrix).cpu().numpy().squeeze()
                if np.isscalar(similarities):
                    similarities = np.array([similarities])
                candidate_indices = np.argsort(similarities)[::-1][:TOPK_CANDIDATES]
                candidate_sims = similarities[candidate_indices]
        else:
            # ì¼ë°˜ì ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
            similarities = util.cos_sim(query_emb[0], pack.matrix).cpu().numpy().squeeze()
            
            if np.isscalar(similarities):
                similarities = np.array([similarities])
            
            # ì§§ì€ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œëŠ” ë” ë§ì€ í›„ë³´ ê³ ë ¤
            if len(cleaned_text) < 100:
                topk_for_short_text = min(1000, len(similarities))
                logger.info(f"ì§§ì€ í…ìŠ¤íŠ¸: ìƒìœ„ {topk_for_short_text}ê°œ í›„ë³´ ê²€ìƒ‰")
            else:
                topk_for_short_text = TOPK_CANDIDATES

            candidate_indices = np.argsort(similarities)[::-1][:topk_for_short_text]
            candidate_sims = similarities[candidate_indices]
        
        # ì§§ì€ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œëŠ” ë” ê´€ëŒ€í•œ ì„ê³„ê°’ ì ìš© (í•˜ì§€ë§Œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ ìƒí•œì„  ì„¤ì •)
        if len(cleaned_text) < 50:  # ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ (50ì ë¯¸ë§Œ)
            # ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ì´ê³  ì„ê³„ê°’ì´ ë§¤ìš° ë†’ìœ¼ë©´ í‚¤ì›Œë“œ ê²€ìƒ‰ ìš°ì„ 
            if min_text_length is not None and min_text_length <= MIN_IMAGE_TEXT_LEN and similarity_threshold >= 0.7:
                adaptive_threshold = similarity_threshold  # ì›ë³¸ ì„ê³„ê°’ ìœ ì§€ (í‚¤ì›Œë“œ ê²€ìƒ‰ ìœ ë„)
                logger.info(f"ì´ë¯¸ì§€ ê³ ì„ê³„ê°’ ëª¨ë“œ: í‚¤ì›Œë“œ ê²€ìƒ‰ ìœ ë„ë¥¼ ìœ„í•´ ì„ê³„ê°’ {adaptive_threshold:.2f} ìœ ì§€")
            else:
                adaptive_threshold = max(0.15, similarity_threshold * 0.5)  # ì„ê³„ê°’ì„ 50% ë‚®ì¶¤
                logger.info(f"ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ ê°ì§€: ì ì‘í˜• ì„ê³„ê°’ {adaptive_threshold:.2f} ì ìš©")
        elif len(cleaned_text) < 100:  # ì§§ì€ í…ìŠ¤íŠ¸ (100ì ë¯¸ë§Œ)
            # ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ì´ê³  ì„ê³„ê°’ì´ ë†’ìœ¼ë©´ í‚¤ì›Œë“œ ê²€ìƒ‰ ìš°ì„ 
            if min_text_length is not None and min_text_length <= MIN_IMAGE_TEXT_LEN and similarity_threshold >= 0.6:
                adaptive_threshold = similarity_threshold  # ì›ë³¸ ì„ê³„ê°’ ìœ ì§€
                logger.info(f"ì´ë¯¸ì§€ ì¤‘ì„ê³„ê°’ ëª¨ë“œ: í‚¤ì›Œë“œ ê²€ìƒ‰ ìœ ë„ë¥¼ ìœ„í•´ ì„ê³„ê°’ {adaptive_threshold:.2f} ìœ ì§€")
            else:
                adaptive_threshold = max(0.20, similarity_threshold * 0.7)  # ì„ê³„ê°’ì„ 30% ë‚®ì¶¤
                logger.info(f"ì§§ì€ í…ìŠ¤íŠ¸ ê°ì§€: ì ì‘í˜• ì„ê³„ê°’ {adaptive_threshold:.2f} ì ìš©")
        else:
            adaptive_threshold = similarity_threshold

        # ì„ê³„ê°’ í•„í„°ë§
        mask = candidate_sims >= adaptive_threshold
        candidate_indices = candidate_indices[mask]
        candidate_sims = candidate_sims[mask]
        
        # NLI í‰ê°€ ì¤€ë¹„ (í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì—ì„œë„ ì‚¬ìš©)
        nli_tokenizer, nli_model, _ = get_nli(use_gpu=use_gpu, fp16=fp16)
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ëŒ€ì²´ ê²€ìƒ‰ (ì´ë¯¸ì§€ í‰ê°€ì—ë§Œ ì ìš©)
        if len(candidate_indices) == 0 and min_text_length is not None and min_text_length <= MIN_IMAGE_TEXT_LEN:
            logger.info("ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨, í‚¤ì›Œë“œ ê¸°ë°˜ ëŒ€ì²´ ê²€ìƒ‰ ì‹œì‘")
            return keyword_based_search(cleaned_text, query_keywords, pack, embedder, nli_tokenizer, nli_model, use_gpu, fp16, nli_batch, start_time)
        elif len(candidate_indices) == 0:
            return {
                "success": False,
                "error": f"ìœ ì‚¬ì„± ì„ê³„ê°’ {adaptive_threshold:.2f} ì´ìƒì¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì›ë³¸ ì„ê³„ê°’: {similarity_threshold})"
            }
        
        # NLI í‰ê°€ (ì´ë¯¸ ìœ„ì—ì„œ ì¤€ë¹„ë¨)
        
        premises = [pack.records[i].chunk for i in candidate_indices]
        hypothesis = cleaned_text
        
        support_scores = []
        
        for i in range(0, len(premises), nli_batch):
            batch_premises = premises[i:i+nli_batch]
            batch_inputs = nli_tokenizer(
                [hypothesis] * len(batch_premises),
                batch_premises,
                truncation=True, padding=True, return_tensors="pt", max_length=512
            )
            
            if use_gpu and torch.cuda.is_available():
                batch_inputs = {k: v.cuda() for k, v in batch_inputs.items()}
            
            with torch.no_grad():
                if fp16:
                    with torch.cuda.amp.autocast():
                        outputs = nli_model(**batch_inputs)
                else:
                    outputs = nli_model(**batch_inputs)
                
                logits = outputs.logits.cpu()
                probs = torch.softmax(logits, dim=-1).numpy()
                support_scores.extend(probs[:, 0])  # entailment í™•ë¥ 
        
        support_scores = np.array(support_scores)
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        final_scores = ALPHA_SIM * candidate_sims + ALPHA_NLI * support_scores
        
        # ì§§ì€ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œëŠ” ë” ê´€ëŒ€í•œ ìµœì¢… ì„ê³„ê°’ ì ìš©
        if len(cleaned_text) < 100:
            adaptive_final_threshold = max(0.15, MIN_FINAL_SCORE * 0.5)  # ìµœì¢… ì„ê³„ê°’ì„ 50% ë‚®ì¶¤
            logger.info(f"ì§§ì€ í…ìŠ¤íŠ¸: ì ì‘í˜• ìµœì¢… ì„ê³„ê°’ {adaptive_final_threshold:.2f} ì ìš©")
        else:
            adaptive_final_threshold = MIN_FINAL_SCORE

        # ê²°ê³¼ ì •ë ¬ ë° í•„í„°ë§ (ìœ ì‚¬ë„ ìš°ì„  ì •ë ¬)
        sorted_indices = np.argsort(candidate_sims)[::-1]  # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        
        # í•œêµ­ ì£¼ìš” ë‰´ìŠ¤ ë„ë©”ì¸ ëª©ë¡
        korean_news_domains = {
            'naver.com', 'daum.net', 'chosun.com', 'donga.com', 'joongang.co.kr',
            'hankyung.com', 'mk.co.kr', 'ytn.co.kr', 'jtbc.co.kr', 'sbs.co.kr',
            'kbs.co.kr', 'mbc.co.kr', 'edaily.co.kr', 'newsis.com', 'yonhapnews.co.kr',
            'hani.co.kr', 'hankookilbo.com', 'seoul.co.kr', 'busan.com', 'imaeil.com',
            'kyeongin.com', 'kwnews.co.kr', 'kwangju.co.kr', 'kado.net'
        }
        
        # URL í’ˆì§ˆ í•„í„°ë§ í•¨ìˆ˜
        def is_quality_news_url(url: str) -> bool:
            """ë‰´ìŠ¤ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸ (ì¼ë°˜ í˜ì´ì§€ ì œì™¸)"""
            url_lower = url.lower()
            
            # ì œì™¸í•  URL íŒ¨í„´ë“¤ (ë” ê°•ë ¥í•˜ê²Œ)
            exclude_patterns = [
                'copyright', 'agreement', 'privacy', 'terms', 'policy',
                'contact', 'about', 'newslist', 'category', 'tag',
                'search', 'login', 'register', 'member', 'mypage',
                'sitemap', 'rss', 'xml', 'api', 'admin', 'management',
                'list', 'index', 'main', 'home', 'plan', 'specialedition',
                'history', 'archive', 'event', 'promotion', 'guide'
            ]
            
            # ì œì™¸ íŒ¨í„´ì´ ìˆìœ¼ë©´ False (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
            for pattern in exclude_patterns:
                if pattern in url_lower:
                    logger.debug(f"ì œì™¸ íŒ¨í„´ '{pattern}' ë°œê²¬: {url}")
                    return False
            
            # í¬í•¨ë˜ì–´ì•¼ í•  íŒ¨í„´ë“¤ (ë‰´ìŠ¤ ê¸°ì‚¬ URL íŠ¹ì§•)
            include_patterns = [
                'article', 'news', 'view', 'read', 'story', 'report'
            ]
            
            # í¬í•¨ íŒ¨í„´ì´ ìˆê±°ë‚˜, ìˆ«ìê°€ ë§ì´ í¬í•¨ëœ URL (ê¸°ì‚¬ ID)
            has_include_pattern = any(pattern in url_lower for pattern in include_patterns)
            has_many_numbers = len([c for c in url if c.isdigit()]) >= 10  # ê¸°ì‚¬ IDëŠ” ë³´í†µ 10ìë¦¬ ì´ìƒ
            
            # URLì— ë‚ ì§œ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸ (YYYY/MM/DD ë˜ëŠ” YYYYMMDD)
            import re
            has_date_pattern = bool(re.search(r'20\d{2}[/\-]?\d{2}[/\-]?\d{2}', url))
            
            result = has_include_pattern or has_many_numbers or has_date_pattern
            logger.debug(f"URL í’ˆì§ˆ ê²€ì‚¬: {url} -> {result} (íŒ¨í„´:{has_include_pattern}, ìˆ«ì:{has_many_numbers}, ë‚ ì§œ:{has_date_pattern})")
            
            return result
        
        results = []
        korean_results = []
        other_results = []
        
        for rank, idx in enumerate(sorted_indices):
            orig_idx = candidate_indices[idx]
            if final_scores[idx] >= adaptive_final_threshold:
                url = pack.records[orig_idx].url
                domain = url.split('/')[2].lower() if '//' in url else ''
                
                # ë„ë©”ì¸ì—ì„œ www. ì œê±°í•˜ê³  ì²´í¬
                clean_domain = domain.replace('www.', '')
                
                # URL í’ˆì§ˆ í™•ì¸
                if not is_quality_news_url(url):
                    logger.debug(f"ì €í’ˆì§ˆ URL ì œì™¸: {url}")
                    continue
                
                result = {
                    "rank": rank + 1,
                    "url": url,
                    "similarity": float(candidate_sims[idx]),
                    "support": float(support_scores[idx]),
                    "score": float(final_scores[idx])
                }
                
                # í•œêµ­ ë‰´ìŠ¤ ë„ë©”ì¸ì¸ì§€ í™•ì¸
                is_korean_news = any(kd in clean_domain for kd in korean_news_domains)
                
                if is_korean_news:
                    korean_results.append(result)
                else:
                    other_results.append(result)
        
        # í•œêµ­ ë‰´ìŠ¤ë¥¼ ìš°ì„ í•˜ë˜, ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ê²°ê³¼ë„ í¬í•¨
        if korean_results:
            results = korean_results[:TOPN_RETURN]
            if len(results) < TOPN_RETURN:
                remaining = TOPN_RETURN - len(results)
                results.extend(other_results[:remaining])
        else:
            results = other_results[:TOPN_RETURN]
        
        # rank ì¬ì •ë ¬
        for i, result in enumerate(results):
            result["rank"] = i + 1
        
        if not results:
            return {
                "success": False,
                "error": f"ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìµœì¢… ì„ê³„ê°’: {adaptive_final_threshold:.2f})"
            }
        
        # ì‹ ë¢°ë„ ê³„ì‚° (í‰ê·  ìœ ì‚¬ë„ì™€ NLI ì ìˆ˜ ê³ ë ¤)
        avg_similarity = sum(r["similarity"] for r in results) / len(results)
        avg_support = sum(r["support"] for r in results) / len(results)
        weighted_avg = sum(r["score"] for r in results) / len(results)
        
        # ì´ë¯¸ì§€ í‰ê°€ì—ì„œ ìœ ì‚¬ë„ê°€ ë‚®ê±°ë‚˜ ê²°ê³¼ê°€ ì ìœ¼ë©´ ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‹œë„
        is_image_eval = (min_text_length is not None and min_text_length <= MIN_IMAGE_TEXT_LEN)
        low_similarity = avg_similarity < 0.75
        few_results = len(results) < 3
        short_text = len(cleaned_text) < 100
        
        # ì´ë¯¸ì§€ í‰ê°€ì—ì„œ ê·¼ê±° ë¶€ì¡± ì‹œ êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ì‹¤ì œ ê¸°ì‚¬ ì°¾ê¸°
        if is_image_eval and (low_similarity or few_results) and short_text:
            logger.info(f"ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ì¡°ê±´ ì¶©ì¡±:")
            logger.info(f"  - ì´ë¯¸ì§€ í‰ê°€: {is_image_eval}")
            logger.info(f"  - ë‚®ì€ ìœ ì‚¬ë„: {low_similarity} (í‰ê· : {avg_similarity:.3f})")
            logger.info(f"  - ì ì€ ê²°ê³¼: {few_results} (ê°œìˆ˜: {len(results)})")
            logger.info(f"  - ì§§ì€ í…ìŠ¤íŠ¸: {short_text} (ê¸¸ì´: {len(cleaned_text)})")
            logger.info(f"ğŸ” êµ¬ê¸€ ê²€ìƒ‰ ì‹œì‘...")
            
            try:
                google_articles = search_google_articles_for_image(cleaned_text, main_keywords)
                
                        # ë¨¼ì € ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ìì²´ì— ëŒ€í•œ íŒ©íŠ¸ì²´í¬ ìˆ˜í–‰
                query_fact_check = fact_check_article(cleaned_text, "", main_keywords)
                
                # ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•œ ê²½ìš° ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‹œë„
                if not google_articles and query_fact_check:
                    logger.info(f"ğŸ”´ ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘")
                    google_articles = search_real_time_news(main_keywords)
                elif not query_fact_check:
                    logger.warning(f"âŒ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ìì²´ê°€ íŒ©íŠ¸ì²´í¬ ì‹¤íŒ¨ - ê²€ìƒ‰ ìƒëµ")
                    google_articles = []
                    
                    # íŒ©íŠ¸ì²´í¬ ì‹¤íŒ¨ ì‹œ ê°•ì œë¡œ ë‚®ì€ ì‹ ë¢°ë„ ë°˜í™˜
                    logger.warning(f"ğŸš¨ íŒ©íŠ¸ì²´í¬ ì‹¤íŒ¨ë¡œ ì¸í•œ í—ˆìœ„ì •ë³´ ì˜ì‹¬ - ì‹ ë¢°ë„ ê°•ì œ í•˜í–¥")
                    
                    return {
                        "success": True,
                        "reliability_score": 25,  # ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„
                        "reliability_level": "ë§¤ìš° ë‚®ìŒ",
                        "recommendation": "íŒ©íŠ¸ì²´í¬ì—ì„œ í—ˆìœ„ì •ë³´ ê°€ëŠ¥ì„±ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                        "evidence": [],
                        "fact_check_failed": True,
                        "searched_keywords": main_keywords,
                        "elapsed_time": time.time() - start_time,
                        "source_type": "image" if is_image_eval else "text",
                        "extracted_text_length": len(cleaned_text),
                        "extracted_text_preview": cleaned_text[:50] + "..." if len(cleaned_text) > 50 else cleaned_text
                    }
                
                if google_articles:
                    logger.info(f"âœ… êµ¬ê¸€ì—ì„œ {len(google_articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ë°œê²¬")
                    
                    # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ë¡œ ê·¼ê±° ìë£Œ ëŒ€ì²´
                    google_results = []
                    for i, article in enumerate(google_articles):
                        # ì‹¤ì‹œê°„ ê²€ìƒ‰ ê²°ê³¼ì™€ ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ êµ¬ë¶„
                        is_realtime = article.get('source') == 'real_time_search'
                        
                        google_results.append({
                            "url": article['url'],
                            "title": article['title'],
                            "similarity": 0.9 if is_realtime else 0.85,  # ì‹¤ì‹œê°„ì´ ë” ì •í™•
                            "support": 0.85 if is_realtime else 0.8,     # ì‹¤ì‹œê°„ì´ ë” ì‹ ë¢°
                            "score": 0.87 if is_realtime else 0.82,
                            "snippet": article['snippet'],
                            "source": "ì‹¤ì‹œê°„ ê²€ìƒ‰" if is_realtime else "êµ¬ê¸€ ê²€ìƒ‰",
                            "keyword_matches": article.get('title_matches', article.get('matches', 0))
                        })
                    
                    # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ë¡œ ê¸°ì¡´ ê²°ê³¼ ëŒ€ì²´
                    results = google_results[:5]  # ìµœëŒ€ 5ê°œ
                    avg_similarity = 0.85
                    avg_support = 0.8
                    weighted_avg = 0.82
                    logger.info("ğŸ”„ êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ë¡œ ê¸°ì¡´ ê²°ê³¼ ëŒ€ì²´")
                else:
                    logger.warning(f"âŒ êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ì£¼ìš” í‚¤ì›Œë“œ: {main_keywords}")
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê´€ë ¨ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ê³  í‘œì‹œ
                    if len(results) < 2:
                        return {
                            "success": True,
                            "reliability_score": 30,
                            "reliability_level": "ë‚®ìŒ",
                            "recommendation": f"'{' '.join(main_keywords)}' ê´€ë ¨ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                            "evidence": [],
                            "no_evidence_found": True,
                            "searched_keywords": main_keywords,
                            "elapsed_time": time.time() - start_time,
                            "source_type": "image" if is_image_eval else "text"
                        }
                
            except Exception as e:
                logger.warning(f"êµ¬ê¸€ ê¸°ì‚¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}, ê¸°ì¡´ ê²°ê³¼ ì‚¬ìš©")
        
        # í•œêµ­ ë‰´ìŠ¤ ë„ë©”ì¸ ë¹„ìœ¨ ê³„ì‚°
        korean_count = len(korean_results)
        total_count = len(results)
        korean_ratio = korean_count / total_count if total_count > 0 else 0
        
        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê·  ê¸°ë°˜)
        base_score = int(weighted_avg * 100)
        
        # í’ˆì§ˆ ë³´ë„ˆìŠ¤: ë†’ì€ ìœ ì‚¬ë„ì™€ ì§€ì§€ë„ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜
        quality_bonus = 0
        if avg_similarity >= 0.45:  # ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„
            quality_bonus += 10
        elif avg_similarity >= 0.40:  # ë†’ì€ ìœ ì‚¬ë„
            quality_bonus += 5
        
        if avg_support >= 0.7:  # ë†’ì€ NLI ì§€ì§€ë„
            quality_bonus += 8
        elif avg_support >= 0.6:  # ì¤‘ê°„ NLI ì§€ì§€ë„
            quality_bonus += 4
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ë³„ ì¡°ì •
        if len(cleaned_text) < 50:
            # ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸: í•œêµ­ ë‰´ìŠ¤ ë¹„ìœ¨ì— ë”°ë¼ ë³´ì •
            if korean_ratio >= 0.8:  # 80% ì´ìƒì´ í•œêµ­ ë‰´ìŠ¤
                reliability_score = min(90, max(0, base_score + quality_bonus + 10))
                logger.info(f"ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ + í•œêµ­ ë‰´ìŠ¤ ìš°ì„¸: +{quality_bonus + 10}ì  ë³´ì •")
            elif korean_ratio >= 0.6:  # 60% ì´ìƒì´ í•œêµ­ ë‰´ìŠ¤
                reliability_score = min(85, max(0, base_score + quality_bonus + 5))
                logger.info(f"ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ + í•œêµ­ ë‰´ìŠ¤ ë‹¤ìˆ˜: +{quality_bonus + 5}ì  ë³´ì •")
            elif korean_ratio >= 0.4:  # 40% ì´ìƒì´ í•œêµ­ ë‰´ìŠ¤
                reliability_score = min(75, max(0, base_score + quality_bonus))
                logger.info(f"ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ + í•œêµ­ ë‰´ìŠ¤ ì¼ë¶€: +{quality_bonus}ì  ë³´ì •")
            else:  # í•œêµ­ ë‰´ìŠ¤ ë¹„ìœ¨ì´ ë‚®ìŒ
                reliability_score = min(65, max(0, base_score + quality_bonus - 5))
                logger.info(f"ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ + ë¹„í•œêµ­ ë‰´ìŠ¤ ìœ„ì£¼: +{quality_bonus - 5}ì  ë³´ì •")
        elif len(cleaned_text) < 100:
            # ì§§ì€ í…ìŠ¤íŠ¸: í•œêµ­ ë‰´ìŠ¤ ë¹„ìœ¨ì— ë”°ë¼ ì¡°ì •
            if korean_ratio >= 0.8:
                reliability_score = min(95, max(0, base_score + quality_bonus + 8))
                logger.info(f"ì§§ì€ í…ìŠ¤íŠ¸ + í•œêµ­ ë‰´ìŠ¤ ìš°ì„¸: +{quality_bonus + 8}ì  ë³´ì •")
            elif korean_ratio >= 0.6:
                reliability_score = min(90, max(0, base_score + quality_bonus + 5))
                logger.info(f"ì§§ì€ í…ìŠ¤íŠ¸ + í•œêµ­ ë‰´ìŠ¤ ë‹¤ìˆ˜: +{quality_bonus + 5}ì  ë³´ì •")
            elif korean_ratio >= 0.4:
                reliability_score = min(85, max(0, base_score + quality_bonus))
                logger.info(f"ì§§ì€ í…ìŠ¤íŠ¸ + í•œêµ­ ë‰´ìŠ¤ ì¼ë¶€: +{quality_bonus}ì  ë³´ì •")
            else:
                reliability_score = min(75, max(0, base_score + quality_bonus - 5))
                logger.info(f"ì§§ì€ í…ìŠ¤íŠ¸ + ë¹„í•œêµ­ ë‰´ìŠ¤ ìœ„ì£¼: +{quality_bonus - 5}ì  ë³´ì •")
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸: ê¸°ë³¸ ì ìˆ˜ + í’ˆì§ˆ ë³´ë„ˆìŠ¤
            reliability_score = min(100, max(0, base_score + quality_bonus))
        
        # ì‹ ë¢°ë„ ë ˆë²¨ ê²°ì • (ì§§ì€ í…ìŠ¤íŠ¸ + í•œêµ­ ë‰´ìŠ¤ ë¹„ìœ¨ ê³ ë ¤)
        if reliability_score >= 80:
            level = "ë§¤ìš° ë†’ìŒ"
            if korean_ratio >= 0.8:
                recommendation = "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í•œêµ­ ì–¸ë¡ ì‚¬ ì¶œì²˜ì—ì„œ í™•ì¸ëœ ì •ë³´ì…ë‹ˆë‹¤."
            elif len(cleaned_text) < 100:
                recommendation = "ì§§ì€ í…ìŠ¤íŠ¸ì´ì§€ë§Œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ì…ë‹ˆë‹¤."
            else:
                recommendation = "ì´ ì •ë³´ëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif reliability_score >= 65:
            level = "ë†’ìŒ"  
            if korean_ratio >= 0.8:
                recommendation = "í•œêµ­ ì–¸ë¡ ì‚¬ì—ì„œ ëŒ€ì²´ë¡œ ì¼ì¹˜í•˜ëŠ” ì •ë³´ì…ë‹ˆë‹¤."
            elif len(cleaned_text) < 100:
                recommendation = "ì§§ì€ í…ìŠ¤íŠ¸ì´ì§€ë§Œ ëŒ€ì²´ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ì…ë‹ˆë‹¤."
            else:
                recommendation = "ì´ ì •ë³´ëŠ” ëŒ€ì²´ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif reliability_score >= 50:
            level = "ë³´í†µ"
            if korean_ratio < 0.4:
                recommendation = "ê´€ë ¨ ê·¼ê±°ê°€ ì£¼ë¡œ í•´ì™¸ ì¶œì²˜ì…ë‹ˆë‹¤. í•œêµ­ ì–¸ë¡ ì‚¬ ë³´ë„ë¥¼ ì¶”ê°€ í™•ì¸í•˜ì„¸ìš”."
            elif len(cleaned_text) < 100:
                recommendation = "ì§§ì€ í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤. ë” ë§ì€ ë§¥ë½ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            else:
                recommendation = "ì´ ì •ë³´ëŠ” ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            level = "ë‚®ìŒ"
            if korean_ratio < 0.2:
                recommendation = "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í•œêµ­ ì–¸ë¡ ì‚¬ ì¶œì²˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¶œì²˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            elif len(cleaned_text) < 100:
                recommendation = "ì§§ì€ í…ìŠ¤íŠ¸ë¡œ íŒë‹¨ì´ ì–´ë µìŠµë‹ˆë‹¤. ì „ì²´ ê¸°ì‚¬ë‚˜ ë” ë§ì€ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            else:
                recommendation = "ì´ ì •ë³´ëŠ” ì‹ ë¢°í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¶œì²˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        
        elapsed_time = time.time() - start_time
        
        # evidence ë°°ì—´ì— ëª…ì‹œì  ë²ˆí˜¸ ì¶”ê°€
        evidence_with_numbers = []
        for i, result in enumerate(results, start=1):
            evidence_item = result.copy()
            evidence_item["number"] = i  # ëª…ì‹œì  ë²ˆí˜¸ í•„ë“œ ì¶”ê°€
            evidence_with_numbers.append(evidence_item)
        
        return {
            "success": True,
            "reliability_score": reliability_score,
            "reliability_level": level,
            "recommendation": recommendation,
            "evidence": evidence_with_numbers,
            "elapsed_time": elapsed_time,
            "source_type": "text",
            "candidates_found": len(candidate_indices),
            "similarity_threshold": similarity_threshold
        }
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ í‰ê°€ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": f"í…ìŠ¤íŠ¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }

def evaluate_url(query_url: str, nli_batch: int, use_gpu: bool, fp16: bool, similarity_threshold: float = 0.35):
    if SESSION is None:
        configure_http(http_pool=64, timeout=12)

    pack = load_index()
    embedder, _ = get_embedder(use_gpu=use_gpu, fp16=fp16)

    logger.info("í‰ê°€ URL íŒŒì‹±: %s", query_url)

    # ë„¤ì´ë²„ëŠ” ëª¨ë°”ì¼ UAê°€ ìœ ë¦¬í•œ ê²½ìš°ê°€ ìˆìŒ: ëª¨ë°”ì¼ â†’ ë°ìŠ¤í¬í†±
    host = domain_of(query_url)
    html = None
    if host.endswith("n.news.naver.com") or host.endswith("news.naver.com"):
        html = polite_get(query_url, mobile=True) or polite_get(query_url)
    else:
        html = polite_get(query_url) or polite_get(query_url, mobile=True)

    q_text, q_dt, q_title = extract_text(query_url, html)
    if len(q_text) < 50:
        print("ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ(ë˜ëŠ” í•œê¸€ ë¹„ì¤‘ ë‚®ìŒ). URL/íŒŒì„œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)

    # ì‚¬ìš©ì ì…ë ¥ URLì„ ì¸ë±ìŠ¤ì— ì¶”ê°€ (ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš°)
    try:
        if add_url_to_index(query_url, q_text, q_dt, q_title, embedder, pack):
            save_index(pack)
            logger.info("ì‚¬ìš©ì URLì´ ì¸ë±ìŠ¤ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            logger.debug("URLì´ ì´ë¯¸ ì¸ë±ìŠ¤ì— ì¡´ì¬í•˜ì—¬ ì¶”ê°€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.warning(f"URL ì¸ë±ìŠ¤ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")

    q_chunks = make_chunks(q_text, min_len=max(120, MIN_TEXT_LEN // 2))
    if not q_chunks:
        q_chunks = [q_text]
    logger.info("ì§ˆì˜ ì²­í¬ ìˆ˜: %d", len(q_chunks))

    q_vecs = embedder.encode(q_chunks, convert_to_numpy=True, normalize_embeddings=True)
    sims = util.cos_sim(torch.tensor(q_vecs), torch.from_numpy(pack.matrix)).cpu().numpy()  # (Q,N)
    sim_per_idx = sims.max(axis=0)

    # í›„ë³´ TopK
    K = min(TOPK_CANDIDATES, pack.matrix.shape[0])
    cand_idx = np.argsort(-sim_per_idx)[:K].tolist()

    tok, mdl, use_fp16 = get_nli(use_gpu=use_gpu, fp16=fp16)
    q_premise = summarize_for_nli(q_text, max_sents=3)

    # ë°˜ë°• ì¦ê±° ê²€ìƒ‰ ì¶”ê°€
    logger.debug("ë°˜ë°• ì¦ê±° ê²€ìƒ‰ ì‹œì‘...")
    contradiction_evidence = search_contradiction_evidence(
        query_url, q_text, pack.matrix, pack.records, embedder, k=3
    )
    
    pairs = [(pack.records[idx].chunk, q_premise) for idx in cand_idx]
    probs = nli_batch_probs(pairs, tok, mdl, batch_size=nli_batch, use_fp16=use_fp16)  # [N,3]
    c_prob = probs[:, 0] if probs.size else np.zeros((len(cand_idx),), dtype=np.float32)  # contradiction
    e_prob = probs[:, 2] if probs.size else np.zeros((len(cand_idx),), dtype=np.float32)  # entailment

    q_lang_kr = korean_ratio(q_text)

    scored = []
    for rank, idx in enumerate(cand_idx):
        rec = pack.records[idx]
        sim_v = float(sim_per_idx[idx])
        sup_v = float(e_prob[rank])
        con_v = float(c_prob[rank])
        
        # ê¸°ë³¸ í•„í„°ë§: ë„ˆë¬´ ë‚®ì€ ìœ ì‚¬ì„±ì´ë‚˜ NLI ì§€ì§€ë„ëŠ” ì œì™¸
        if sim_v < similarity_threshold or sup_v < MIN_NLI_SUPPORT_THRESHOLD:
            continue
        
        # ì–¸ì–´/ì§€ì—­ í•„í„°ë§ ê°•í™”: í•œêµ­ì–´ ê¸°ì‚¬ì¸ ê²½ìš° ì™¸êµ­ ì‚¬ì´íŠ¸ ì œí•œ
        rec_domain = domain_of(rec.url)
        
        # í•œêµ­ ì‚¬ì´íŠ¸ íŒë³„ (ë” ì—„ê²©í•˜ê²Œ)
        korean_domains = [
            'naver.com', 'daum.net', 'chosun.com', 'joins.com', 'donga.com',
            'hani.co.kr', 'khan.co.kr', 'ytn.co.kr', 'jtbc.co.kr', 'sbs.co.kr',
            'kbs.co.kr', 'mbc.co.kr', 'news1.kr', 'newsis.com', 'edaily.co.kr',
            'mk.co.kr', 'hankyung.com', 'korea.kr', 'koreaherald.com', 'koreatimes.co.kr',
            'koreajoongangdaily.joins.com', 'pressian.com', 'ohmynews.com'
        ]
        
        is_korean_site = any(korean_domain in rec_domain for korean_domain in korean_domains)
        is_foreign_site = not is_korean_site and any(tld in rec_domain for tld in ['.fr', '.de', '.it', '.es', '.com', '.net', '.org'])
        
        # í•œêµ­ì–´ ë¹„ì¤‘ì´ ë†’ì€ ì§ˆì˜ì˜ ê²½ìš° ì™¸êµ­ ì‚¬ì´íŠ¸ ê°•ë ¥ ì œí•œ
        if q_lang_kr >= 0.3:  # í•œêµ­ì–´ ë¹„ì¤‘ 30% ì´ìƒ
            if is_foreign_site:
                # ì™¸êµ­ ì‚¬ì´íŠ¸ì´ì§€ë§Œ í•œêµ­ ê´€ë ¨ ë‚´ìš©ì¸ì§€ ë§¤ìš° ì—„ê²©í•˜ê²Œ í™•ì¸
                korean_keywords = ['í•œêµ­', 'ëŒ€í•œë¯¼êµ­', 'ì„œìš¸', 'ë¶€ì‚°', 'ì •ë¶€', 'ëŒ€í†µë ¹', 'êµ­ì •ê°ì‚¬', 'êµ­íšŒ', 'ì²­ì™€ëŒ€', 'Korea', 'South Korea', 'Seoul']
                has_strong_korean_context = sum(1 for keyword in korean_keywords if keyword in rec.chunk) >= 2  # 2ê°œ ì´ìƒ í‚¤ì›Œë“œ í•„ìš”
                
                if not has_strong_korean_context:
                    # í•œêµ­ ë§¥ë½ì´ ì•½í•œ ì™¸êµ­ ê¸°ì‚¬ëŠ” ì œì™¸
                    continue
                    
                # í•œêµ­ ë§¥ë½ì´ ìˆì–´ë„ í˜ë„í‹° ì ìš©
                sim_v *= 0.7  # ìœ ì‚¬ì„±ì— í˜ë„í‹°
        
        # ë‚´ìš© ê´€ë ¨ì„± ê°•í™”: ì£¼ìš” í‚¤ì›Œë“œ ë§¤ì¹­ ì ê²€ (ê°•í™”ëœ ë²„ì „)
        content_relevance = 1.0
        
        # ë” ì •êµí•œ í‚¤ì›Œë“œ ê´€ë ¨ì„± ê²€ì¦ ì‚¬ìš©
        is_relevant = check_keyword_relevance(q_text, rec.chunk, min_common_keywords=2)
        
        # ê¸°ì¡´ ë°©ì‹ë„ ë³‘í–‰ (í˜¸í™˜ì„± ìœ ì§€)
        q_keywords = set()
        import re
        # í•œê¸€ 2ê¸€ì ì´ìƒ ë‹¨ì–´ë“¤
        korean_words = re.findall(r'[ê°€-í£]{2,}', q_text)
        for word in korean_words[:10]:  # ìƒìœ„ 10ê°œë§Œ
            if len(word) >= 2 and word not in ['ê²ƒì€', 'ìˆë‹¤', 'í•œë‹¤', 'ëœë‹¤', 'ì´ë‹¤', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜']:
                q_keywords.add(word)
        
        # ì˜ì–´ ë‹¨ì–´ë“¤ë„ ì¶”ê°€
        english_words = re.findall(r'[A-Za-z]{3,}', q_text)
        for word in english_words[:5]:  # ìƒìœ„ 5ê°œë§Œ
            if word.lower() not in ['the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'was', 'one', 'our', 'has']:
                q_keywords.add(word.lower())
        
        if q_keywords:
            # í›„ë³´ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°
            rec_text_lower = rec.chunk.lower()
            matched_keywords = 0
            for keyword in q_keywords:
                if keyword.lower() in rec_text_lower:
                    matched_keywords += 1
            
            keyword_match_ratio = matched_keywords / len(q_keywords) if q_keywords else 0
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ ì œì™¸ (0% ë§¤ì¹­ í—ˆìš© ì•ˆí•¨)
            if keyword_match_ratio == 0.0:
                logger.debug(f"í‚¤ì›Œë“œ ë§¤ì¹­ 0%ë¡œ ì¦ê±°ì—ì„œ ì œì™¸: {rec.url}")
                continue
            
            # ìƒˆë¡œìš´ ê´€ë ¨ì„± ê²€ì¦ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ë‚®ìœ¼ë©´ ê°•í•œ í˜ë„í‹°
            if not is_relevant or keyword_match_ratio < 0.15:  # ê¸°ì¤€ ê°•í™”: 10% â†’ 15%
                content_relevance = 0.5  # ë” ê°•í•œ í˜ë„í‹°: 0.8 â†’ 0.5
                logger.debug(f"ê´€ë ¨ì„± ë‚®ìŒ: ìƒˆê²€ì¦={is_relevant}, í‚¤ì›Œë“œë§¤ì¹­={keyword_match_ratio:.2f}")
            elif keyword_match_ratio < 0.25:  # 25% ë¯¸ë§Œë„ í˜ë„í‹°
                content_relevance = 0.7
                logger.debug(f"ê´€ë ¨ì„± ë³´í†µ: í‚¤ì›Œë“œë§¤ì¹­={keyword_match_ratio:.2f}")
            elif keyword_match_ratio < 0.2:  # 20% ë¯¸ë§Œ ë§¤ì¹­
                content_relevance = 0.9
        
        dt = datetime.fromtimestamp(rec.published, tz=timezone.utc) if rec.published else None
        time_v = time_weight(dt)
        src_v = source_reputation(rec.url, rec.from_seed)
        
        # ì–¸ì–´ ì •í•© ê°€ì¤‘: ì§ˆì˜ê°€ í•œê¸€ ë¹„ì¤‘ ë†’ìœ¼ë©´ í•œê¸€ ë¹„ì¤‘ ë†’ì€ ì²­í¬ì— ë³´ë„ˆìŠ¤
        lang_align = 1.0
        if q_lang_kr >= 0.25:
            lang_align = 0.8 + 0.2 * (1.0 if korean_ratio(rec.chunk) >= 0.25 else 0.0)
        lang_v = (lang_align - 1.0)  # -0.2 ~ 0.0
        
        score = (ALPHA_SIM * sim_v) + (BETA_SUP * sup_v) - (GAMMA_CONTRA * con_v) \
                + (DELTA_TIME * time_v) + (EPS_SOURCE * src_v) + (EPS_LANG * lang_v)
        
        # ë‚´ìš© ê´€ë ¨ì„± ë³´ì • ì ìš©
        score *= content_relevance
        
        # ìµœì¢… ì ìˆ˜ ì„ê³„ê°’ ì ìš©
        if score >= MIN_FINAL_SCORE:
            scored.append((idx, score, {"url": rec.url, "similarity": sim_v, "support": sup_v}))

    # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ë¡œ ë³€ê²½ (ìµœì¢… ì ìˆ˜ ëŒ€ì‹  ìœ ì‚¬ë„ ìš°ì„ )
    scored.sort(key=lambda x: x[2]["similarity"], reverse=True)

    # ê°œì„ ëœ ì¤‘ë³µ ì œê±°: URL ìœ ì‚¬ì„±ê³¼ ë‚´ìš© ìœ ì‚¬ì„± ëª¨ë‘ ê³ ë ¤
    seen = set()
    uniq_top = []
    url_groups = {}  # URL ê·¸ë£¹ë³„ë¡œ ìµœê³  ì ìˆ˜ë§Œ ìœ ì§€
    
    for idx, s, meta in scored:
        u = meta["url"]
        canonical_u = canonical_url(u)
        
        # 1) ì •í™•íˆ ê°™ì€ URLì€ ì œì™¸
        if canonical_u in seen:
            continue
        
        # 2) URL ìœ ì‚¬ì„± ê²€ì‚¬
        is_similar = False
        for existing_url in seen:
            if url_similarity(canonical_u, existing_url) >= 0.9:
                is_similar = True
                break
        
        if is_similar:
            continue
        
        # 3) ë„ë©”ì¸ë³„ ê·¸ë£¹í•‘ - ê°™ì€ ë„ë©”ì¸ì—ì„œ ë„ˆë¬´ ë§ì€ ê²°ê³¼ ë°©ì§€
        domain = domain_of(u)
        if domain not in url_groups:
            url_groups[domain] = []
        
        # ê°™ì€ ë„ë©”ì¸ì—ì„œ ì´ë¯¸ 2ê°œ ì´ìƒ ì„ íƒë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µ (ë§¤ìš° ë†’ì€ ì ìˆ˜ê°€ ì•„ë‹Œ ê²½ìš°)
        if len(url_groups[domain]) >= 2 and s < 2.0:
            continue
        
        url_groups[domain].append((idx, s, meta))
        uniq_top.append((idx, s, meta))
        seen.add(canonical_u)
        
        if len(uniq_top) >= TOPN_RETURN:
            break

    if not uniq_top:
        print("============================================================")
        print("ğŸ“Š ì‹ ë¢°ë„ ìƒì„¸ ë¶„ì„")
        print("============================================================")
        print("â€¢ ë‚´ìš© ì¼ê´€ì„±: 0% (ê°€ì¤‘ì¹˜ 40%)")
        print("â€¢ ì¶œì²˜ ë‹¤ì–‘ì„±: 0% (ê°€ì¤‘ì¹˜ 25%)")
        print("â€¢ ì‹œê°„ì  ê´€ë ¨ì„±: 0% (ê°€ì¤‘ì¹˜ 20%)")
        print("â€¢ ê·¼ê±° í’ˆì§ˆ: 0% (ê°€ì¤‘ì¹˜ 15%)")
        print("")
        print("ì—°ê´€ì„± ë†’ì€ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("")
        print("============================================================")
        print("ğŸ¯ ìµœì¢… í‰ê°€ ê²°ê³¼")
        print("============================================================")
        print("ì‹ ë¢°ë„: ê´€ë ¨ëœ ìë£Œë¥¼ ì°¾ì§€ ëª» í•˜ì˜€ìŠµë‹ˆë‹¤.")
        print("ê¶Œì¥ì‚¬í•­: í—ˆìœ„ì •ë³´ í˜¹ì€ ì˜¤ë³´ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë‹ˆ, ê³µì‹ ì¶œì²˜ë¥¼ í†µí•´ ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("")
        print("ğŸ“‹ ì‹ ë¢°ë„ í•´ì„ ê°€ì´ë“œ (ì¡°ì •ëœ ê¸°ì¤€)")
        print("----------------------------------------")
        print("â€¢ 80% ì´ìƒ: ë§¤ìš° ë†’ìŒ - ì‹ ë¢° ê°€ëŠ¥")
        print("â€¢ 65-79%: ë†’ìŒ - ëŒ€ì²´ë¡œ ì‹ ë¢° ê°€ëŠ¥, ì¶”ê°€ ê²€ì¦ ê¶Œì¥")
        print("â€¢ 50-64%: ë³´í†µ - ì‹ ì¤‘í•œ ê²€í†  í•„ìš”")
        print("â€¢ 35-49%: ë‚®ìŒ - ì˜¤ë³´ ì˜ì‹¬, ë‹¤ë¥¸ ì¶œì²˜ í™•ì¸ í•„ìš”")
        print("â€¢ 35% ë¯¸ë§Œ: ë§¤ìš° ë‚®ìŒ - í—ˆìœ„ì •ë³´ í˜¹ì€ ì˜¤ë³´ ì˜ì‹¬")
        print("============================================================")
        sys.exit(0)  # ì •ìƒ ì¢…ë£Œë¡œ ë³€ê²½

    total_score = sum(s for _, s, __ in uniq_top)
    base_trust_prob = 1 / (1 + math.exp(-total_score))
    
    # ë‹¤ì°¨ì› ì‹ ë¢°ë„ í‰ê°€
    reliability_factors = {
        'content_consistency': base_trust_prob,  # ê¸°ë³¸ ì¼ê´€ì„± ì ìˆ˜
        'source_diversity': 0.0,                # ì¶œì²˜ ë‹¤ì–‘ì„±
        'temporal_relevance': 0.0,              # ì‹œê°„ì  ê´€ë ¨ì„±
        'evidence_quality': 0.0                 # ê·¼ê±° í’ˆì§ˆ
    }
    
    # 1. ì¶œì²˜ ë‹¤ì–‘ì„± í‰ê°€
    unique_domains = set()
    government_sources = 0
    media_sources = 0
    total_articles = len(uniq_top)
    
    for idx, s, meta in uniq_top:
        url = meta['url']
        domain = url.split('/')[2] if '//' in url else url
        unique_domains.add(domain)
        
        # ì •ë¶€/ê³µê³µê¸°ê´€ ì¶œì²˜
        if any(gov_domain in domain for gov_domain in ['korea.kr', 'mofa.go.kr', 'mois.go.kr', 'gov.kr']):
            government_sources += 1
        # ì–¸ë¡ ì‚¬ ì¶œì²˜  
        elif any(media_domain in domain for media_domain in ['yna.co.kr', 'ytn.co.kr', 'jtbc.co.kr', 'naver.com', 'hankyung.com']):
            media_sources += 1
    
    # ì¶œì²˜ ë‹¤ì–‘ì„± ì ìˆ˜ (0~1)
    domain_diversity = min(1.0, len(unique_domains) / max(1, total_articles))
    source_balance = 0.5 if government_sources > 0 and media_sources > 0 else 0.3
    reliability_factors['source_diversity'] = (domain_diversity + source_balance) / 2
    
    # 2. ì‹œê°„ì  ê´€ë ¨ì„± í‰ê°€ (í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬ í¬í•¨)
    very_old_count = 0
    old_count = 0
    recent_count = 0
    
    # í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬ì˜ ì—°ë„ ë¨¼ì € í™•ì¸
    query_year = None
    if 'jtbc.co.kr' in query_url:
        jtbc_match = re.search(r'NB(\d{2})', query_url)
        if jtbc_match:
            year_suffix = int(jtbc_match.group(1))
            if year_suffix <= 25:
                query_year = 2000 + year_suffix
            else:
                query_year = 1900 + year_suffix
            logger.debug(f"í‰ê°€ ëŒ€ìƒ JTBC ê¸°ì‚¬ ì—°ë„: {query_url} -> {query_year}")
    
    # í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬ê°€ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ê°•ë ¥í•œ í˜ë„í‹°
    if query_year and query_year <= 2015:
        logger.debug(f"í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬ê°€ ë§¤ìš° ì˜¤ë˜ë¨: {query_year} - ì‹œê°„ì  ê´€ë ¨ì„±ì„ 0.1ë¡œ ì„¤ì •")
        reliability_factors['temporal_relevance'] = 0.1  # ë§¤ìš° ê°•í•œ í˜ë„í‹°
    elif query_year and query_year <= 2020:
        logger.debug(f"í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬ê°€ ì˜¤ë˜ë¨: {query_year} - ì‹œê°„ì  ê´€ë ¨ì„±ì„ 0.3ìœ¼ë¡œ ì„¤ì •")
        reliability_factors['temporal_relevance'] = 0.3  # ê°•í•œ í˜ë„í‹°
    else:
        # ê¸°ì¡´ ê·¼ê±° ê¸°ì‚¬ë“¤ ê¸°ë°˜ í‰ê°€
        if total_articles > 0:
            for idx, s, meta in uniq_top:
                url = meta['url']
                import re
                
                logger.debug(f"ì—°ë„ ê°ì§€ ì‹œì‘: {url}")
                
                # ì—°ë„ ê°ì§€ ë¡œì§ (ê°œì„ ë¨)
                year_matches = re.findall(r'20(\d{2})', url)
                detected_year = None
                
                if 'jtbc.co.kr' in url:
                    # JTBC íŒ¨í„´: NB11272032 -> 11ì€ 2011ë…„
                    jtbc_match = re.search(r'NB(\d{2})', url)
                    if jtbc_match:
                        year_suffix = int(jtbc_match.group(1))
                        if year_suffix <= 25:  # 00-25ëŠ” 2000-2025
                            detected_year = 2000 + year_suffix
                        else:  # 26-99ëŠ” 1926-1999 (í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ê±°ì˜ ì—†ìŒ)
                            detected_year = 1900 + year_suffix
                        logger.debug(f"JTBC ì—°ë„ ê°ì§€: {url} -> {detected_year} (year_suffix: {year_suffix})")
                
                if not detected_year and year_matches:
                    for year_suffix in year_matches:
                        year_candidate = int('20' + year_suffix)
                        if 2000 <= year_candidate <= 2025:
                            detected_year = year_candidate
                            logger.debug(f"ì¼ë°˜ ì—°ë„ ê°ì§€: {url} -> {detected_year}")
                            break
                
                if not detected_year and 'korea.kr' in url and '132038018' in url:
                    detected_year = 2016
                    logger.debug(f"korea.kr íŠ¹ìˆ˜ ì²˜ë¦¬: {url} -> {detected_year}")
                
                logger.debug(f"ìµœì¢… ê°ì§€ëœ ì—°ë„: {url} -> {detected_year}")
                
                if detected_year:
                    if detected_year <= 2015:  # 2015ë…„ ì´ì „ (ë” ì—„ê²©)
                        very_old_count += 1
                        old_count += 1
                        logger.debug(f"ë§¤ìš° ì˜¤ë˜ëœ ê¸°ì‚¬ë¡œ ë¶„ë¥˜: {detected_year}")
                    elif detected_year <= 2020:  # 2020ë…„ ì´ì „ (ì¡°ì •)
                        old_count += 1
                        logger.debug(f"ì˜¤ë˜ëœ ê¸°ì‚¬ë¡œ ë¶„ë¥˜: {detected_year}")
                    else:
                        recent_count += 1
                        logger.debug(f"ìµœì‹  ê¸°ì‚¬ë¡œ ë¶„ë¥˜: {detected_year}")
                else:
                    logger.debug(f"ì—°ë„ ê°ì§€ ì‹¤íŒ¨: {url}")
            
            logger.debug(f"ì—°ë„ë³„ ë¶„ë¥˜ ê²°ê³¼ - ë§¤ìš° ì˜¤ë˜ë¨: {very_old_count}, ì˜¤ë˜ë¨: {old_count}, ìµœì‹ : {recent_count}, ì „ì²´: {total_articles}")
            
            # ì‹œê°„ì  ê´€ë ¨ì„± ì ìˆ˜ (ë” ì—„ê²©í•œ ê¸°ì¤€ìœ¼ë¡œ ê°•í™”)
            recent_ratio = recent_count / total_articles
            old_ratio = old_count / total_articles
            very_old_ratio = very_old_count / total_articles
            
            if very_old_ratio > 0.8:  # ë§¤ìš° ì˜¤ë˜ëœ ê¸°ì‚¬ 80% ì´ìƒ
                reliability_factors['temporal_relevance'] = 0.1  # ë§¤ìš° ë‚®ìŒ (ê°•í™”)
            elif very_old_ratio > 0.6:  # ë§¤ìš° ì˜¤ë˜ëœ ê¸°ì‚¬ 60% ì´ìƒ
                reliability_factors['temporal_relevance'] = 0.2  # ë§¤ìš° ë‚®ìŒ (ê°•í™”)
            elif very_old_ratio > 0.4:  # ë§¤ìš° ì˜¤ë˜ëœ ê¸°ì‚¬ 40% ì´ìƒ
                reliability_factors['temporal_relevance'] = 0.3  # ë‚®ìŒ (ê°•í™”)
            elif very_old_ratio > 0.2:  # ë§¤ìš° ì˜¤ë˜ëœ ê¸°ì‚¬ 20% ì´ìƒ
                reliability_factors['temporal_relevance'] = 0.4  # ë‚®ìŒ (ê°•í™”)
            elif old_ratio > 0.6:
                reliability_factors['temporal_relevance'] = 0.6  # ë³´í†µ (ì¡°ì •)
            else:
                reliability_factors['temporal_relevance'] = 0.9  # ë†’ìŒ
    
    # 3. ê·¼ê±° í’ˆì§ˆ í‰ê°€
    high_similarity_count = sum(1 for _, s, meta in uniq_top if meta.get('similarity', 0) > 0.7)
    high_support_count = sum(1 for _, s, meta in uniq_top if meta.get('support', 0) > 0.8)
    
    similarity_quality = high_similarity_count / max(1, total_articles)
    support_quality = high_support_count / max(1, total_articles)
    
    # 4. ê·¹ë‹¨ì  ì£¼ì¥ íƒì§€ (ìƒˆë¡œ ì¶”ê°€)
    extreme_claim_penalty = 0.0
    
    # ì§ˆì˜ í…ìŠ¤íŠ¸ì—ì„œ ê·¹ë‹¨ì  í‘œí˜„ íƒì§€
    query_text = q_text + " " + (q_title or "")
    logger.debug(f"í—ˆìœ„ë‰´ìŠ¤ íŒ¨í„´ ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸: {query_text[:200]}...")
    
    # ê°œì„ ëœ í—ˆìœ„ë‰´ìŠ¤ íŒ¨í„´ íƒì§€ (ì¼ë°˜ì  íŒ¨í„´)
    # 1. ì¼ë°˜ì ì¸ í—ˆìœ„ë‰´ìŠ¤ íŠ¹ì§• íŒ¨í„´ (3ê°œ í‚¤ì›Œë“œ ì¡°í•©)
    fake_patterns = [
        # ì˜¤ë³´/ì •ì • ê´€ë ¨ íŒ¨í„´ (ìƒˆë¡œ ì¶”ê°€)
        ('ì˜¤ë³´', 'ì •ì •', 'ì‚¬ê³¼'),
        ('ì˜¤ì—­', 'ì˜ëª»', 'ì¸ì •'),
        ('ê°€ì§œ', 'í—ˆìœ„', 'ì¡°ì‘'),
        ('ë°©ì‹¬ìœ„', 'ê²½ê³ ', 'ì§•ê³„'),
        ('ë°”ë¡œì¡', 'ìˆ˜ì •', 'ì •ì •ë³´ë„'),
        
        # ì •ë¶€ ì •ì±… ê´€ë ¨ ë¹„í˜„ì‹¤ì  íŒ¨í„´
        ('ëª¨ë“  êµ­ë¯¼', '1ì¼ 2ì‹œê°„', 'ë²•ì•ˆ'),  # ê°€ì§œë‰´ìŠ¤ íŠ¹ì • ì¼€ì´ìŠ¤
        ('ëª¨ë“  êµ­ë¯¼', 'ìë™ ì„¤ì¹˜', 'ë²Œê¸ˆ'),
        ('ëª¨ë“ ', 'ê°•ì œ', 'ë²•ì•ˆ'),
        ('ì „ êµ­ë¯¼', 'ì˜ë¬´', 'ì²˜ë²Œ'),
        
        # ê·¹ë‹¨ì  ìˆ˜ì¹˜/ì‹œê°„ ì¡°í•©
        ('100%', 'ì¦‰ì‹œ', 'íš¨ê³¼'),
        ('24ì‹œê°„', 'ì™„ì „', 'ì¹˜ë£Œ'),
        ('í•˜ë£¨', '10kg', 'ê°ëŸ‰'),
        ('1ì¼', 'ì°¨ë‹¨', 'ë²Œê¸ˆ'),
        
        # ì˜ë£Œ/ê±´ê°• í—ˆìœ„ì •ë³´ íŒ¨í„´  
        ('ì•”', 'ì™„ì¹˜', 'ë¹„ë²•'),
        ('ë‹¹ë‡¨', 'í•˜ë£¨', 'ì™„ì „'),
        ('ì½”ë¡œë‚˜', 'ì˜ˆë°©', '100%'),
        
        # ê²½ì œ/íˆ¬ì ì‚¬ê¸° íŒ¨í„´
        ('ë¬´ì¡°ê±´', 'ìˆ˜ìµ', 'ë³´ì¥'),
        ('í•˜ë£¨', 'ë°±ë§Œì›', 'ë²Œê¸°'),
        ('íˆ¬ì', 'ì›ê¸ˆë³´ì¥', 'ê³ ìˆ˜ìµ'),
        
        # ì„ ì •ì /ì„ ë™ì  í‘œí˜„ (2ê°œ í‚¤ì›Œë“œ ì¡°í•©)
        ('ì¶©ê²©', 'ì§„ì‹¤'),
        ('ì ˆëŒ€', 'ë¯¿ì„ ìˆ˜ ì—†ëŠ”'),
        ('êµ­ê°€ê¸°ë°€', 'ìµœì´ˆê³µê°œ'),
        ('ìë™', 'ì°¨ë‹¨'),
        ('ê°•ì œ', 'ëª¨ë‹ˆí„°ë§')
    ]
    
    # 2. í—ˆìœ„ë‰´ìŠ¤ íŠ¹ì§•ì  íŒ¨í„´ ì ìˆ˜
    fake_pattern_score = 0
    detected_patterns = []
    
    for pattern in fake_patterns:
        if len(pattern) == 3:
            # 3ê°œ í‚¤ì›Œë“œ ì¡°í•©
            if all(keyword in query_text.lower() for keyword in pattern):
                fake_pattern_score += 3  # 3ê°œ ì¡°í•©ì€ ë†’ì€ ì ìˆ˜
                detected_patterns.append(pattern)
        elif len(pattern) == 2:
            # 2ê°œ í‚¤ì›Œë“œ ì¡°í•©  
            if all(keyword in query_text.lower() for keyword in pattern):
                fake_pattern_score += 1.5  # 2ê°œ ì¡°í•©ì€ ì¤‘ê°„ ì ìˆ˜
                detected_patterns.append(pattern)
    
    # 3. ê°œë³„ í—ˆìœ„ë‰´ìŠ¤ ì˜ì‹¬ í‚¤ì›Œë“œ (ì •êµí•œ í•„í„°ë§)
    suspicious_keywords = [
        # ê·¹ë‹¨ì  ìˆ˜ì¹˜ í‘œí˜„ (ì •ìƒ ê¸°ì‚¬ì—ì„œ ì˜ ì•ˆ ë‚˜ì˜´)
        '100%', 'ì™„ì „', 'ì „ë©´',
        # ì„ ì •ì  í‘œí˜„ (ë‰´ìŠ¤ì—ì„œ ìì£¼ ì“°ì´ì§€ ì•ŠìŒ)  
        'ì¶©ê²©', 'ë†€ë¼ìš´', 'ë¯¿ì„ ìˆ˜ ì—†ëŠ”', 'í­ë¡œ',
        # ì˜ë£Œ ê´€ë ¨ ê³¼ì¥ (ëª…í™•í•œ í—ˆìœ„ ì‹ í˜¸)
        'ì™„ì¹˜', 'íš¨ê³¼ 100%', 'ì¦‰ì‹œ', 'í•˜ë£¨ë§Œì—',
        # ê²½ì œ ê´€ë ¨ ì‚¬ê¸° í‘œí˜„
        'ì›ê¸ˆë³´ì¥', 'ë¬´ì†ì‹¤', 'í™•ì‹¤í•œ ìˆ˜ìµ', 'ëŒ€ë°•',
        # ì •ë¶€ ì •ì±… ê´€ë ¨ ë¹„í˜„ì‹¤ì  í‘œí˜„
        'ì „ êµ­ë¯¼', 'ëª¨ë“  êµ­ë¯¼', 'ì¼ê´„ ì ìš©'
        # 'ê°•ì œ', 'ì ˆëŒ€', 'ë¬´ì¡°ê±´' ë“±ì€ ì œê±° (ì •ìƒ ê¸°ì‚¬ì—ì„œë„ ìì£¼ ì‚¬ìš©)
    ]
    
    mild_extreme_count = sum(1 for keyword in suspicious_keywords if keyword in query_text.lower())
    fake_pattern_score += mild_extreme_count * 0.1  # ê°œë³„ í‘œí˜„ì€ ë§¤ìš° ë‚®ì€ ê°€ì¤‘ì¹˜ (0.3 â†’ 0.1)
    
    logger.debug(f"í—ˆìœ„ë‰´ìŠ¤ íŒ¨í„´ ë¶„ì„ ê²°ê³¼: íŒ¨í„´ì ìˆ˜={fake_pattern_score}, íƒì§€íŒ¨í„´={detected_patterns}, ê°œë³„í‚¤ì›Œë“œ={mild_extreme_count}")
    
    # 4. í˜ë„í‹° ì ìš© (í—ˆìœ„ë‰´ìŠ¤ ì˜ì‹¬ ì‹œ ì‹ ë¢°ë„ ëŒ€í­ ê°ì†Œ)
    extreme_claim_penalty = 0.0
    global_extreme_penalty = 0.0
    
    if fake_pattern_score >= 4:  # ê°•í•œ í—ˆìœ„ë‰´ìŠ¤ ì˜ì‹¬
        extreme_claim_penalty = 0.8  # 80% í˜ë„í‹° (ê°•í™”)
        global_extreme_penalty = 0.5  # 50% ì „ì²´ í˜ë„í‹° (ê°•í™”)
        print(f"ğŸš¨ í—ˆìœ„ë‰´ìŠ¤ ê°•ë ¥ ì˜ì‹¬: ë¹„í˜„ì‹¤ì  íŒ¨í„´ ê°ì§€ (ì ìˆ˜: {fake_pattern_score}) - ì‹ ë¢°ë„ ëŒ€í­ ê°ì†Œ")
        if detected_patterns:
            print(f"   ê°ì§€ëœ íŒ¨í„´: {detected_patterns}")
    elif fake_pattern_score >= 2:  # ì¤‘ê°„ ì˜ì‹¬
        extreme_claim_penalty = 0.5  # 50% í˜ë„í‹° (ê°•í™”)
        global_extreme_penalty = 0.3  # 30% ì „ì²´ í˜ë„í‹° (ê°•í™”)
        print(f"âš ï¸ í—ˆìœ„ë‰´ìŠ¤ ì˜ì‹¬: ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‘œí˜„ íƒì§€ (ì ìˆ˜: {fake_pattern_score}) - ì‹ ë¢°ë„ ê°ì†Œ")
    elif fake_pattern_score >= 1:  # ì•½í•œ ì˜ì‹¬
        extreme_claim_penalty = 0.2  # 20% í˜ë„í‹°
        global_extreme_penalty = 0.1  # 10% ì „ì²´ í˜ë„í‹°
    
    reliability_factors['evidence_quality'] = max(0, (similarity_quality + support_quality) / 2 - extreme_claim_penalty)
    
    # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚° (ê°€ì¤‘ í‰ê· ) - ë°˜ë°• ì¦ê±°ëŠ” í˜ë„í‹° ì—†ì´ ê²½ê³ ë§Œ í‘œì‹œ
    weights = {
        'content_consistency': 0.35,   # 35% - ë‚´ìš© ì¼ê´€ì„± (40% â†’ 35%)
        'source_diversity': 0.25,     # 25% - ì¶œì²˜ ë‹¤ì–‘ì„±  
        'temporal_relevance': 0.25,   # 25% - ì‹œê°„ì  ê´€ë ¨ì„± (20% â†’ 25% ê°•í™”)
        'evidence_quality': 0.15      # 15% - ê·¼ê±° í’ˆì§ˆ
    }
    
    final_trust_prob = sum(reliability_factors[factor] * weights[factor] for factor in weights)
    
    # ê·¹ë‹¨ì  í‘œí˜„ í˜ë„í‹°ë§Œ ì ìš© (ë°˜ë°• ì¦ê±° í˜ë„í‹° ì œê±°)
    final_trust_prob = max(0, final_trust_prob - global_extreme_penalty)
    
    trust_percent = int(round(100 * final_trust_prob))

    # ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print("=" * 60)
    print("[ANALYSIS] ì‹ ë¢°ë„ ìƒì„¸ ë¶„ì„")
    print("=" * 60)
    print(f"- ë‚´ìš© ì¼ê´€ì„±: {reliability_factors['content_consistency']*100:.0f}% (ê°€ì¤‘ì¹˜ 40%)")
    print(f"- ì¶œì²˜ ë‹¤ì–‘ì„±: {reliability_factors['source_diversity']*100:.0f}% (ê°€ì¤‘ì¹˜ 25%)")
    print(f"- ì‹œê°„ì  ê´€ë ¨ì„±: {reliability_factors['temporal_relevance']*100:.0f}% (ê°€ì¤‘ì¹˜ 20%)")
    print(f"- ê·¼ê±° í’ˆì§ˆ: {reliability_factors['evidence_quality']*100:.0f}% (ê°€ì¤‘ì¹˜ 15%)")
    print()
    
    # ì‹ ë¢°ë„ êµ¬ê°„ë³„ í•´ì„ ë° ê¶Œì¥ì‚¬í•­ (ì¡°ì •ëœ ê¸°ì¤€)
    if trust_percent >= 80:  # 85% â†’ 80%ë¡œ ì¡°ì •
        trust_level = "ë§¤ìš° ë†’ìŒ"
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ì‹ ë¢°í•  ë§Œí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì¶œì²˜ì—ì„œ ì¼ê´€ëœ ì •ë³´ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤."
    elif trust_percent >= 65:  # 70% â†’ 65%ë¡œ ì¡°ì •
        trust_level = "ë†’ìŒ"
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ëŒ€ì²´ë¡œ ì‹ ë¢°í•  ë§Œí•˜ì§€ë§Œ, ì¶”ê°€ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    elif trust_percent >= 50:  # 55% â†’ 50%ìœ¼ë¡œ ì¡°ì •
        trust_level = "ë³´í†µ"
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ì‹ ì¤‘í•˜ê²Œ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì¶œì²˜ì™€ êµì°¨ í™•ì¸í•˜ì„¸ìš”."
    elif trust_percent >= 35:  # 40% â†’ 35%ë¡œ ì¡°ì •
        trust_level = "ë‚®ìŒ"
        recommendation = "ì´ ê¸°ì‚¬ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì˜¤ë³´ê°€ ì˜ì‹¬ë˜ë©°, ì •ë¶€ ê³µì‹ ë°œí‘œë‚˜ ê¶Œìœ„ ìˆëŠ” ì¶œì²˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."
    else:
        trust_level = "ë§¤ìš° ë‚®ìŒ"
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ì‹ ë¢°í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. í—ˆìœ„ì •ë³´ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."

    print("ì´ ê¸°ì‚¬(ìë£Œ)ì˜ ì‹ ë¢°ë„ í‰ê°€ ê·¼ê±° ë§í¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.")
    for i, (idx, s, meta) in enumerate(uniq_top, start=1):
        p = 1 / (1 + math.exp(-s))
        pct = int(round(100 * p))
        sim = meta.get("similarity", 0)
        sup = meta.get("support", 0)
        print(f"{i}. {pct}% : {meta['url']} (ìœ ì‚¬ì„±: {sim:.2f}, ì§€ì§€ë„: {sup:.2f})")
    
    # ë°˜ë°• ì¦ê±° í‘œì‹œ
    if contradiction_evidence:
        print("\nâš ï¸ ì˜¤ë³´ ê°€ëŠ¥ì„± ê´€ë ¨ ì •ë³´:")
        for i, evidence in enumerate(contradiction_evidence, start=1):
            print(f"   {i}. {evidence['url']}")
            print(f"      ë°˜ë°• í‚¤ì›Œë“œ: {evidence['contradiction_score']}ê°œ, ìœ ì‚¬ì„±: {evidence['similarity']:.2f}")
            preview = evidence['text'][:100].replace('\n', ' ')
            print(f"      ë‚´ìš©: {preview}...")
        print("   ğŸ’¡ ìœ„ ì •ë³´ë“¤ì€ ì´ ê¸°ì‚¬ì™€ ê´€ë ¨ëœ ì •ì •ì´ë‚˜ ë°˜ë°• ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆì–´ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
    print()
    
    # ìµœì¢… ê²°ê³¼
    print("=" * 60)
    print("[RESULT] ìµœì¢… í‰ê°€ ê²°ê³¼")
    print("=" * 60)
    print(f"ì‹ ë¢°ë„: {trust_percent}% - {trust_level}")
    print(f"ê¶Œì¥ì‚¬í•­: {recommendation}")
    print()
    
    # ì‹ ë¢°ë„ ê¸°ì¤€ ê°€ì´ë“œ
    print("[GUIDE] ì‹ ë¢°ë„ í•´ì„ ê°€ì´ë“œ (ì¡°ì •ëœ ê¸°ì¤€)")
    print("-" * 40)
    print("- 80% ì´ìƒ: ë§¤ìš° ë†’ìŒ - ì‹ ë¢° ê°€ëŠ¥")
    print("- 65-79%: ë†’ìŒ - ëŒ€ì²´ë¡œ ì‹ ë¢° ê°€ëŠ¥, ì¶”ê°€ ê²€ì¦ ê¶Œì¥")
    print("- 50-64%: ë³´í†µ - ì‹ ì¤‘í•œ ê²€í†  í•„ìš”")
    print("- 35-49%: ë‚®ìŒ - ì˜¤ë³´ ì˜ì‹¬, ë‹¤ë¥¸ ì¶œì²˜ í™•ì¸ í•„ìš”")
    print("- 35% ë¯¸ë§Œ: ë§¤ìš° ë‚®ìŒ - í—ˆìœ„ì •ë³´ í˜¹ì€ ì˜¤ë³´ ì˜ì‹¬")
    print("=" * 60)
    
    # JSON ê²°ê³¼ ë°˜í™˜
    evidence_list = []
    for i, (idx, s, meta) in enumerate(uniq_top, start=1):
        p = 1 / (1 + math.exp(-s))
        pct = int(round(100 * p))
        evidence_list.append({
            "number": i,
            "rank": i,
            "score": pct,
            "url": meta['url'],
            "similarity": meta.get("similarity", 0),
            "support": meta.get("support", 0)
        })
    
    return {
        "success": True,
        "reliability_score": trust_percent,
        "reliability_level": trust_level,
        "recommendation": recommendation,
        "evidence_count": len(evidence_list),
        "evidence": evidence_list
    }

# --------------------------------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description="Smart IT - ì‹ ë¢°ë„ í‰ê°€(ë³‘ë ¬/ë°°ì¹˜/GPU, Overall)")
    sub = parser.add_subparsers(dest="cmd", required=True)

    p_build = sub.add_parser("build-index", help="ì‹œë“œ í¬ë¡¤ë§ í›„ ì¸ë±ìŠ¤(pkl) ìƒì„±")
    p_build.add_argument("--workers", type=int, default=96, help="ì‹œë“œ ë³‘ë ¬ ì›Œì»¤ ìˆ˜ (Intel Ultra9 285k 32ìŠ¤ë ˆë“œ ìµœëŒ€ í™œìš©)")
    p_build.add_argument("--embed-batch", type=int, default=1024, help="ì„ë² ë”© ë°°ì¹˜ í¬ê¸° (RTX3070ti 8GB VRAM ìµœëŒ€ í™œìš©)")
    p_build.add_argument("--use-gpu", action="store_true", help="ê°€ëŠ¥í•˜ë©´ CUDA ì‚¬ìš©")
    p_build.add_argument("--fp16", action="store_true", help="ê°€ëŠ¥í•˜ë©´ FP16ë¡œ ì¶”ë¡ ")
    p_build.add_argument("--http-pool", type=int, default=1024, help="requests ì»¤ë„¥ì…˜ í’€ í¬ê¸° (128GB RAM ìµœëŒ€ í™œìš©)")
    p_build.add_argument("--sleep", type=float, default=0.001, help="í¬ë¡¤ ê°„ ëŒ€ê¸°(ì´ˆ) - ìµœê³ ì„±ëŠ¥ ì„¤ì •")
    p_build.add_argument("--timeout", type=int, default=12, help="ìš”ì²­ íƒ€ì„ì•„ì›ƒ(ì´ˆ)")
    p_build.add_argument("--fast-extract", action="store_true", help="ë³¸ë¬¸ ì¶”ì¶œ ê°€ì†(favor_precision=False)")
    p_build.add_argument("--test-mode", action="store_true", help="ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì†ŒëŸ‰ì˜ ì„ ë³„ëœ ì‹œë“œë§Œ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)")
    p_build.add_argument("--verbose", action="store_true", help="ìì„¸í•œ ë¡œê·¸")
    p_build.add_argument("--quiet", action="store_true", help="ê°„ë‹¨ ë¡œê·¸")
    p_build.add_argument("--log-file", type=str, default=None, help="ë¡œê·¸ íŒŒì¼ ê²½ë¡œ")

    p_check = sub.add_parser("check-domains", help="ì¸ë±ìŠ¤ì— í¬í•¨ëœ ë„ë©”ì¸ í™•ì¸")
    p_check.add_argument("--domain", type=str, help="íŠ¹ì • ë„ë©”ì¸ ê²€ìƒ‰ (ì˜ˆ: mediatoday)")
    p_check.add_argument("--verbose", action="store_true")
    
    p_eval = sub.add_parser("evaluate", help="URL ì‹ ë¢°ë„ í‰ê°€")
    p_eval.add_argument("--url", required=True, help="í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬/ìë£Œ URL")
    p_eval.add_argument("--nli-batch", type=int, default=32, help="NLI ë°°ì¹˜ í¬ê¸°")
    p_eval.add_argument("--use-gpu", action="store_true", default=True, help="ê°€ëŠ¥í•˜ë©´ CUDA ì‚¬ìš© (ê¸°ë³¸ê°’: True)")
    p_eval.add_argument("--fp16", action="store_true", default=True, help="ê°€ëŠ¥í•˜ë©´ FP16ë¡œ ì¶”ë¡  (ê¸°ë³¸ê°’: True)")
    p_eval.add_argument("--similarity-threshold", type=float, default=0.6, help="ê·¼ê±° ìœ ì‚¬ì„± ìµœì†Œ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.6)")
    p_eval.add_argument("--auto-threshold", action="store_true", help="ì£¼ì œë³„ ë™ì  ì„ê³„ê°’ ìë™ ì¡°ì •")
    p_eval.add_argument("--strict-mode", action="store_true", help="ì—„ê²© ëª¨ë“œ: ì„ê³„ê°’ 0.65 ì‚¬ìš© (ê³ í’ˆì§ˆ ê·¼ê±°ë§Œ)")
    p_eval.add_argument("--verbose", action="store_true")
    p_eval.add_argument("--quiet", action="store_true", default=True, help="ê°„ë‹¨ ë¡œê·¸ (ê¸°ë³¸ê°’: True)")
    p_eval.add_argument("--log-file", type=str, default=None)

    p_eval_img = sub.add_parser("evaluate-image", help="ì´ë¯¸ì§€ ì‹ ë¢°ë„ í‰ê°€")
    p_eval_img.add_argument("--image", required=True, help="í‰ê°€ ëŒ€ìƒ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ")
    p_eval_img.add_argument("--ocr-method", choices=["easyocr", "tesseract"], default="easyocr", help="OCR ë°©ë²• ì„ íƒ")
    p_eval_img.add_argument("--nli-batch", type=int, default=32, help="NLI ë°°ì¹˜ í¬ê¸°")
    p_eval_img.add_argument("--use-gpu", action="store_true", default=True, help="ê°€ëŠ¥í•˜ë©´ CUDA ì‚¬ìš© (ê¸°ë³¸ê°’: True)")
    p_eval_img.add_argument("--fp16", action="store_true", default=True, help="ê°€ëŠ¥í•˜ë©´ FP16ë¡œ ì¶”ë¡  (ê¸°ë³¸ê°’: True)")
    p_eval_img.add_argument("--similarity-threshold", type=float, default=0.5, help="ê·¼ê±° ìœ ì‚¬ì„± ìµœì†Œ ì„ê³„ê°’")
    p_eval_img.add_argument("--verbose", action="store_true")
    p_eval_img.add_argument("--quiet", action="store_true", default=True, help="ê°„ë‹¨ ë¡œê·¸ (ê¸°ë³¸ê°’: True)")
    p_eval_img.add_argument("--log-file", type=str, default=None)

    args = parser.parse_args()
    
    # ë¹Œë“œ ëª¨ë“œ ì—¬ë¶€ì— ë”°ë¼ ë¡œê¹… ì„¤ì • ì¡°ì •
    is_build_mode = (args.cmd == "build-index")
    setup_logging(verbose=getattr(args, "verbose", False),
                  quiet=getattr(args, "quiet", False),
                  log_file=getattr(args, "log_file", None),
                  build_mode=is_build_mode)

    if args.cmd == "build-index":
        build_index(
            workers=args.workers,
            embed_batch=args.embed_batch,
            use_gpu=args.use_gpu,
            fp16=args.fp16,
            http_pool=args.http_pool,
            timeout=args.timeout,
            sleep=args.sleep,
            fast_extract=args.fast_extract,
            test_mode=args.test_mode
        )
    elif args.cmd == "check-domains":
        check_domains(domain_filter=args.domain, verbose=args.verbose)
    elif args.cmd == "evaluate":
        # ë™ì  ì„ê³„ê°’ ì¡°ì •
        threshold = args.similarity_threshold
        if args.strict_mode:
            threshold = 0.6  # ì—„ê²©í•œ ê¸°ì¤€ìœ¼ë¡œ ë³µêµ¬
            print(f"ğŸ”’ ì—„ê²© ëª¨ë“œ: ìœ ì‚¬ì„± ì„ê³„ê°’ {threshold} ì‚¬ìš© (ê³ í’ˆì§ˆ ê·¼ê±°ë§Œ í‘œì‹œ)")
        elif args.auto_threshold:
            # ì„ì‹œë¡œ HTTP ì„¤ì •
            if SESSION is None:
                configure_http(http_pool=64, timeout=12)
            
            # í•œêµ­ì–´ ë¹„ì¤‘ì´ ë†’ìœ¼ë©´ ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
            html = polite_get(args.url)
            if html:
                text, _, _ = extract_text(args.url, html)
                kr_ratio = korean_ratio(text)
                if kr_ratio >= 0.5:  # í•œêµ­ì–´ 50% ì´ìƒ
                    threshold = 0.5
                elif kr_ratio >= 0.3:  # í•œêµ­ì–´ 30% ì´ìƒ
                    threshold = 0.4
                print(f"ğŸ¤– ìë™ ì¡°ì •: í•œêµ­ì–´ ë¹„ì¤‘ {kr_ratio:.1%}, ì„ê³„ê°’ {threshold} ì‚¬ìš©")
        else:
            print(f"[INFO] ê¸°ë³¸ ì„¤ì •: ìœ ì‚¬ì„± ì„ê³„ê°’ {threshold} ì‚¬ìš©")
        
        result = evaluate_url(
            query_url=args.url,
            nli_batch=args.nli_batch,
            use_gpu=args.use_gpu,
            fp16=args.fp16,
            similarity_threshold=threshold
        )
        
        # JSON ê²°ê³¼ë„ ì¶œë ¥ (API íŒŒì‹±ìš©)
        if result:
            import json
            print(f"\nJSON_RESULT:{json.dumps(result, ensure_ascii=False)}")
        
        return result
    elif args.cmd == "evaluate-image":
        # OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
        if not IMAGE_OCR_AVAILABLE:
            print("âŒ ì´ë¯¸ì§€ OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
            print("pip install pillow pytesseract easyocr")
            if args.ocr_method == "tesseract":
                print("Tesseract OCR ì—”ì§„ë„ ë³„ë„ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤:")
                print("Windows: https://github.com/UB-Mannheim/tesseract/wiki")
            return
        
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì‹ ë¢°ë„ í‰ê°€ ì‹œì‘")
        print(f"- ì´ë¯¸ì§€: {args.image}")
        print(f"- OCR ë°©ë²•: {args.ocr_method}")
        print(f"- ìœ ì‚¬ì„± ì„ê³„ê°’: {args.similarity_threshold}")
        
        result = evaluate_image(
            image_path=args.image,
            nli_batch=args.nli_batch,
            use_gpu=args.use_gpu,
            fp16=args.fp16,
            similarity_threshold=args.similarity_threshold,
            ocr_method=args.ocr_method
        )
        
        if result.get("success"):
            print("âœ… ì´ë¯¸ì§€ í‰ê°€ ì™„ë£Œ")
            print(f"ğŸ“Š ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {result.get('extracted_text_length')}ì")
            if result.get('extracted_text_preview'):
                print(f"ğŸ“ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {result['extracted_text_preview'][:100]}...")
            
            # API íŒŒì‹±ì„ ìœ„í•œ í‘œì¤€ í˜•ì‹ ì¶œë ¥
            if result.get('reliability_score') is not None:
                print(f"\nì‹ ë¢°ë„: {result['reliability_score']}% - {result.get('reliability_level', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                print(f"ê¶Œì¥ì‚¬í•­: {result.get('recommendation', 'ê¶Œì¥ì‚¬í•­ ì—†ìŒ')}")
                
                # ê·¼ê±° ìë£Œ ì¶œë ¥
                evidence = result.get('evidence', [])
                for i, ev in enumerate(evidence, start=1):  # start=1ë¡œ 1ë¶€í„° ì‹œì‘
                    reliability_percent = int(ev.get('score', 0) * 100)
                    print(f"{i}. {reliability_percent}%: {ev.get('url', '')} (ìœ ì‚¬ì„±: {ev.get('similarity', 0):.3f}, ì§€ì§€ë„: {ev.get('support', 0):.3f})")
        else:
            print(f"âŒ ì´ë¯¸ì§€ í‰ê°€ ì‹¤íŒ¨: {result.get('error')}")
            if result.get('extracted_text_preview'):
                print(f"ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {result['extracted_text_preview']}")
        
        # JSON ê²°ê³¼ë„ ì¶œë ¥ (API íŒŒì‹±ìš©)
        import json
        print(f"\nJSON_RESULT:{json.dumps(result, ensure_ascii=False)}")
        
        return result

if __name__ == "__main__":
    main()
