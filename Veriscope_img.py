# test_sec.py
# --------------------------------------------------------------------------------------------
# ì‹œë“œ í¬ë¡¤(ë³‘ë ¬) â†’ ì„ë² ë”© ì¸ë±ìŠ¤(pkl) â†’ í‰ê°€ ì‹œ NLI ì¬ë­í¬(ë°°ì¹˜)
# ì¶œë ¥: Top-5 ê·¼ê±°(ê° %) + ìµœì¢… ì‹ ë¢°ë„ %
# - ë„¤ì´ë²„/ JTBC ì „ìš© ë³¸ë¬¸ ì¶”ì¶œê¸° ì¶”ê°€
# - AMP ì„œë¸Œë„ë©”ì¸ ì˜ëª» ì‹œë„ ì œê±° (amp.news.*)
# - í›„ë³´ TopK ìƒí–¥(500) + í•œêµ­ì–´ ë¹„ìœ¨ ê°€ì¤‘ì¹˜ + ì¦ê±° ì¤‘ë³µ ì œê±°
# --------------------------------------------------------------------------------------------
# --------------------------------------------------------------------------------------------
# ğŸ§ª í…ŒìŠ¤íŠ¸ìš© ë¹ ë¥¸ ë¹Œë“œ (3ê°œ ì‹œë“œ, 1-2ë¶„ ì†Œìš”) - í•˜ë“œì›¨ì–´ ìµœëŒ€ í™œìš©:
# & C:\Smart_IT\.venv\Scripts\python.exe C:\Smart_IT\test_sec.py build-index --test-mode --use-gpu --fp16 --fast-extract
#
# ï¿½ ì „ì²´ ì¸ë±ìŠ¤ ë¹Œë“œ (238ê°œ ì‹œë“œ, 15-30ë¶„ ì†Œìš”) - ìµœê³  ì„±ëŠ¥ ëª¨ë“œ:
# & C:\Smart_IT\.venv\Scripts\python.exe C:\Smart_IT\test_sec.py build-index --use-gpu --fp16 --fast-extract
#
# ğŸ” ì‹ ë¢°ë„ í‰ê°€ ì‹¤í–‰:
# & C:\Smart_IT\.venv\Scripts\python.exe C:\Smart_IT\test_sec.py evaluate --url "ê¸°ì‚¬URL" --use-gpu --fp16
#
# ğŸ’» í•˜ë“œì›¨ì–´ ìµœì í™”: Intel Ultra9 285k (32ìŠ¤ë ˆë“œ) + RTX3070ti (8GB) + 128GB RAM ìµœëŒ€ í™œìš©
# --------------------------------------------------------------------------------------------

import os
import re
import csv
import sys
import math
import time
import pickle
import queue
import argparse
import urllib.parse as up
import logging
import multiprocessing as mp
from datetime import datetime, timezone
from dataclasses import dataclass
from typing import List, Tuple, Optional, Callable
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from threading import Lock

# í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ì„¤ì • (Windows)
if sys.platform == "win32":
    try:
        import psutil
        # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ë¥¼ ë†’ìŒìœ¼ë¡œ ì„¤ì •
        current_process = psutil.Process()
        current_process.nice(psutil.HIGH_PRIORITY_CLASS)
    except (ImportError, Exception):
        pass

# NPU/OpenVINO ì§€ì› (ì„ íƒì )
NPU_AVAILABLE = False
try:
    import openvino as ov
    NPU_AVAILABLE = True
except ImportError:
    pass

# --- third-party ---
import numpy as np
import requests
from bs4 import BeautifulSoup
import trafilatura
from newspaper import Article
from tqdm import tqdm

# BeautifulSoup ê²½ê³  ì–µì œ
import warnings
from bs4 import MarkupResemblesLocatorWarning
warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)

import torch
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModelForSequenceClassification

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from readability import Document
import json

# --------------------------------------------------------------------------------------------
# ê³ ì • ê²½ë¡œ
SEED_CSV  = r"C:\Smart_IT\enhanced_seed_links.csv"  # ê°œì„ ëœ ì‹œë“œ ë§í¬ ì‚¬ìš©
INDEX_PKL = r"C:\Smart_IT\smart_it_index.pkl"

# ê¸°ë³¸ ì •ì±…
MAX_DEPTH = 2
MAX_PAGES_PER_DOMAIN = 150      # ë„ë©”ì¸ë‹¹ ìµœëŒ€ í˜ì´ì§€(í’ˆì§ˆ/ì‹œê°„ íŠ¸ë ˆì´ë“œì˜¤í”„)
REQUEST_TIMEOUT = 12
CRAWL_SLEEP = 0.5

# ê²€ìƒ‰/ìŠ¤ì½”ì–´ ì •ì±…
TOPK_CANDIDATES = 500           # â† ì¤‘ìš”: 0 ì´ë©´ NLIê°€ ë¹„ì–´ë²„ë¦¼
TOPN_RETURN = 5
MIN_TEXT_LEN = 200              # ìš´ì˜ìš© ê¶Œì¥ê°’(ë””ë²„ê¹… ì‹œ ë‚®ì¶°ë„ ë¨)
MIN_SIMILARITY_THRESHOLD = 0.35  # ìµœì†Œ ìœ ì‚¬ì„± ì„ê³„ê°’ (í’ˆì§ˆ ê°œì„ : 0.15 â†’ 0.35)
MIN_NLI_SUPPORT_THRESHOLD = 0.1  # ìµœì†Œ NLI ì§€ì§€ë„ ì„ê³„ê°’
MIN_FINAL_SCORE = 0.3           # ìµœì¢… ì ìˆ˜ ìµœì†Œ ì„ê³„ê°’

# ìŠ¤ì½”ì–´ ê°€ì¤‘ì¹˜ (ì¡°ì •ë¨)
ALPHA_SIM = 0.65      # ìœ ì‚¬ì„± ê°€ì¤‘ì¹˜ (ë†’ì„)
BETA_SUP  = 0.35      # NLI ì§€ì§€ë„ ê°€ì¤‘ì¹˜ (ë†’ì„)  
GAMMA_CONTRA = 0.50   # NLI ë°˜ë°• ê°€ì¤‘ì¹˜ (ë†’ì„)
DELTA_TIME = 0.20     # ì‹œê°„ ê°€ì¤‘ì¹˜ (0.10 â†’ 0.20ìœ¼ë¡œ ê°•í™”)
EPS_SOURCE = 0.20     # ì¶œì²˜ ì‹ ë¢°ì„± ê°€ì¤‘ì¹˜ (ë†’ì„)
EPS_LANG   = 0.15     # í•œêµ­ì–´/ì˜ì–´ ë“± ì§ˆì˜-ë¬¸ì„œ ì–¸ì–´ ì •í•© ê°€ì¤‘ (ë†’ì„)
TIME_LAMBDA = 0.0025

# ë¡œê¹…
logger = logging.getLogger("smart_it")

def setup_logging(verbose: bool, quiet: bool, log_file: Optional[str], build_mode: bool = False):
    if build_mode and not verbose:
        # ë¹Œë“œ ëª¨ë“œì—ì„œëŠ” ì§„í–‰ë„ ê°€ì‹œì„±ì„ ìœ„í•´ ë¡œê·¸ ìµœì†Œí™”
        level = logging.ERROR
    else:
        level = logging.INFO
        if verbose:
            level = logging.DEBUG
        if quiet:
            level = logging.WARNING
    
    handlers = []
    if log_file:
        # íŒŒì¼ë¡œë§Œ ë¡œê·¸ ì¶œë ¥ (ì§„í–‰ë„ í‘œì‹œ ë°©í•´ ë°©ì§€)
        handlers.append(logging.FileHandler(log_file, encoding="utf-8"))
    elif not build_mode or verbose:
        # ë¹Œë“œ ëª¨ë“œê°€ ì•„ë‹ˆê±°ë‚˜ verbose ëª¨ë“œì¼ ë•Œë§Œ ì½˜ì†” ì¶œë ¥
        handlers.append(logging.StreamHandler(sys.stderr))  # stderrë¡œ ë³€ê²½í•˜ì—¬ ì§„í–‰ë„ì™€ ë¶„ë¦¬
    
    if handlers:
        logging.basicConfig(level=level, format="%(asctime)s [%(levelname)s] %(message)s", handlers=handlers)
    else:
        # ë¡œê·¸ í•¸ë“¤ëŸ¬ê°€ ì—†ìœ¼ë©´ NullHandler ì‚¬ìš©
        logging.basicConfig(level=logging.CRITICAL, handlers=[logging.NullHandler()])
    
    if verbose or not build_mode:
        logger.debug("ë¡œê¹… ì´ˆê¸°í™”(level=%s, log_file=%s)", logging.getLevelName(level), log_file)

# --------------------------------------------------------------------------------------------
# HTTP ì„¸ì…˜(ì»¤ë„¥ì…˜ í’€/ì¬ì‹œë„)
SESSION = None
def configure_http(http_pool: int, timeout: int):
    global SESSION, REQUEST_TIMEOUT
    REQUEST_TIMEOUT = timeout
    sess = requests.Session()
    adapter = HTTPAdapter(
        pool_connections=http_pool,
        pool_maxsize=http_pool,
        max_retries=Retry(total=2, backoff_factor=0.2, status_forcelist=[429, 500, 502, 503, 504]),
    )
    sess.mount("http://", adapter)
    sess.mount("https://", adapter)
    SESSION = sess
    logger.info("HTTP ì„¤ì •: pool=%d, timeout=%ds", http_pool, timeout)

def polite_get(url: str, mobile: bool = False) -> Optional[str]:
    try:
        # ë¡œì»¬ íŒŒì¼ ì§€ì›
        if url.startswith('file://'):
            file_path = url.replace('file://', '').replace('/', '\\')
            if file_path.startswith('\\C:'):
                file_path = 'C:' + file_path[3:]
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        
        ua_desktop = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36"
        ua_mobile  = "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Mobile Safari/537.36"
        h = {
            "User-Agent": (ua_mobile if mobile else ua_desktop),
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Referer": url,
        }
        r = SESSION.get(url, headers=h, timeout=REQUEST_TIMEOUT)
        if r.status_code == 200 and "text/html" in r.headers.get("Content-Type", ""):
            return r.text
        logger.debug("GET %s -> %s (%s)", url, r.status_code, r.headers.get("Content-Type"))
    except Exception as e:
        logger.debug("GET ì‹¤íŒ¨ %s (%s)", url, e)
    return None

# --------------------------------------------------------------------------------------------
# ìœ í‹¸/ì „ì²˜ë¦¬
def now_utc() -> datetime:
    return datetime.now(tz=timezone.utc)

def normalize_space(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def canonical_url(u: str) -> str:
    try:
        p = up.urlparse(u)
        q = up.parse_qs(p.query)
        # ì¶”ì  íŒŒë¼ë¯¸í„° ë° ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œê±°
        filtered = {k: v for k, v in q.items() if not k.startswith((
            "utm_", "fbclid", "gclid", "igshid", "ref", "share", "_ga", "campaign", 
            "source", "medium", "term", "content", "spm_id", "module", "pgtype"
        ))}
        new_q = up.urlencode([(k, vv) for k, vals in filtered.items() for vv in vals])
        p2 = p._replace(query=new_q)
        netloc = p2.netloc.lower()
        path = re.sub(r"/+", "/", p2.path)
        if path != "/" and path.endswith("/"):
            path = path[:-1]
        p3 = p2._replace(netloc=netloc, path=path)
        return up.urlunparse(p3)
    except Exception:
        return u

def url_similarity(url1: str, url2: str) -> float:
    """ë‘ URL ê°„ì˜ ìœ ì‚¬ì„±ì„ ê³„ì‚° (0~1)"""
    try:
        p1, p2 = up.urlparse(url1), up.urlparse(url2)
        
        # ë„ë©”ì¸ì´ ë‹¤ë¥´ë©´ 0
        if p1.netloc.lower() != p2.netloc.lower():
            return 0.0
        
        # ê²½ë¡œê°€ ì™„ì „íˆ ê°™ìœ¼ë©´ 1
        if p1.path == p2.path:
            return 1.0
        
        # ê²½ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ë¹„êµ
        path1_parts = [part for part in p1.path.split('/') if part]
        path2_parts = [part for part in p2.path.split('/') if part]
        
        if not path1_parts or not path2_parts:
            return 0.0
        
        # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸(ì£¼ë¡œ ê¸°ì‚¬ ID)ë§Œ ë‹¤ë¥¸ ê²½ìš° ë†’ì€ ìœ ì‚¬ì„±
        if len(path1_parts) == len(path2_parts):
            common_parts = sum(1 for a, b in zip(path1_parts[:-1], path2_parts[:-1]) if a == b)
            similarity = common_parts / max(1, len(path1_parts) - 1)
            if similarity >= 0.8:  # ê²½ë¡œì˜ 80% ì´ìƒì´ ê°™ìœ¼ë©´
                return 0.9
        
        return 0.0
    except Exception:
        return 0.0

def domain_of(u: str) -> str:
    try:
        return up.urlparse(u).netloc.lower()
    except Exception:
        return ""

def is_same_domain(u: str, seed_domain: str) -> bool:
    d = domain_of(u)
    return d == seed_domain or d.endswith("." + seed_domain)

def extract_links(base_url: str, html: str) -> List[str]:
    out = []
    try:
        soup = BeautifulSoup(html, "html.parser")
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            abs_u = up.urljoin(base_url, href)
            if abs_u.startswith(("http://", "https://")):
                out.append(canonical_url(abs_u))
    except Exception:
        pass
    return list(set(out))

def korean_ratio(text: str) -> float:
    if not text:
        return 0.0
    total = len(text)
    hangul = sum(1 for ch in text if '\uac00' <= ch <= '\ud7a3')
    return hangul / max(1, total)

# --------------------------------------------------------------------------------------------
# ë³¸ë¬¸ ì¶”ì¶œ (ë„ë©”ì¸ ì „ìš© â†’ AMP/JSON-LD/Next.js/Readability â†’ trafilatura â†’ manual â†’ newspaper3k)
def extract_text(url: str, html: Optional[str], fast: bool = False) -> Tuple[str, Optional[datetime], str]:
    text, dt, title = "", None, ""

    # 0) html ì—†ìœ¼ë©´ ë°ìŠ¤í¬í†±â†’ëª¨ë°”ì¼ ìˆœìœ¼ë¡œ ì‹œë„ (ë„¤ì´ë²„ëŠ” ì•„ë˜ ì „ìš©ê¸°ë¡œ ë³´ì •)
    if not html:
        html = polite_get(url) or polite_get(url, mobile=True)

    def _clean_html_text(h: str) -> str:
        soup = BeautifulSoup(h, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        return normalize_space(soup.get_text(" ", strip=True))

    # ----- ì „ìš© ì¶”ì¶œê¸°: ë„¤ì´ë²„ -----
    def _extract_naver_article(h: str) -> Tuple[str, Optional[str], Optional[str]]:
        if not h:
            return "", None, None
        soup = BeautifulSoup(h, "html.parser")
        node = soup.select_one("#dic_area") or soup.select_one(".newsct_article")
        body = ""
        if node:
            for bad in node.select("figure, .promotion, .byline, .copyright, .end_photo_org, .img_desc"):
                bad.decompose()
            body = normalize_space(node.get_text(" ", strip=True))
        hed = None
        tnode = soup.select_one("h2#title_area .media_end_head_headline") or soup.select_one("h2.media_end_head_headline")
        if tnode:
            hed = normalize_space(tnode.get_text(" ", strip=True))
        if not hed and soup.title and soup.title.string:
            hed = normalize_space(soup.title.string)
        date_str = None
        meta = soup.find("meta", attrs={"property": "article:published_time"})
        if meta and meta.get("content"):
            date_str = meta["content"]
        else:
            dnode = soup.select_one("span.media_end_head_info_datestamp_time")
            if dnode and dnode.get("data-date-time"):
                date_str = dnode["data-date-time"]
        return body, date_str, hed

    # ----- ì „ìš© ì¶”ì¶œê¸°: JTBC(Next.js JSON + CSS ì„ íƒì) -----
    def _extract_jtbc_nextdata(h: str) -> Tuple[str, Optional[str], Optional[str]]:
        if not h:
            return "", None, None
        soup = BeautifulSoup(h, "html.parser")
        
        # 0) Next.js self.__next_f.push ë°©ì‹ ì²˜ë¦¬ (ìµœìš°ì„ )
        scripts = soup.find_all("script")
        for script in scripts:
            if script.string and "self.__next_f.push" in script.string:
                try:
                    content = script.string
                    
                    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ íŒ¨í„´ìœ¼ë¡œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
                    korean_pattern = r'"([^"]*[ê°€-í£]+[^"]*)"'
                    matches = re.findall(korean_pattern, content)
                    
                    # ê°€ì¥ ê¸´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ê¸°ì‚¬ ë³¸ë¬¸ìœ¼ë¡œ ì‚¬ìš©
                    longest_text = ""
                    for match in matches:
                        if len(match) > len(longest_text) and len(match) > 100:
                            # ê¸°ì‚¬ ë‚´ìš© ê°™ì€ íŒ¨í„´ì¸ì§€ í™•ì¸
                            if any(keyword in match for keyword in ['ë§í–ˆë‹¤', 'ë°í˜”ë‹¤', 'ì „í–ˆë‹¤', 'ë°œí‘œí–ˆë‹¤', 'ì„¤ëª…í–ˆë‹¤']):
                                longest_text = match
                            elif len(match) > 200:  # ì¶©ë¶„íˆ ê¸´ í…ìŠ¤íŠ¸ë©´ ê¸°ì‚¬ ë‚´ìš©ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
                                longest_text = match
                    
                    if longest_text:
                        # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì²˜ë¦¬
                        cleaned = longest_text.replace('\\n', '\n').replace('\\t', ' ')
                        cleaned = cleaned.replace('\\"', '"').replace('\\\\', '\\')
                        
                        # HTML íƒœê·¸ ì œê±°
                        cleaned = re.sub(r'<[^>]+>', '', cleaned)
                        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
                        
                        if len(cleaned) > 100:
                            logger.debug(f"JTBC Next.js í•œêµ­ì–´ íŒ¨í„´ ì¶”ì¶œ ì„±ê³µ ({len(cleaned)}ì)")
                            
                            # ì œëª© ì¶”ì¶œ
                            title = ""
                            if soup.title:
                                title = soup.title.get_text(strip=True)
                            
                            return normalize_space(cleaned), None, normalize_space(title) if title else None
                                
                except Exception as e:
                    logger.debug(f"JTBC Next.js íŒ¨í„´ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
        
        # 0-1) Meta description fallback
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if not meta_desc:
            meta_desc = soup.find("meta", attrs={"property": "og:description"})
        
        if meta_desc and meta_desc.get('content'):
            desc = meta_desc.get('content')
            if len(desc) > 80:  # Meta descriptionì´ ì¶©ë¶„íˆ ê¸´ ê²½ìš°
                logger.debug(f"JTBC Meta description ì¶”ì¶œ ì„±ê³µ ({len(desc)}ì)")
                
                title = ""
                if soup.title:
                    title = soup.title.get_text(strip=True)
                
                return normalize_space(desc), None, normalize_space(title) if title else None
        
        # 1) CSS ì„ íƒì ê¸°ë°˜ ë³¸ë¬¸ ì¶”ì¶œ (JTBC ì „ìš© ì„ íƒì ì¶”ê°€)
        article_selectors = [
            "div[data-module='ArticleContent']",
            "article .newsroom_article_content",
            ".newsroom_article_content", 
            "article .article_content",
            ".article_content",
            "[data-testid='article-content']",
            ".news_article_body",
            ".article_body_content",
            ".MuiBox-root p",  # Material-UI êµ¬ì¡°
            "main p",
            "[class*='ArticleContent']",
            "[class*='article-body']",
            "[class*='news-body']"
        ]
        
        for selector in article_selectors:
            elements = soup.select(selector)
            if elements:
                # ì—¬ëŸ¬ ìš”ì†Œì¸ ê²½ìš° í•©ì¹˜ê¸°
                content_parts = []
                for elem in elements:
                    # ê´‘ê³ , ê´€ë ¨ê¸°ì‚¬ ë“± ì œê±°
                    for unwanted in elem.select(".ad, .advertisement, .related, .taboola, script, style, .share, .sns"):
                        unwanted.decompose()
                    
                    text = elem.get_text(" ", strip=True)
                    if len(text) > 50:
                        content_parts.append(text)
                
                if content_parts:
                    body_text = " ".join(content_parts)
                    body_text = normalize_space(body_text)
                    
                    if len(body_text) >= 100:
                        logger.debug(f"JTBC CSS ì„ íƒì ì¶”ì¶œ ì„±ê³µ (ì„ íƒì: {selector}, {len(body_text)}ì)")
                        
                        # ì œëª© ì¶”ì¶œ
                        title_selectors = ["h1", ".headline", ".article_title", "title"]
                        title = ""
                        for title_sel in title_selectors:
                            title_node = soup.select_one(title_sel)
                            if title_node:
                                title = normalize_space(title_node.get_text(strip=True))
                                break
                        
                        # ë‚ ì§œ ì¶”ì¶œ
                        date_str = None
                        date_meta = soup.find("meta", attrs={"property": "article:published_time"})
                        if date_meta and date_meta.get("content"):
                            date_str = date_meta["content"]
                        else:
                            time_node = soup.select_one("time[datetime]")
                            if time_node and time_node.get("datetime"):
                                date_str = time_node["datetime"]
                        
                        return body_text, date_str, title
        
        # 2) Next.js __NEXT_DATA__ ì²˜ë¦¬
        sc = soup.find("script", id="__NEXT_DATA__", type="application/json")
        if sc and sc.string:
            try:
                data = json.loads(sc.string)
                texts, pub, hed = [], None, None
                def _walk(x):
                    nonlocal pub, hed
                    if isinstance(x, dict):
                        for k in ("headline","title","name"):
                            v = x.get(k)
                            if isinstance(v, str) and not hed:
                                hed = v
                        for k in ("datePublished","dateModified","publishDate","publishedAt"):
                            v = x.get(k)
                            if isinstance(v, str) and not pub:
                                pub = v
                        for k, v in x.items():
                            if isinstance(v, str) and k.lower() in {"articlebody","body","content","text","value","rawhtml","html"}:
                                if len(v) > 20:
                                    texts.append(v)
                            elif isinstance(v, (list, dict)):
                                _walk(v)
                    elif isinstance(x, list):
                        for v in x:
                            _walk(v)
                _walk(data)
                if texts:
                    raw = "\n".join(texts)
                    body = _clean_html_text(raw)
                    if len(body) >= 50:
                        return normalize_space(body), pub, (normalize_space(hed) if hed else None)
            except Exception:
                pass
        
        # 3) ì •ê·œì‹ fallback
        patterns = [
            r'"articleBody"\s*:\s*"(.+?)"',
            r'"content"\s*:\s*"(.+?)"',
            r'"text"\s*:\s*"(.+?)"',
            r'"body"\s*:\s*"(.+?)"'
        ]
        
        for pattern in patterns:
            m = re.search(pattern, h, re.DOTALL)
            if m:
                try:
                    raw = m.group(1).encode('utf-8', 'backslashreplace').decode('unicode_escape')
                    body = _clean_html_text(raw)
                    if len(body) >= 50:
                        title = ""
                        if soup.title and soup.title.string:
                            title = normalize_space(soup.title.string)
                        return normalize_space(body), None, title
                except Exception:
                    continue
        
        # 4) ì¼ë°˜ì ì¸ ê¸°ì‚¬ êµ¬ì¡° ì‹œë„
        article_node = soup.find("article") or soup.find("main")
        if article_node:
            paragraphs = article_node.find_all("p")
            if len(paragraphs) >= 3:
                body_parts = []
                for p in paragraphs:
                    text = normalize_space(p.get_text(strip=True))
                    if len(text) > 20 and not any(skip in text.lower() for skip in ["ê´‘ê³ ", "advertisement", "ê´€ë ¨ê¸°ì‚¬", "ì¶”ì²œ"]):
                        body_parts.append(text)
                
                if body_parts:
                    body = " ".join(body_parts)
                    if len(body) >= 100:
                        title = ""
                        if soup.title and soup.title.string:
                            title = normalize_space(soup.title.string)
                        return normalize_space(body), None, title
        
        return "", None, None

    # 1) AMP link
    amp_html = None
    if html:
        try:
            soup = BeautifulSoup(html, "html.parser")
            amp_link = soup.find("link", rel=lambda v: v and "amphtml" in v.lower())
            if amp_link and amp_link.get("href"):
                amp_url = up.urljoin(url, amp_link["href"])
                # ì˜ëª»ëœ amp ì„œë¸Œë„ë©”ì¸ ì‹œë„ ê¸ˆì§€
                if "amp." not in up.urlparse(amp_url).netloc:
                    amp_html = polite_get(amp_url) or polite_get(amp_url, mobile=True)
        except Exception:
            pass

    # 2) JSON-LD
    def extract_from_jsonld(h: str) -> Tuple[str, Optional[str], Optional[str]]:
        try:
            soup = BeautifulSoup(h, "html.parser")
            for script in soup.find_all("script", type="application/ld+json"):
                try:
                    data = json.loads(script.string or "")
                except Exception:
                    continue
                candidates = data if isinstance(data, list) else [data]
                for item in candidates:
                    if not isinstance(item, dict):
                        continue
                    typ = item.get("@type") or item.get("@graph", [{}])[0].get("@type")
                    if (typ and ("Article" in str(typ) or "NewsArticle" in str(typ))) or item.get("articleBody"):
                        body = item.get("articleBody")
                        if isinstance(body, list):
                            body = "\n".join([str(x) for x in body])
                        hed = (item.get("headline") or item.get("name") or "")[:300]
                        date_str = item.get("datePublished") or item.get("dateModified")
                        return normalize_space(body or ""), date_str, hed
        except Exception:
            pass
        return "", None, None

    # 3) Next.js-ish generic
    def extract_from_next_json(h: str) -> Tuple[str, Optional[str], Optional[str]]:
        try:
            soup = BeautifulSoup(h, "html.parser")
            scripts_raw = []
            for sc in soup.find_all("script"):
                raw = sc.string or sc.get_text() or ""
                raw = raw.strip()
                if not raw:
                    continue
                if any(k in raw for k in ("__NEXT_DATA__", "articleBody", "\"content\"", "\"text\"", "\"value\"", "\"datePublished\"")) \
                or (raw.startswith("{") and raw.endswith("}")) \
                or (raw.startswith("[") and raw.endswith("]")):
                    scripts_raw.append(raw)

            def _walk(x, texts: list, meta: dict):
                if isinstance(x, dict):
                    for k in ("headline","title","name"):
                        v = x.get(k)
                        if isinstance(v, str) and not meta.get("title"):
                            meta["title"] = v
                    for k in ("datePublished","dateModified","publishDate","publishedAt"):
                        v = x.get(k)
                        if isinstance(v, str) and not meta.get("date"):
                            meta["date"] = v
                    for k, v in x.items():
                        if isinstance(v, str):
                            kl = k.lower()
                            if kl in {"articlebody","body","content","rawhtml","html","text","value"} and v.strip():
                                texts.append(v)
                        elif isinstance(v, (list, dict)):
                            _walk(v, texts, meta)
                elif isinstance(x, list):
                    for el in x:
                        _walk(el, texts, meta)

            for raw in scripts_raw:
                parsed = None
                try:
                    parsed = json.loads(raw)
                except Exception:
                    found = re.findall(r'"(?:text|value|content)"\s*:\s*"([^"]+)"', raw)
                    if found:
                        body_raw = "\n".join([fx for fx in found if fx.strip()])
                        body_txt = BeautifulSoup(body_raw, "html.parser").get_text(" ", strip=True)
                        body_txt = normalize_space(body_txt)
                        if len(body_txt) >= 50:
                            return body_txt, None, None
                if parsed is not None:
                    texts, meta = [], {"title": None, "date": None}
                    _walk(parsed, texts, meta)
                    if texts:
                        body_raw = "\n".join([t for t in texts if t.strip()])
                        body_txt = BeautifulSoup(body_raw, "html.parser").get_text(" ", strip=True)
                        body_txt = normalize_space(body_txt)
                        if len(body_txt) >= 50:
                            return body_txt, meta.get("date"), meta.get("title")
        except Exception:
            pass
        return "", None, None

    # 4) Readability
    def extract_with_readability(h: str) -> Tuple[str, str]:
        try:
            doc = Document(h)
            hed = normalize_space(doc.short_title())
            summ = doc.summary(html_partial=False)
            soup = BeautifulSoup(summ, "html.parser")
            parts = [normalize_space(p.get_text(" ", strip=True)) for p in soup.find_all("p")]
            body = normalize_space("\n".join([p for p in parts if p]))
            return body, hed
        except Exception:
            return "", ""

    # 5) trafilatura
    def extract_with_trafilatura(h: str) -> str:
        try:
            return normalize_space(trafilatura.extract(
                h, include_comments=False, include_tables=False,
                favor_precision=(False if fast else True)
            ) or "")
        except Exception:
            return ""

    # 6) newspaper3k
    def extract_with_newspaper(u: str) -> Tuple[str, Optional[datetime], str]:
        try:
            art = Article(u, keep_article_html=False, language="ko")
            art.download()
            art.parse()
            t = normalize_space(art.text)
            hed = normalize_space(art.title)
            d = art.publish_date
            if d and d.tzinfo is None:
                d = d.replace(tzinfo=timezone.utc)
            return t, d, hed
        except Exception:
            return "", None, ""

    # 7) ìˆ˜ë™ CSS í›„ë³´
    def extract_manual(h: str) -> str:
        try:
            soup = BeautifulSoup(h, "html.parser")
            candidates = [
                {"id": "article", "cls": None},
                {"id": "article_body", "cls": None},
                {"id": "articleContent", "cls": None},
                {"id": None, "cls": "article_body"},
                {"id": None, "cls": "article-content"},
                {"id": None, "cls": "news_article"},
                {"id": None, "cls": "content_article"},
            ]
            chunks = []
            for c in candidates:
                node = soup.find(id=c["id"]) if c["id"] else soup.find(class_=lambda v: v and c["cls"] in v)
                if node:
                    ps = node.find_all(["p", "div"])
                    for p in ps:
                        txt = normalize_space(p.get_text(" ", strip=True))
                        if txt:
                            chunks.append(txt)
            if chunks:
                return normalize_space("\n".join(chunks))
        except Exception:
            pass
        return ""

    # ---- ì‹¤ì œ ì¶”ì¶œ ìˆœì„œ ----
    host = domain_of(url)

    # (Z) ë„ë©”ì¸ ì „ìš© ë¹ ë¥¸ ê²½ë¡œ: NAVER / JTBC
    if html:
        if host.endswith("n.news.naver.com") or host.endswith("news.naver.com"):
            b, dstr, hed = _extract_naver_article(html)
            if len(b) >= 80:
                text = b; title = hed or title
                if dstr:
                    try: dt = datetime.fromisoformat(dstr.replace("Z","+00:00"))
                    except Exception: pass
        elif host.endswith("news.jtbc.co.kr"):
            b, dstr, hed = _extract_jtbc_nextdata(html)
            if len(b) >= 80:
                text = b; title = hed or title
                if dstr:
                    try: dt = datetime.fromisoformat(dstr.replace("Z","+00:00"))
                    except Exception: pass

    # (A) AMP ìš°ì„ 
    if amp_html and not text:
        body, date_str, hed = extract_from_jsonld(amp_html)
        if not body:
            body, hed2 = extract_with_readability(amp_html); hed = hed or hed2
        if not body:
            body = extract_with_trafilatura(amp_html)
        if not body:
            body = extract_manual(amp_html)
        if body:
            text = body; title = hed or title
            if date_str:
                try: dt = datetime.fromisoformat(date_str.replace("Z","+00:00"))
                except Exception: pass

    # (B) ì›ë³¸ HTML: JSON-LD â†’ Next.js JSON â†’ Readability â†’ trafilatura â†’ manual
    if html and not text:
        body, date_str, hed = extract_from_jsonld(html)
        if not body:
            body, date_str2, hed2 = extract_from_next_json(html)
            hed = hed or hed2
            date_str = date_str or date_str2
        if not body:
            body, hed2 = extract_with_readability(html); hed = hed or hed2
        if not body:
            body = extract_with_trafilatura(html)
        if not body:
            body = extract_manual(html)
        if body:
            text = body; title = hed or title
            if date_str and not dt:
                try: dt = datetime.fromisoformat(date_str.replace("Z","+00:00"))
                except Exception: pass

    # (C) newspaper3k ìµœí›„
    if not text:
        t2, d2, hed = extract_with_newspaper(url)
        if t2:
            text, dt, title = t2, (dt or d2), (title or hed)

    # (D) ë©”íƒ€ íƒ€ì´í‹€ ë³´ì •
    if not title and html:
        try:
            soup = BeautifulSoup(html, "html.parser")
            if soup.title and soup.title.string:
                title = normalize_space(soup.title.string)
        except Exception:
            pass

    return text, dt, title

# --------------------------------------------------------------------------------------------
# ë„ë©”ì¸ í‰íŒ íœ´ë¦¬ìŠ¤í‹±
GOOD_TLD_HINTS = (".go.kr", ".ac.kr", ".lg.jp", ".gov", ".edu")
OK_TLD_HINTS   = (".or.kr", ".or.jp", ".org", ".co.kr", ".co.jp", ".com", ".net")
LOW_TLD_HINTS  = (".info", ".biz")

def source_reputation(url: str, in_seed: bool) -> float:
    d = domain_of(url)
    score = 0.0
    if in_seed: score += 0.4
    if d.endswith(GOOD_TLD_HINTS): score += 0.4
    elif d.endswith(OK_TLD_HINTS): score += 0.2
    elif d.endswith(LOW_TLD_HINTS): score -= 0.1
    if url.lower().startswith("https://"): score += 0.05
    return max(-0.2, min(0.8, score))

def time_weight(dt_pub: Optional[datetime]) -> float:
    if not dt_pub: return 0.0
    age_days = max(0.0, (now_utc() - dt_pub).total_seconds()/86400.0)
    
    # ì—°ë„ë³„ ëŒ€í­ ê°•í™”ëœ í˜ë„í‹° (ì˜¤ë³´ ê¸°ì‚¬ ëŒ€ì‘)
    if age_days > 365 * 13:  # 13ë…„ ì´ìƒ (2011ë…„ ì´ì „) - JTBC ì˜¤ë³´ ê¸°ì‚¬ ëŒ€ì‘
        return -1.2  # ë§¤ìš° ê°•í•œ í˜ë„í‹° (ê°•í™”)
    elif age_days > 365 * 10:  # 10ë…„ ì´ìƒ (2014ë…„ ì´ì „)
        return -1.0  # ê°•í•œ í˜ë„í‹° (ê°•í™”)
    elif age_days > 365 * 7:  # 7ë…„ ì´ìƒ (2017ë…„ ì´ì „)
        return -0.8  # ê°•í•œ í˜ë„í‹°
    elif age_days > 365 * 5:  # 5ë…„ ì´ìƒ (2019ë…„ ì´ì „)
        return -0.6  # ì¤‘ê°„ í˜ë„í‹°
    elif age_days > 365 * 3:  # 3ë…„ ì´ìƒ (2021ë…„ ì´ì „)
        return -0.4  # ì•½ê°„ í˜ë„í‹°
    elif age_days > 365 * 1:  # 1ë…„ ì´ìƒ
        return -0.2  # ìµœì†Œ í˜ë„í‹°
    else:
        w = math.exp(-TIME_LAMBDA * age_days)  # 1ë…„ ì´ë‚´ëŠ” ê¸°ì¡´ ê³µì‹
        return -0.1 + 0.9 * w  # -0.1 ~ +0.8

# --------------------------------------------------------------------------------------------
# ë°ì´í„° êµ¬ì¡°
@dataclass
class DocRecord:
    url: str
    title: str
    published: Optional[float]
    chunk: str
    domain: str
    from_seed: bool

@dataclass
class IndexPack:
    model_name: str
    embed_dim: int
    matrix: np.ndarray
    records: List[DocRecord]

# --------------------------------------------------------------------------------------------
# ë¬¸ì¥ ë¶„í• /ì²­í‚¹
def split_into_sentences(text: str) -> List[str]:
    parts = re.split(r"(?<=[.!?â€¦]|[ã€‚ï¼ï¼Ÿ])\s+", text)
    parts = [normalize_space(p) for p in parts if len(normalize_space(p)) > 0]
    return parts

def make_chunks(text: str, window: int = 4, step: int = 3, min_len: int = 200) -> List[str]:
    sents = split_into_sentences(text)
    chunks = []
    i = 0
    while i < len(sents):
        block = normalize_space(" ".join(sents[i:i+window]))
        if len(block) >= min_len:
            chunks.append(block)
        i += step
    if not chunks and len(text) >= min_len:
        chunks = [text]
    return chunks

# --------------------------------------------------------------------------------------------
# ëª¨ë¸ ë¡œë”©(GPU/FP16)
DEVICE = "cuda" if (hasattr(torch, "cuda") and torch.cuda.is_available()) else "cpu"

def get_embedder(use_gpu: bool, fp16: bool):
    model = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    device = "cuda" if (use_gpu and DEVICE == "cuda") else "cpu"
    
    # CUDA ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    if device == "cuda":
        torch.backends.cudnn.benchmark = True  # ë°˜ë³µì ì¸ ì—°ì‚° ìµœì í™”
        torch.backends.cudnn.deterministic = False  # ì„±ëŠ¥ ìš°ì„ 
        torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
        # GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ìµœì í™”
        torch.cuda.set_per_process_memory_fraction(0.9)  # 90% VRAM ì‚¬ìš© í—ˆìš©
    
    emb = SentenceTransformer(model, device=device)
    logger.info("ì„ë² ë”© ëª¨ë¸: %s (device=%s, fp16=%s)", model, emb._target_device, fp16)
    return emb, fp16

def get_nli(use_gpu: bool, fp16: bool):
    name = "cross-encoder/nli-deberta-v3-small"
    tok = AutoTokenizer.from_pretrained(name)
    mdl = AutoModelForSequenceClassification.from_pretrained(
        name, torch_dtype=(torch.float16 if (fp16 and DEVICE == "cuda") else None)
    )
    mdl.eval()
    mdl.to("cuda" if (use_gpu and DEVICE == "cuda") else "cpu")
    logger.info("NLI ëª¨ë¸: %s (device=%s, dtype=%s)", name, next(mdl.parameters()).device, next(mdl.parameters()).dtype)
    return tok, mdl, fp16

@torch.no_grad()
def nli_batch_probs(pairs: List[Tuple[str, str]], tok, mdl, batch_size: int, use_fp16: bool) -> np.ndarray:
    outs = []
    device = next(mdl.parameters()).device
    for i in range(0, len(pairs), batch_size):
        prem = [p for p, _ in pairs[i:i+batch_size]]
        hypo = [h for _, h in pairs[i:i+batch_size]]
        inputs = tok(prem, hypo, truncation=True, max_length=256, padding=True, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}
        if use_fp16 and device.type == "cuda":
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                logits = mdl(**inputs).logits
        else:
            logits = mdl(**inputs).logits
        probs = torch.softmax(logits, dim=-1).cpu().numpy()
        outs.append(probs)
    return np.vstack(outs) if outs else np.zeros((0, 3), dtype=np.float32)

# --------------------------------------------------------------------------------------------
# í¬ë¡¤ë§(ë„ë©”ì¸ ë‹¨ìœ„) + Overall ì§„í–‰ë°”
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

def crawl_domain(seed_url: str, max_depth: int, max_pages: int,
                 overall_update: Optional[Callable[[int], None]] = None) -> List[Tuple[str, str]]:
    visited = set()
    out = []
    seed_dom = domain_of(seed_url)
    q = queue.Queue()
    q.put((seed_url, 0))
    
    # ê°„ë‹¨í•œ ì§„í–‰ë¥  í‘œì‹œ (ë°±ê·¸ë¼ìš´ë“œ, ë¹„í™œì„±í™”)
    pbar = tqdm(total=max_pages, desc=f"Crawl {seed_dom}", 
                leave=False, position=None, disable=True)  # ì™„ì „íˆ ë¹„í™œì„±í™”
    
    while not q.empty() and len(visited) < max_pages:
        url, depth = q.get()
        if url in visited:
            continue
        visited.add(url)
        html = polite_get(url) or polite_get(url, mobile=True)
        if html:
            out.append((url, html))
            if depth < max_depth:
                for nxt in extract_links(url, html):
                    if is_same_domain(nxt, seed_dom):
                        q.put((nxt, depth + 1))
        pbar.update(1)
        if overall_update:
            overall_update(1)
        time.sleep(CRAWL_SLEEP)
    pbar.close()
    logger.info("ë„ë©”ì¸ í¬ë¡¤ ì™„ë£Œ: %s (ìˆ˜ì§‘ %d / ë°©ë¬¸ %d)", seed_dom, len(out), len(visited))
    return out

# --------------------------------------------------------------------------------------------
# ì‹œë“œ ì²˜ë¦¬ - í¬ë¡¤ë§ë§Œ (ì„ë² ë”©ì€ ë³„ë„ ì²˜ë¦¬)
def process_seed_crawl_only(seed: str, fast_extract: bool,
                           overall_update: Optional[Callable[[int], None]] = None) -> List[Tuple[str, str, str, str]]:
    """í¬ë¡¤ë§ë§Œ ìˆ˜í–‰í•˜ê³  í…ìŠ¤íŠ¸ ì²­í¬ ë°˜í™˜"""
    dom = domain_of(seed)
    pages = crawl_domain(seed, MAX_DEPTH, MAX_PAGES_PER_DOMAIN, overall_update=overall_update)
    logger.info("ë„ë©”ì¸ ìˆ˜ì§‘ ì™„ë£Œ: %s (%d pages)", dom, len(pages))

    text_chunks = []
    for url, html in pages:
        text, dt, title = extract_text(url, html, fast=fast_extract)
        if len(text) >= MIN_TEXT_LEN:
            chunks = make_chunks(text, min_len=MIN_TEXT_LEN)
            for ch in chunks:
                text_chunks.append((url, dt, title, ch))
    return text_chunks

# ê¸°ì¡´ í•¨ìˆ˜ë„ ìœ ì§€ (í˜¸í™˜ì„±)
def process_seed(seed: str, embedder, embed_batch: int, fast_extract: bool,
                 overall_update: Optional[Callable[[int], None]] = None) -> Tuple[List[np.ndarray], List[DocRecord]]:
    dom = domain_of(seed)
    pages = crawl_domain(seed, MAX_DEPTH, MAX_PAGES_PER_DOMAIN, overall_update=overall_update)
    logger.info("ë„ë©”ì¸ ìˆ˜ì§‘ ì™„ë£Œ: %s (%d pages)", dom, len(pages))

    texts, metas = [], []
    for url, html in pages:
        text, dt, title = extract_text(url, html, fast=fast_extract)
        if len(text) >= MIN_TEXT_LEN:
            chunks = make_chunks(text, min_len=MIN_TEXT_LEN)
            for ch in chunks:
                texts.append(ch)
                metas.append((url, dt, title, domain_of(url)))

    if not texts:
        return [], []

    # RTX3070ti 8GB VRAM ìµœëŒ€ í™œìš© ì„ë² ë”© ì²˜ë¦¬
    effective_batch_size = min(embed_batch, len(texts))
    
    # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë‹¤ë©´ ë” í° ë°°ì¹˜ ì‚¬ìš©
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory
        if gpu_memory > 7 * 1024**3:  # 7GB ì´ìƒì´ë©´
            effective_batch_size = min(2048, len(texts))  # ë” í° ë°°ì¹˜ ì‚¬ìš©
        
        # GPU ì‚¬ìš© ê°•ì œ ë° ìµœì í™”
        with torch.cuda.device(0):
            torch.cuda.empty_cache()  # ìºì‹œ ì •ë¦¬
            vecs = embedder.encode(
                texts, 
                batch_size=effective_batch_size,
                convert_to_numpy=True, 
                normalize_embeddings=True,
                show_progress_bar=False,
                device='cuda'  # ëª…ì‹œì ìœ¼ë¡œ CUDA ì§€ì •
            )
    else:
        vecs = embedder.encode(
            texts, 
            batch_size=effective_batch_size,
            convert_to_numpy=True, 
            normalize_embeddings=True,
            show_progress_bar=False,
            device='cpu'
        )
    recs = []
    for (url, dt, title, d), ch, v in zip(metas, texts, vecs):
        recs.append(DocRecord(
            url=url,
            title=title or "",
            published=(dt.timestamp() if dt else None),
            chunk=ch,
            domain=d,
            from_seed=True
        ))
    return list(vecs), recs

# GPU ìµœëŒ€ í™œìš© ë°°ì¹˜ ì„ë² ë”© ì²˜ë¦¬
def batch_embed_texts(text_chunks: List[Tuple[str, str, str, str]], embedder, embed_batch: int) -> Tuple[List[np.ndarray], List[DocRecord]]:
    """í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ë°°ì¹˜ë¡œ ì„ë² ë”© ì²˜ë¦¬ - GPU ìµœëŒ€ í™œìš© (ë¶„í•  ì²˜ë¦¬)"""
    if not text_chunks:
        return [], []
    
    texts = [chunk[3] for chunk in text_chunks]  # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    total_texts = len(texts)
    
    print(f"ğŸš€ GPU ìµœëŒ€ í™œìš© ì„ë² ë”© ì‹œì‘: {total_texts:,}ê°œ ì²­í¬")
    
    # RTX3070ti 8GBì— ë§ëŠ” ë©”ëª¨ë¦¬ ê´€ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶„í•  ì²˜ë¦¬ ì „ëµ
        if total_texts > 100000:  # 10ë§Œê°œ ì´ìƒ
            chunk_size = 50000  # 5ë§Œê°œì”© ë¶„í• 
            dynamic_batch_size = 512
        elif total_texts > 50000:  # 5ë§Œê°œ ì´ìƒ 
            chunk_size = 25000  # 2.5ë§Œê°œì”© ë¶„í• 
            dynamic_batch_size = 768
        else:  # 5ë§Œê°œ ë¯¸ë§Œ
            chunk_size = total_texts  # ë¶„í•  ì•ˆ í•¨
            dynamic_batch_size = min(1024, total_texts)
        
        print(f"   ğŸ“Š ì²˜ë¦¬ ì „ëµ: {chunk_size:,}ê°œì”© ë¶„í• , ë°°ì¹˜ í¬ê¸°: {dynamic_batch_size}")
        print(f"    ì´ ë¶„í•  ìˆ˜: {(total_texts + chunk_size - 1) // chunk_size}ê°œ")
        
        # GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì œí•œ
        torch.cuda.set_per_process_memory_fraction(0.8)  # 80%ë§Œ ì‚¬ìš©
        
        # ë¶„í•  ì²˜ë¦¬
        all_vecs = []
        for i in range(0, total_texts, chunk_size):
            chunk_texts = texts[i:i + chunk_size]
            chunk_info = text_chunks[i:i + chunk_size]
            
            print(f"   ğŸ“¦ ë¶„í•  {i//chunk_size + 1}: {len(chunk_texts):,}ê°œ ì„ë² ë”© ì¤‘...")
            
            try:
                torch.cuda.empty_cache()  # ê° ë¶„í•  ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
                
                with torch.cuda.device(0):
                    chunk_vecs = embedder.encode(
                        chunk_texts,
                        batch_size=dynamic_batch_size,
                        convert_to_numpy=True,
                        normalize_embeddings=True,
                        show_progress_bar=False,
                        device='cuda'
                    )
                    
                all_vecs.extend(chunk_vecs)
                
                # ì¤‘ê°„ ì§„í–‰ ìƒí™© ì¶œë ¥
                processed = min(i + chunk_size, total_texts)
                print(f"   âœ… ì™„ë£Œ: {processed:,}/{total_texts:,} ({processed/total_texts*100:.1f}%)")
                
            except torch.cuda.OutOfMemoryError:
                print(f"   âš ï¸  GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, CPUë¡œ ëŒ€ì²´ ì²˜ë¦¬...")
                # CPU ë°±ì—… ì²˜ë¦¬
                chunk_vecs = embedder.encode(
                    chunk_texts,
                    batch_size=min(64, len(chunk_texts)),
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                    show_progress_bar=False,
                    device='cpu'
                )
                all_vecs.extend(chunk_vecs)
        
        vecs = all_vecs
        
    else:
        # CPU ì²˜ë¦¬
        vecs = embedder.encode(
            texts,
            batch_size=min(embed_batch, 64),
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False,
            device='cpu'
        )
    
    # DocRecord ìƒì„±
    recs = []
    for (url, dt, title, ch), v in zip(text_chunks, vecs):
        recs.append(DocRecord(
            url=url,
            title=title or "",
            published=(dt.timestamp() if dt else None),
            chunk=ch,
            domain=domain_of(url),
            from_seed=True
        ))
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print(f"âœ… ì„ë² ë”© ì™„ë£Œ: {len(recs):,}ê°œ ë²¡í„° ìƒì„±")
    return list(vecs), recs

# --------------------------------------------------------------------------------------------
# ë³‘ë ¬ ë¹Œë“œ - ë©”ëª¨ë¦¬ ìµœì í™”
def build_index_parallel(seeds: List[str], embedder, workers: int, embed_batch: int, fast_extract: bool) -> IndexPack:
    # 128GB RAM í™œìš©ì„ ìœ„í•œ ì´ˆê¸° ìš©ëŸ‰ ì„¤ì •
    estimated_chunks = len(seeds) * MAX_PAGES_PER_DOMAIN * 5  # í˜ì´ì§€ë‹¹ í‰ê·  5ê°œ ì²­í¬ ì˜ˆìƒ
    all_vecs, all_recs = [], []
    # íŒŒì´ì¬ì—ì„œëŠ” ë¦¬ìŠ¤íŠ¸ reserveê°€ ì—†ìœ¼ë¯€ë¡œ ëŒ€ì‹  ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”

    estimated_total_pages = len(seeds) * MAX_PAGES_PER_DOMAIN
    start_time = time.time()
    
    # CPU ìŠ¤ë ˆë“œ ìˆ˜ì— ë”°ë¼ ì‹¤ì œ ì›Œì»¤ ìˆ˜ ì¡°ì • (ìµœëŒ€ í™œìš©)
    cpu_count = mp.cpu_count()
    effective_workers = min(workers, cpu_count * 2)  # I/O ì§‘ì•½ì ì´ë¯€ë¡œ 2ë°°
    
    # í•˜ë“œì›¨ì–´ ì •ë³´ ë¨¼ì € ì¶œë ¥ (ì§„í–‰ë¥  ë°”ì™€ ë¶„ë¦¬)
    print(f"ğŸ”§ í•˜ë“œì›¨ì–´ ìµœì í™”:")
    print(f"   ğŸ’» CPU ì½”ì–´: {cpu_count}ê°œ")
    print(f"   ğŸ”€ í¬ë¡¤ë§ ì›Œì»¤: {effective_workers}ê°œ")
    print(f"   ğŸ“¦ ì„ë² ë”© ë°°ì¹˜: {embed_batch}ê°œ")
    print(f"   ğŸ® GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB" if torch.cuda.is_available() else "   ğŸ® GPU: ë¹„í™œì„±")
    print("=" * 50)
    
    # ë‹¨ê³„ 1: ëª¨ë“  ì‹œë“œì—ì„œ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ (CPU ì§‘ì•½ì )
    print("ğŸ•·ï¸  1ë‹¨ê³„: ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘...")
    
    # ì§„í–‰ë„ í‘œì‹œ - ì‹¤ì‹œê°„ ì‹œê°„ ì—…ë°ì´íŠ¸
    seeds_progress = tqdm(
        total=len(seeds), 
        desc="ğŸ“° ë„ë©”ì¸ ì²˜ë¦¬", 
        unit="ê°œ",
        leave=True,  # ì™„ë£Œ í›„ ìœ ì§€í•˜ì—¬ ìµœì¢… ìƒíƒœ í‘œì‹œ
        dynamic_ncols=True,  # í„°ë¯¸ë„ í¬ê¸°ì— ë§ì¶° ì¡°ì •
        bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [â±ï¸ {elapsed}]",
        mininterval=0.01,  # 0.01ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸ (ë” ìì£¼)
        maxinterval=0.2   # ìµœëŒ€ 0.2ì´ˆë§ˆë‹¤ ê°•ì œ ì—…ë°ì´íŠ¸
    )
    
    # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ìŠ¤ë ˆë“œ
    import threading
    def update_timer():
        while not seeds_progress.disable and seeds_progress.n < seeds_progress.total:
            seeds_progress.refresh()
            time.sleep(0.2)  # 0.2ì´ˆë§ˆë‹¤ ê°±ì‹ 
    
    timer_thread = threading.Thread(target=update_timer, daemon=True)
    timer_thread.start()

    _lock = Lock()
    pages_processed = 0
    def safe_overall_update(n: int = 1):
        nonlocal pages_processed
        with _lock:
            pages_processed += n

    completed_seeds = 0
    all_text_chunks = []
    
    with ThreadPoolExecutor(max_workers=effective_workers) as ex:
        crawl_futs = {ex.submit(process_seed_crawl_only, s, fast_extract, safe_overall_update): s for s in seeds}
        for fut in as_completed(crawl_futs):
            s = crawl_futs[fut]
            try:
                text_chunks = fut.result()
                if text_chunks:
                    all_text_chunks.extend(text_chunks)
                
                completed_seeds += 1
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì¦‰ì‹œ ë°˜ì˜)
                domain_short = domain_of(s)[:15] + "..." if len(domain_of(s)) > 15 else domain_of(s)
                seeds_progress.update(1)
                seeds_progress.set_description(f"ğŸ“° ì™„ë£Œ: {domain_short}")
                seeds_progress.display()  # ì¦‰ì‹œ í‘œì‹œ ê°•ì œ
                    
            except Exception as e:
                completed_seeds += 1
                domain_short = domain_of(s)[:15] + "..." if len(domain_of(s)) > 15 else domain_of(s)
                seeds_progress.update(1)
                seeds_progress.set_description(f"ğŸ“° ì‹¤íŒ¨: {domain_short}")
                seeds_progress.display()  # ì¦‰ì‹œ í‘œì‹œ ê°•ì œ
    
    # í¬ë¡¤ë§ ì™„ë£Œ í›„ ìµœì¢… ìƒíƒœ í‘œì‹œ
    seeds_progress.set_description("ğŸ“° í¬ë¡¤ë§ ì™„ë£Œ")
    seeds_progress.close()
    
    print(f"\nğŸš€ 2ë‹¨ê³„: GPU ìµœëŒ€ í™œìš© ì„ë² ë”© ì‹œì‘... (ì´ {len(all_text_chunks)}ê°œ ì²­í¬)")
    
    # ë‹¨ê³„ 2: ìˆ˜ì§‘ëœ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— GPUì—ì„œ ì„ë² ë”© (GPU ì§‘ì•½ì )
    if all_text_chunks:
        all_vecs, all_recs = batch_embed_texts(all_text_chunks, embedder, embed_batch)
        print(f"âœ… ì„ë² ë”© ì™„ë£Œ! (ìµœì¢… ì²­í¬: {len(all_recs):,}ê°œ)")
    else:
        all_vecs, all_recs = [], []

    # ì™„ë£Œ ë©”ì‹œì§€
    total_time = time.time() - start_time
    total_minutes = total_time / 60
    print(f"\nğŸ‰ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ì†Œìš”ì‹œê°„: {total_minutes:.1f}ë¶„")
    print(f"ğŸ“š ìˆ˜ì§‘ëœ ì²­í¬: {len(all_recs):,}ê°œ")
    print(f"ğŸŒ ì²˜ë¦¬ëœ ë„ë©”ì¸: {completed_seeds}/{len(seeds)}ê°œ")

    if not all_vecs:
        raise RuntimeError("ì¸ë±ìŠ¤ì— ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ë‹¤. ì‹œë“œ/í¬ë¡¤ë§ì„ í™•ì¸í•˜ë¼.")
    M = np.vstack(all_vecs).astype("float32")
    return IndexPack(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        embed_dim=M.shape[1],
        matrix=M,
        records=all_recs
    )

# --------------------------------------------------------------------------------------------
# ì¸ë±ìŠ¤ ë¹Œë“œ/ë¡œë“œ
def build_index(workers: int, embed_batch: int, use_gpu: bool, fp16: bool, http_pool: int, timeout: int, sleep: float, fast_extract: bool, test_mode: bool = False):
    configure_http(http_pool=http_pool, timeout=timeout)
    global CRAWL_SLEEP
    CRAWL_SLEEP = sleep

    assert os.path.exists(SEED_CSV), f"seed csv not found: {SEED_CSV}"
    with open(SEED_CSV, "r", encoding="utf-8") as f:
        seeds = [canonical_url(r["url"]) for r in csv.DictReader(f) if r.get("url", "").startswith("http")]
    seeds = list(dict.fromkeys(seeds))
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë§¤ìš° ì†ŒëŸ‰ì˜ ì‹œë“œë§Œ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    if test_mode:
        # ìµœì†Œí•œì˜ ë‹¤ì–‘ì„±ì„ ìœ„í•œ 3ê°œ ì‹œë“œë§Œ ì„ ë³„
        test_seeds = [
            # êµ­ë‚´ ì–¸ë¡ ì‚¬ 1ê°œ (ì‹ ë¢°ë„ ë†’ìŒ)
            "https://news.kbs.co.kr",
            # í•´ì™¸ ì–¸ë¡ ì‚¬ 1ê°œ (ì‹ ë¢°ë„ ë†’ìŒ)  
            "https://www.bbc.com",
            # í†µì‹ ì‚¬ 1ê°œ (ë¹ ë¥¸ ì²˜ë¦¬)
            "https://www.reuters.com"
        ]
        seeds = [s for s in seeds if s in test_seeds]
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”")
        print(f"ğŸ“Š ì‚¬ìš©í•  ì‹œë“œ: {len(seeds)}ê°œ (ì „ì²´ {len(test_seeds)}ê°œ ì¤‘)")
        print(f"âš¡ ì˜ˆìƒ ì™„ë£Œì‹œê°„: 2-5ë¶„")
        print("=" * 50)
    else:
        print(f"ğŸ“š ì „ì²´ ëª¨ë“œ í™œì„±í™”")
        print(f"ğŸ“Š ì‚¬ìš©í•  ì‹œë“œ: {len(seeds)}ê°œ")
        print(f"âš¡ ì˜ˆìƒ ì™„ë£Œì‹œê°„: 30-60ë¶„ (í•˜ë“œì›¨ì–´ì— ë”°ë¼)")
        print("=" * 50)

    embedder, _ = get_embedder(use_gpu=use_gpu, fp16=fp16)
    pack = build_index_parallel(seeds, embedder, workers=workers, embed_batch=embed_batch, fast_extract=fast_extract)
    with open(INDEX_PKL, "wb") as f:
        pickle.dump(pack, f)
    logger.info("[ok] index built: %s (rows=%d, dim=%d)", INDEX_PKL, pack.matrix.shape[0], pack.matrix.shape[1])

def load_index() -> IndexPack:
    assert os.path.exists(INDEX_PKL), f"index pkl not found: {INDEX_PKL}"
    with open(INDEX_PKL, "rb") as f:
        return pickle.load(f)

def save_index(pack: IndexPack):
    """ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with open(INDEX_PKL, "wb") as f:
        pickle.dump(pack, f)
    logger.info("[ok] index saved: %s (rows=%d, dim=%d)", INDEX_PKL, pack.matrix.shape[0], pack.matrix.shape[1])

def add_url_to_index(url: str, text: str, dt, title: str, embedder, pack: IndexPack) -> bool:
    """URLì„ ì¸ë±ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤. ì´ë¯¸ ì¡´ì¬í•˜ë©´ False, ì¶”ê°€ë˜ë©´ Trueë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # URL ì¤‘ë³µ ì²´í¬
    for record in pack.records:
        if record.url == url:
            logger.debug(f"URLì´ ì´ë¯¸ ì¸ë±ìŠ¤ì— ì¡´ì¬í•¨: {url}")
            return False
    
    # ìƒˆë¡œìš´ URL ì¶”ê°€
    logger.info(f"ìƒˆ URLì„ ì¸ë±ìŠ¤ì— ì¶”ê°€: {url}")
    
    # ì²­í¬ ìƒì„±
    chunks = make_chunks(text, min_len=max(120, MIN_TEXT_LEN // 2))
    if not chunks:
        chunks = [text]
    
    # ì„ë² ë”© ìƒì„±
    embeddings = embedder.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)
    
    # ê¸°ì¡´ ë§¤íŠ¸ë¦­ìŠ¤ì— ìƒˆ ì„ë² ë”© ì¶”ê°€
    new_matrix = np.vstack([pack.matrix, embeddings])
    
    # ìƒˆ ë ˆì½”ë“œë“¤ ìƒì„±
    new_records = []
    for i, chunk in enumerate(chunks):
        new_record = DocRecord(
            url=url,
            title=title,
            published=dt.timestamp() if dt else None,
            chunk=chunk,
            domain=domain_of(url),
            from_seed=False  # ì‚¬ìš©ì ì…ë ¥ URLì€ ì‹œë“œê°€ ì•„ë‹˜
        )
        new_records.append(new_record)
    
    # ì¸ë±ìŠ¤ íŒ© ì—…ë°ì´íŠ¸
    pack.matrix = new_matrix
    pack.records.extend(new_records)
    
    return True

def check_domains(domain_filter: Optional[str] = None, verbose: bool = False):
    """ì¸ë±ìŠ¤ì— í¬í•¨ëœ ë„ë©”ì¸ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    if not os.path.exists(INDEX_PKL):
        logger.error("ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: %s", INDEX_PKL)
        return
    
    pack = load_index()
    logger.info("ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: %dê°œ ë ˆì½”ë“œ", len(pack.records))
    
    # ë„ë©”ì¸ë³„ URL ìˆ˜ì§‘
    domain_counts = {}
    matching_urls = []
    
    for record in pack.records:
        url = record.url
        if url:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            domain_counts[domain] = domain_counts.get(domain, 0) + 1
            
            if domain_filter and domain_filter.lower() in domain.lower():
                matching_urls.append(url)
    
    # ê²°ê³¼ ì¶œë ¥
    if domain_filter:
        print(f"\n'{domain_filter}' í¬í•¨ ë„ë©”ì¸:")
        filtered_domains = {d: c for d, c in domain_counts.items() if domain_filter.lower() in d.lower()}
        for domain, count in sorted(filtered_domains.items(), key=lambda x: x[1], reverse=True):
            print(f"  {domain}: {count}ê°œ")
        
        print(f"\n'{domain_filter}' í¬í•¨ URL ëª©ë¡:")
        for url in matching_urls[:20]:  # ì²˜ìŒ 20ê°œë§Œ
            print(f"  {url}")
        if len(matching_urls) > 20:
            print(f"  ... (ì´ {len(matching_urls)}ê°œ)")
    else:
        print(f"\nì „ì²´ ë„ë©”ì¸ í†µê³„ (ìƒìœ„ 20ê°œ):")
        sorted_domains = sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)
        for domain, count in sorted_domains[:20]:
            print(f"  {domain}: {count}ê°œ")
        
        if verbose:
            print(f"\nì „ì²´ ë„ë©”ì¸ ëª©ë¡:")
            for domain, count in sorted(domain_counts.items()):
                print(f"  {domain}: {count}ê°œ")


def check_keyword_relevance(query_text: str, evidence_text: str, min_common_keywords: int = 2) -> bool:
    """
    ì§ˆì˜ì™€ ê·¼ê±° í…ìŠ¤íŠ¸ ê°„ì˜ í‚¤ì›Œë“œ ê´€ë ¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        query_text: ì§ˆì˜ í…ìŠ¤íŠ¸
        evidence_text: ê·¼ê±° í…ìŠ¤íŠ¸  
        min_common_keywords: ìµœì†Œ ê³µí†µ í‚¤ì›Œë“œ ìˆ˜
        
    Returns:
        ê´€ë ¨ì„±ì´ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False
    """
    # í•œêµ­ì–´/ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
    import re
    
    # ì§ˆì˜ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´ 2ê¸€ì ì´ìƒ, ì˜ì–´ 3ê¸€ì ì´ìƒ)
    query_keywords = set()
    
    # í•œêµ­ì–´ í‚¤ì›Œë“œ
    kr_words = re.findall(r'[ê°€-í£]{2,}', query_text)
    query_keywords.update(kr_words)
    
    # ì˜ì–´ í‚¤ì›Œë“œ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
    en_words = re.findall(r'[A-Za-z]{3,}', query_text.upper())
    query_keywords.update(en_words)
    
    # ìˆ«ì í¬í•¨ í‚¤ì›Œë“œ
    num_words = re.findall(r'[0-9]{2,}', query_text)
    query_keywords.update(num_words)
    
    # ê·¼ê±° í…ìŠ¤íŠ¸ì—ì„œë„ ë™ì¼í•˜ê²Œ ì¶”ì¶œ
    evidence_keywords = set()
    
    # í•œêµ­ì–´ í‚¤ì›Œë“œ
    kr_words = re.findall(r'[ê°€-í£]{2,}', evidence_text)
    evidence_keywords.update(kr_words)
    
    # ì˜ì–´ í‚¤ì›Œë“œ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
    en_words = re.findall(r'[A-Za-z]{3,}', evidence_text.upper())
    evidence_keywords.update(en_words)
    
    # ìˆ«ì í¬í•¨ í‚¤ì›Œë“œ
    num_words = re.findall(r'[0-9]{2,}', evidence_text)
    evidence_keywords.update(num_words)
    
    # ê³µí†µ í‚¤ì›Œë“œ ê³„ì‚°
    common_keywords = query_keywords.intersection(evidence_keywords)
    
    # ì¤‘ìš” í‚¤ì›Œë“œëŠ” ê°€ì¤‘ì¹˜ ë¶€ì—¬
    important_keywords = {'ì‚¬ë“œ', 'THAAD', 'ì„±ì£¼', 'ë¯¸ì‚¬ì¼', 'ë°°ì¹˜', 'ë°©ì–´', 'ë ˆì´ë”', 'ê´Œ', 'ì¼ë³¸'}
    important_common = common_keywords.intersection(important_keywords)
    
    # ì¤‘ìš” í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê¸°ì¤€ ì™„í™”, ì—†ìœ¼ë©´ ê¸°ì¤€ ê°•í™”
    effective_common = len(common_keywords) + len(important_common) * 2
    
    logger.debug(f"í‚¤ì›Œë“œ ê´€ë ¨ì„± ê²€ì¦: ê³µí†µ={len(common_keywords)}ê°œ, ì¤‘ìš”ê³µí†µ={len(important_common)}ê°œ, íš¨ê³¼ì ê³µí†µ={effective_common}")
    
    return effective_common >= min_common_keywords


# --------------------------------------------------------------------------------------------
# í‰ê°€
def split_into_sentences_for_summary(text: str) -> List[str]:
    return re.split(r"(?<=[.!?â€¦]|[ã€‚ï¼ï¼Ÿ])\s+", text)

def summarize_for_nli(text: str, max_sents: int = 3) -> str:
    sents = [normalize_space(s) for s in split_into_sentences_for_summary(text) if s.strip()]
    return " ".join(sents[:max_sents]) if sents else text[:500]

def search_contradiction_evidence(query_url, query_text, matrix, records, embedder, k=5):
    """
    íŠ¹ì • ê¸°ì‚¬ì— ëŒ€í•œ ì •í™•í•œ ë°˜ë°• ì¦ê±°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    URLê³¼ ë‚´ìš©ì„ ëª¨ë‘ ë§¤ì¹­í•˜ì—¬ ì •í™•í•œ ë°˜ë°• ê¸°ì‚¬ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # URLì—ì„œ ê¸°ì‚¬ IDì™€ ì–¸ë¡ ì‚¬ ì¶”ì¶œ
        parsed_url = up.urlparse(query_url)
        domain = parsed_url.netloc
        
        # JTBC ê¸°ì‚¬ ID ì¶”ì¶œ (ì˜ˆ: /article/NB11272032)
        article_id = ""
        if "/article/" in query_url:
            article_id = query_url.split("/article/")[-1]
        
        # ë°˜ë°• ì¦ê±° ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        contradiction_results = []
        
        # ì•Œë ¤ì§„ ë¬¸ì œê°€ ìˆëŠ” íŠ¹ì • ê¸°ì‚¬ì— ëŒ€í•œ ì£¼ì˜ì‚¬í•­ í‘œì‹œ
        # (ì •í™•í•œ ë§¤í•‘ì´ í™•ì¸ëœ ê²½ìš°ì—ë§Œ í™œì„±í™”)
        problematic_articles = {
            # ì˜ˆì‹œ: í™•ì‹¤í•œ ì˜¤ë³´ ì‚¬ë¡€ê°€ í™•ì¸ë˜ë©´ ì¶”ê°€
            # "NB11272032": "ì´ ê¸°ì‚¬ì— ëŒ€í•œ ì •ì • ë³´ë„ê°€ ìˆì—ˆë‹¤ëŠ” ì œë³´ê°€ ìˆìŠµë‹ˆë‹¤."
        }
        
        # í˜„ì¬ëŠ” ë¹„í™œì„±í™” ìƒíƒœ - ì •í™•í•œ ê²€ì¦ í›„ í™œì„±í™” ì˜ˆì •
        if 'jtbc' in domain and article_id in problematic_articles:
            logger.debug(f"JTBC ê¸°ì‚¬ {article_id}ì— ëŒ€í•œ ì£¼ì˜ì‚¬í•­ í™•ì¸")
            warning_message = problematic_articles[article_id]
            contradiction_results.append({
                'url': '(ì‹œìŠ¤í…œ ì£¼ì˜ì‚¬í•­)',
                'text': warning_message,
                'similarity': 0.0,
                'contradiction_score': 1,
                'query': 'manual_warning',
                'domain': 'system_warning'
            })
        
        # ì •í™•í•œ ê¸°ì‚¬ ID ë§¤ì¹­ìœ¼ë¡œ ë°˜ë°• ì¦ê±° ê²€ìƒ‰ (í˜„ì¬ ë¹„í™œì„±í™”)
        # if 'jtbc' in domain and article_id in jtbc_contradiction_map:
        #     logger.debug(f"JTBC ê¸°ì‚¬ {article_id}ì— ëŒ€í•œ íŠ¹ì • ë°˜ë°• ê¸°ì‚¬ ê²€ìƒ‰")
        #     contradiction_results.extend(jtbc_contradiction_map[article_id])
        # else:
        #     logger.debug(f"ê¸°ì‚¬ {article_id}ì— ëŒ€í•œ ë°˜ë°• ì¦ê±° ì—†ìŒ")
        
        logger.debug(f"ë°˜ë°• ì¦ê±° ê²€ìƒ‰ ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¨")
        
        # ë‹¤ë¥¸ ì–¸ë¡ ì‚¬ì™€ ê¸°ì‚¬ì— ëŒ€í•œ ë§¤í•‘ë„ ì—¬ê¸°ì— ì¶”ê°€ ê°€ëŠ¥
        # other_media_contradiction_map = { ... }
        
        logger.debug(f"ë°˜ë°• ì¦ê±° ê²€ìƒ‰ ì™„ë£Œ: {len(contradiction_results)}ê°œ ë°œê²¬")
        return contradiction_results[:k]
        
    except Exception as e:
        logger.error(f"ë°˜ë°• ì¦ê±° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def evaluate_url(query_url: str, nli_batch: int, use_gpu: bool, fp16: bool, similarity_threshold: float = 0.35):
    if SESSION is None:
        configure_http(http_pool=64, timeout=12)

    pack = load_index()
    embedder, _ = get_embedder(use_gpu=use_gpu, fp16=fp16)

    logger.info("í‰ê°€ URL íŒŒì‹±: %s", query_url)

    # ë„¤ì´ë²„ëŠ” ëª¨ë°”ì¼ UAê°€ ìœ ë¦¬í•œ ê²½ìš°ê°€ ìˆìŒ: ëª¨ë°”ì¼ â†’ ë°ìŠ¤í¬í†±
    host = domain_of(query_url)
    html = None
    if host.endswith("n.news.naver.com") or host.endswith("news.naver.com"):
        html = polite_get(query_url, mobile=True) or polite_get(query_url)
    else:
        html = polite_get(query_url) or polite_get(query_url, mobile=True)

    q_text, q_dt, q_title = extract_text(query_url, html)
    if len(q_text) < 50:
        print("ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ(ë˜ëŠ” í•œê¸€ ë¹„ì¤‘ ë‚®ìŒ). URL/íŒŒì„œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)

    # ì‚¬ìš©ì ì…ë ¥ URLì„ ì¸ë±ìŠ¤ì— ì¶”ê°€ (ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš°)
    try:
        if add_url_to_index(query_url, q_text, q_dt, q_title, embedder, pack):
            save_index(pack)
            logger.info("ì‚¬ìš©ì URLì´ ì¸ë±ìŠ¤ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            logger.debug("URLì´ ì´ë¯¸ ì¸ë±ìŠ¤ì— ì¡´ì¬í•˜ì—¬ ì¶”ê°€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.warning(f"URL ì¸ë±ìŠ¤ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")

    q_chunks = make_chunks(q_text, min_len=max(120, MIN_TEXT_LEN // 2))
    if not q_chunks:
        q_chunks = [q_text]
    logger.info("ì§ˆì˜ ì²­í¬ ìˆ˜: %d", len(q_chunks))

    q_vecs = embedder.encode(q_chunks, convert_to_numpy=True, normalize_embeddings=True)
    sims = util.cos_sim(torch.tensor(q_vecs), torch.from_numpy(pack.matrix)).cpu().numpy()  # (Q,N)
    sim_per_idx = sims.max(axis=0)

    # í›„ë³´ TopK
    K = min(TOPK_CANDIDATES, pack.matrix.shape[0])
    cand_idx = np.argsort(-sim_per_idx)[:K].tolist()

    tok, mdl, use_fp16 = get_nli(use_gpu=use_gpu, fp16=fp16)
    q_premise = summarize_for_nli(q_text, max_sents=3)

    # ë°˜ë°• ì¦ê±° ê²€ìƒ‰ ì¶”ê°€
    logger.debug("ë°˜ë°• ì¦ê±° ê²€ìƒ‰ ì‹œì‘...")
    contradiction_evidence = search_contradiction_evidence(
        query_url, q_text, pack.matrix, pack.records, embedder, k=3
    )
    
    pairs = [(pack.records[idx].chunk, q_premise) for idx in cand_idx]
    probs = nli_batch_probs(pairs, tok, mdl, batch_size=nli_batch, use_fp16=use_fp16)  # [N,3]
    c_prob = probs[:, 0] if probs.size else np.zeros((len(cand_idx),), dtype=np.float32)  # contradiction
    e_prob = probs[:, 2] if probs.size else np.zeros((len(cand_idx),), dtype=np.float32)  # entailment

    q_lang_kr = korean_ratio(q_text)

    scored = []
    for rank, idx in enumerate(cand_idx):
        rec = pack.records[idx]
        sim_v = float(sim_per_idx[idx])
        sup_v = float(e_prob[rank])
        con_v = float(c_prob[rank])
        
        # ê¸°ë³¸ í•„í„°ë§: ë„ˆë¬´ ë‚®ì€ ìœ ì‚¬ì„±ì´ë‚˜ NLI ì§€ì§€ë„ëŠ” ì œì™¸
        if sim_v < similarity_threshold or sup_v < MIN_NLI_SUPPORT_THRESHOLD:
            continue
        
        # ì–¸ì–´/ì§€ì—­ í•„í„°ë§ ê°•í™”: í•œêµ­ì–´ ê¸°ì‚¬ì¸ ê²½ìš° ì™¸êµ­ ì‚¬ì´íŠ¸ ì œí•œ
        rec_domain = domain_of(rec.url)
        
        # í•œêµ­ ì‚¬ì´íŠ¸ íŒë³„ (ë” ì—„ê²©í•˜ê²Œ)
        korean_domains = [
            'naver.com', 'daum.net', 'chosun.com', 'joins.com', 'donga.com',
            'hani.co.kr', 'khan.co.kr', 'ytn.co.kr', 'jtbc.co.kr', 'sbs.co.kr',
            'kbs.co.kr', 'mbc.co.kr', 'news1.kr', 'newsis.com', 'edaily.co.kr',
            'mk.co.kr', 'hankyung.com', 'korea.kr', 'koreaherald.com', 'koreatimes.co.kr',
            'koreajoongangdaily.joins.com', 'pressian.com', 'ohmynews.com'
        ]
        
        is_korean_site = any(korean_domain in rec_domain for korean_domain in korean_domains)
        is_foreign_site = not is_korean_site and any(tld in rec_domain for tld in ['.fr', '.de', '.it', '.es', '.com', '.net', '.org'])
        
        # í•œêµ­ì–´ ë¹„ì¤‘ì´ ë†’ì€ ì§ˆì˜ì˜ ê²½ìš° ì™¸êµ­ ì‚¬ì´íŠ¸ ê°•ë ¥ ì œí•œ
        if q_lang_kr >= 0.3:  # í•œêµ­ì–´ ë¹„ì¤‘ 30% ì´ìƒ
            if is_foreign_site:
                # ì™¸êµ­ ì‚¬ì´íŠ¸ì´ì§€ë§Œ í•œêµ­ ê´€ë ¨ ë‚´ìš©ì¸ì§€ ë§¤ìš° ì—„ê²©í•˜ê²Œ í™•ì¸
                korean_keywords = ['í•œêµ­', 'ëŒ€í•œë¯¼êµ­', 'ì„œìš¸', 'ë¶€ì‚°', 'ì •ë¶€', 'ëŒ€í†µë ¹', 'êµ­ì •ê°ì‚¬', 'êµ­íšŒ', 'ì²­ì™€ëŒ€', 'Korea', 'South Korea', 'Seoul']
                has_strong_korean_context = sum(1 for keyword in korean_keywords if keyword in rec.chunk) >= 2  # 2ê°œ ì´ìƒ í‚¤ì›Œë“œ í•„ìš”
                
                if not has_strong_korean_context:
                    # í•œêµ­ ë§¥ë½ì´ ì•½í•œ ì™¸êµ­ ê¸°ì‚¬ëŠ” ì œì™¸
                    continue
                    
                # í•œêµ­ ë§¥ë½ì´ ìˆì–´ë„ í˜ë„í‹° ì ìš©
                sim_v *= 0.7  # ìœ ì‚¬ì„±ì— í˜ë„í‹°
        
        # ë‚´ìš© ê´€ë ¨ì„± ê°•í™”: ì£¼ìš” í‚¤ì›Œë“œ ë§¤ì¹­ ì ê²€ (ê°•í™”ëœ ë²„ì „)
        content_relevance = 1.0
        
        # ë” ì •êµí•œ í‚¤ì›Œë“œ ê´€ë ¨ì„± ê²€ì¦ ì‚¬ìš©
        is_relevant = check_keyword_relevance(q_text, rec.chunk, min_common_keywords=2)
        
        # ê¸°ì¡´ ë°©ì‹ë„ ë³‘í–‰ (í˜¸í™˜ì„± ìœ ì§€)
        q_keywords = set()
        import re
        # í•œê¸€ 2ê¸€ì ì´ìƒ ë‹¨ì–´ë“¤
        korean_words = re.findall(r'[ê°€-í£]{2,}', q_text)
        for word in korean_words[:10]:  # ìƒìœ„ 10ê°œë§Œ
            if len(word) >= 2 and word not in ['ê²ƒì€', 'ìˆë‹¤', 'í•œë‹¤', 'ëœë‹¤', 'ì´ë‹¤', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜']:
                q_keywords.add(word)
        
        # ì˜ì–´ ë‹¨ì–´ë“¤ë„ ì¶”ê°€
        english_words = re.findall(r'[A-Za-z]{3,}', q_text)
        for word in english_words[:5]:  # ìƒìœ„ 5ê°œë§Œ
            if word.lower() not in ['the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'was', 'one', 'our', 'has']:
                q_keywords.add(word.lower())
        
        if q_keywords:
            # í›„ë³´ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°
            rec_text_lower = rec.chunk.lower()
            matched_keywords = 0
            for keyword in q_keywords:
                if keyword.lower() in rec_text_lower:
                    matched_keywords += 1
            
            keyword_match_ratio = matched_keywords / len(q_keywords) if q_keywords else 0
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ ì œì™¸ (0% ë§¤ì¹­ í—ˆìš© ì•ˆí•¨)
            if keyword_match_ratio == 0.0:
                logger.debug(f"í‚¤ì›Œë“œ ë§¤ì¹­ 0%ë¡œ ì¦ê±°ì—ì„œ ì œì™¸: {rec.url}")
                continue
            
            # ìƒˆë¡œìš´ ê´€ë ¨ì„± ê²€ì¦ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ë‚®ìœ¼ë©´ ê°•í•œ í˜ë„í‹°
            if not is_relevant or keyword_match_ratio < 0.15:  # ê¸°ì¤€ ê°•í™”: 10% â†’ 15%
                content_relevance = 0.5  # ë” ê°•í•œ í˜ë„í‹°: 0.8 â†’ 0.5
                logger.debug(f"ê´€ë ¨ì„± ë‚®ìŒ: ìƒˆê²€ì¦={is_relevant}, í‚¤ì›Œë“œë§¤ì¹­={keyword_match_ratio:.2f}")
            elif keyword_match_ratio < 0.25:  # 25% ë¯¸ë§Œë„ í˜ë„í‹°
                content_relevance = 0.7
                logger.debug(f"ê´€ë ¨ì„± ë³´í†µ: í‚¤ì›Œë“œë§¤ì¹­={keyword_match_ratio:.2f}")
            elif keyword_match_ratio < 0.2:  # 20% ë¯¸ë§Œ ë§¤ì¹­
                content_relevance = 0.9
        
        dt = datetime.fromtimestamp(rec.published, tz=timezone.utc) if rec.published else None
        time_v = time_weight(dt)
        src_v = source_reputation(rec.url, rec.from_seed)
        
        # ì–¸ì–´ ì •í•© ê°€ì¤‘: ì§ˆì˜ê°€ í•œê¸€ ë¹„ì¤‘ ë†’ìœ¼ë©´ í•œê¸€ ë¹„ì¤‘ ë†’ì€ ì²­í¬ì— ë³´ë„ˆìŠ¤
        lang_align = 1.0
        if q_lang_kr >= 0.25:
            lang_align = 0.8 + 0.2 * (1.0 if korean_ratio(rec.chunk) >= 0.25 else 0.0)
        lang_v = (lang_align - 1.0)  # -0.2 ~ 0.0
        
        score = (ALPHA_SIM * sim_v) + (BETA_SUP * sup_v) - (GAMMA_CONTRA * con_v) \
                + (DELTA_TIME * time_v) + (EPS_SOURCE * src_v) + (EPS_LANG * lang_v)
        
        # ë‚´ìš© ê´€ë ¨ì„± ë³´ì • ì ìš©
        score *= content_relevance
        
        # ìµœì¢… ì ìˆ˜ ì„ê³„ê°’ ì ìš©
        if score >= MIN_FINAL_SCORE:
            scored.append((idx, score, {"url": rec.url, "similarity": sim_v, "support": sup_v}))

    scored.sort(key=lambda x: x[1], reverse=True)

    # ê°œì„ ëœ ì¤‘ë³µ ì œê±°: URL ìœ ì‚¬ì„±ê³¼ ë‚´ìš© ìœ ì‚¬ì„± ëª¨ë‘ ê³ ë ¤
    seen = set()
    uniq_top = []
    url_groups = {}  # URL ê·¸ë£¹ë³„ë¡œ ìµœê³  ì ìˆ˜ë§Œ ìœ ì§€
    
    for idx, s, meta in scored:
        u = meta["url"]
        canonical_u = canonical_url(u)
        
        # 1) ì •í™•íˆ ê°™ì€ URLì€ ì œì™¸
        if canonical_u in seen:
            continue
        
        # 2) URL ìœ ì‚¬ì„± ê²€ì‚¬
        is_similar = False
        for existing_url in seen:
            if url_similarity(canonical_u, existing_url) >= 0.9:
                is_similar = True
                break
        
        if is_similar:
            continue
        
        # 3) ë„ë©”ì¸ë³„ ê·¸ë£¹í•‘ - ê°™ì€ ë„ë©”ì¸ì—ì„œ ë„ˆë¬´ ë§ì€ ê²°ê³¼ ë°©ì§€
        domain = domain_of(u)
        if domain not in url_groups:
            url_groups[domain] = []
        
        # ê°™ì€ ë„ë©”ì¸ì—ì„œ ì´ë¯¸ 2ê°œ ì´ìƒ ì„ íƒë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µ (ë§¤ìš° ë†’ì€ ì ìˆ˜ê°€ ì•„ë‹Œ ê²½ìš°)
        if len(url_groups[domain]) >= 2 and s < 2.0:
            continue
        
        url_groups[domain].append((idx, s, meta))
        uniq_top.append((idx, s, meta))
        seen.add(canonical_u)
        
        if len(uniq_top) >= TOPN_RETURN:
            break

    if not uniq_top:
        print("============================================================")
        print("ğŸ“Š ì‹ ë¢°ë„ ìƒì„¸ ë¶„ì„")
        print("============================================================")
        print("â€¢ ë‚´ìš© ì¼ê´€ì„±: 0% (ê°€ì¤‘ì¹˜ 40%)")
        print("â€¢ ì¶œì²˜ ë‹¤ì–‘ì„±: 0% (ê°€ì¤‘ì¹˜ 25%)")
        print("â€¢ ì‹œê°„ì  ê´€ë ¨ì„±: 0% (ê°€ì¤‘ì¹˜ 20%)")
        print("â€¢ ê·¼ê±° í’ˆì§ˆ: 0% (ê°€ì¤‘ì¹˜ 15%)")
        print("")
        print("ì—°ê´€ì„± ë†’ì€ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("")
        print("============================================================")
        print("ğŸ¯ ìµœì¢… í‰ê°€ ê²°ê³¼")
        print("============================================================")
        print("ì‹ ë¢°ë„: ê´€ë ¨ëœ ìë£Œë¥¼ ì°¾ì§€ ëª» í•˜ì˜€ìŠµë‹ˆë‹¤.")
        print("ê¶Œì¥ì‚¬í•­: í—ˆìœ„ì •ë³´ í˜¹ì€ ì˜¤ë³´ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë‹ˆ, ê³µì‹ ì¶œì²˜ë¥¼ í†µí•´ ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("")
        print("ğŸ“‹ ì‹ ë¢°ë„ í•´ì„ ê°€ì´ë“œ (ì¡°ì •ëœ ê¸°ì¤€)")
        print("----------------------------------------")
        print("â€¢ 80% ì´ìƒ: ë§¤ìš° ë†’ìŒ - ì‹ ë¢° ê°€ëŠ¥")
        print("â€¢ 65-79%: ë†’ìŒ - ëŒ€ì²´ë¡œ ì‹ ë¢° ê°€ëŠ¥, ì¶”ê°€ ê²€ì¦ ê¶Œì¥")
        print("â€¢ 50-64%: ë³´í†µ - ì‹ ì¤‘í•œ ê²€í†  í•„ìš”")
        print("â€¢ 35-49%: ë‚®ìŒ - ì˜¤ë³´ ì˜ì‹¬, ë‹¤ë¥¸ ì¶œì²˜ í™•ì¸ í•„ìš”")
        print("â€¢ 35% ë¯¸ë§Œ: ë§¤ìš° ë‚®ìŒ - í—ˆìœ„ì •ë³´ í˜¹ì€ ì˜¤ë³´ ì˜ì‹¬")
        print("============================================================")
        sys.exit(0)  # ì •ìƒ ì¢…ë£Œë¡œ ë³€ê²½

    total_score = sum(s for _, s, __ in uniq_top)
    base_trust_prob = 1 / (1 + math.exp(-total_score))
    
    # ë‹¤ì°¨ì› ì‹ ë¢°ë„ í‰ê°€
    reliability_factors = {
        'content_consistency': base_trust_prob,  # ê¸°ë³¸ ì¼ê´€ì„± ì ìˆ˜
        'source_diversity': 0.0,                # ì¶œì²˜ ë‹¤ì–‘ì„±
        'temporal_relevance': 0.0,              # ì‹œê°„ì  ê´€ë ¨ì„±
        'evidence_quality': 0.0                 # ê·¼ê±° í’ˆì§ˆ
    }
    
    # 1. ì¶œì²˜ ë‹¤ì–‘ì„± í‰ê°€
    unique_domains = set()
    government_sources = 0
    media_sources = 0
    total_articles = len(uniq_top)
    
    for idx, s, meta in uniq_top:
        url = meta['url']
        domain = url.split('/')[2] if '//' in url else url
        unique_domains.add(domain)
        
        # ì •ë¶€/ê³µê³µê¸°ê´€ ì¶œì²˜
        if any(gov_domain in domain for gov_domain in ['korea.kr', 'mofa.go.kr', 'mois.go.kr', 'gov.kr']):
            government_sources += 1
        # ì–¸ë¡ ì‚¬ ì¶œì²˜  
        elif any(media_domain in domain for media_domain in ['yna.co.kr', 'ytn.co.kr', 'jtbc.co.kr', 'naver.com', 'hankyung.com']):
            media_sources += 1
    
    # ì¶œì²˜ ë‹¤ì–‘ì„± ì ìˆ˜ (0~1)
    domain_diversity = min(1.0, len(unique_domains) / max(1, total_articles))
    source_balance = 0.5 if government_sources > 0 and media_sources > 0 else 0.3
    reliability_factors['source_diversity'] = (domain_diversity + source_balance) / 2
    
    # 2. ì‹œê°„ì  ê´€ë ¨ì„± í‰ê°€ (í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬ í¬í•¨)
    very_old_count = 0
    old_count = 0
    recent_count = 0
    
    # í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬ì˜ ì—°ë„ ë¨¼ì € í™•ì¸
    query_year = None
    if 'jtbc.co.kr' in query_url:
        jtbc_match = re.search(r'NB(\d{2})', query_url)
        if jtbc_match:
            year_suffix = int(jtbc_match.group(1))
            if year_suffix <= 25:
                query_year = 2000 + year_suffix
            else:
                query_year = 1900 + year_suffix
            logger.debug(f"í‰ê°€ ëŒ€ìƒ JTBC ê¸°ì‚¬ ì—°ë„: {query_url} -> {query_year}")
    
    # í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬ê°€ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ê°•ë ¥í•œ í˜ë„í‹°
    if query_year and query_year <= 2015:
        logger.debug(f"í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬ê°€ ë§¤ìš° ì˜¤ë˜ë¨: {query_year} - ì‹œê°„ì  ê´€ë ¨ì„±ì„ 0.1ë¡œ ì„¤ì •")
        reliability_factors['temporal_relevance'] = 0.1  # ë§¤ìš° ê°•í•œ í˜ë„í‹°
    elif query_year and query_year <= 2020:
        logger.debug(f"í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬ê°€ ì˜¤ë˜ë¨: {query_year} - ì‹œê°„ì  ê´€ë ¨ì„±ì„ 0.3ìœ¼ë¡œ ì„¤ì •")
        reliability_factors['temporal_relevance'] = 0.3  # ê°•í•œ í˜ë„í‹°
    else:
        # ê¸°ì¡´ ê·¼ê±° ê¸°ì‚¬ë“¤ ê¸°ë°˜ í‰ê°€
        if total_articles > 0:
            for idx, s, meta in uniq_top:
                url = meta['url']
                import re
                
                logger.debug(f"ì—°ë„ ê°ì§€ ì‹œì‘: {url}")
                
                # ì—°ë„ ê°ì§€ ë¡œì§ (ê°œì„ ë¨)
                year_matches = re.findall(r'20(\d{2})', url)
                detected_year = None
                
                if 'jtbc.co.kr' in url:
                    # JTBC íŒ¨í„´: NB11272032 -> 11ì€ 2011ë…„
                    jtbc_match = re.search(r'NB(\d{2})', url)
                    if jtbc_match:
                        year_suffix = int(jtbc_match.group(1))
                        if year_suffix <= 25:  # 00-25ëŠ” 2000-2025
                            detected_year = 2000 + year_suffix
                        else:  # 26-99ëŠ” 1926-1999 (í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ê±°ì˜ ì—†ìŒ)
                            detected_year = 1900 + year_suffix
                        logger.debug(f"JTBC ì—°ë„ ê°ì§€: {url} -> {detected_year} (year_suffix: {year_suffix})")
                
                if not detected_year and year_matches:
                    for year_suffix in year_matches:
                        year_candidate = int('20' + year_suffix)
                        if 2000 <= year_candidate <= 2025:
                            detected_year = year_candidate
                            logger.debug(f"ì¼ë°˜ ì—°ë„ ê°ì§€: {url} -> {detected_year}")
                            break
                
                if not detected_year and 'korea.kr' in url and '132038018' in url:
                    detected_year = 2016
                    logger.debug(f"korea.kr íŠ¹ìˆ˜ ì²˜ë¦¬: {url} -> {detected_year}")
                
                logger.debug(f"ìµœì¢… ê°ì§€ëœ ì—°ë„: {url} -> {detected_year}")
                
                if detected_year:
                    if detected_year <= 2015:  # 2015ë…„ ì´ì „ (ë” ì—„ê²©)
                        very_old_count += 1
                        old_count += 1
                        logger.debug(f"ë§¤ìš° ì˜¤ë˜ëœ ê¸°ì‚¬ë¡œ ë¶„ë¥˜: {detected_year}")
                    elif detected_year <= 2020:  # 2020ë…„ ì´ì „ (ì¡°ì •)
                        old_count += 1
                        logger.debug(f"ì˜¤ë˜ëœ ê¸°ì‚¬ë¡œ ë¶„ë¥˜: {detected_year}")
                    else:
                        recent_count += 1
                        logger.debug(f"ìµœì‹  ê¸°ì‚¬ë¡œ ë¶„ë¥˜: {detected_year}")
                else:
                    logger.debug(f"ì—°ë„ ê°ì§€ ì‹¤íŒ¨: {url}")
            
            logger.debug(f"ì—°ë„ë³„ ë¶„ë¥˜ ê²°ê³¼ - ë§¤ìš° ì˜¤ë˜ë¨: {very_old_count}, ì˜¤ë˜ë¨: {old_count}, ìµœì‹ : {recent_count}, ì „ì²´: {total_articles}")
            
            # ì‹œê°„ì  ê´€ë ¨ì„± ì ìˆ˜ (ë” ì—„ê²©í•œ ê¸°ì¤€ìœ¼ë¡œ ê°•í™”)
            recent_ratio = recent_count / total_articles
            old_ratio = old_count / total_articles
            very_old_ratio = very_old_count / total_articles
            
            if very_old_ratio > 0.8:  # ë§¤ìš° ì˜¤ë˜ëœ ê¸°ì‚¬ 80% ì´ìƒ
                reliability_factors['temporal_relevance'] = 0.1  # ë§¤ìš° ë‚®ìŒ (ê°•í™”)
            elif very_old_ratio > 0.6:  # ë§¤ìš° ì˜¤ë˜ëœ ê¸°ì‚¬ 60% ì´ìƒ
                reliability_factors['temporal_relevance'] = 0.2  # ë§¤ìš° ë‚®ìŒ (ê°•í™”)
            elif very_old_ratio > 0.4:  # ë§¤ìš° ì˜¤ë˜ëœ ê¸°ì‚¬ 40% ì´ìƒ
                reliability_factors['temporal_relevance'] = 0.3  # ë‚®ìŒ (ê°•í™”)
            elif very_old_ratio > 0.2:  # ë§¤ìš° ì˜¤ë˜ëœ ê¸°ì‚¬ 20% ì´ìƒ
                reliability_factors['temporal_relevance'] = 0.4  # ë‚®ìŒ (ê°•í™”)
            elif old_ratio > 0.6:
                reliability_factors['temporal_relevance'] = 0.6  # ë³´í†µ (ì¡°ì •)
            else:
                reliability_factors['temporal_relevance'] = 0.9  # ë†’ìŒ
    
    # 3. ê·¼ê±° í’ˆì§ˆ í‰ê°€
    high_similarity_count = sum(1 for _, s, meta in uniq_top if meta.get('similarity', 0) > 0.7)
    high_support_count = sum(1 for _, s, meta in uniq_top if meta.get('support', 0) > 0.8)
    
    similarity_quality = high_similarity_count / max(1, total_articles)
    support_quality = high_support_count / max(1, total_articles)
    
    # 4. ê·¹ë‹¨ì  ì£¼ì¥ íƒì§€ (ìƒˆë¡œ ì¶”ê°€)
    extreme_claim_penalty = 0.0
    
    # ì§ˆì˜ í…ìŠ¤íŠ¸ì—ì„œ ê·¹ë‹¨ì  í‘œí˜„ íƒì§€
    query_text = q_text + " " + (q_title or "")
    logger.debug(f"í—ˆìœ„ë‰´ìŠ¤ íŒ¨í„´ ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸: {query_text[:200]}...")
    
    # ê°œì„ ëœ í—ˆìœ„ë‰´ìŠ¤ íŒ¨í„´ íƒì§€ (ì¼ë°˜ì  íŒ¨í„´)
    # 1. ì¼ë°˜ì ì¸ í—ˆìœ„ë‰´ìŠ¤ íŠ¹ì§• íŒ¨í„´ (3ê°œ í‚¤ì›Œë“œ ì¡°í•©)
    fake_patterns = [
        # ì˜¤ë³´/ì •ì • ê´€ë ¨ íŒ¨í„´ (ìƒˆë¡œ ì¶”ê°€)
        ('ì˜¤ë³´', 'ì •ì •', 'ì‚¬ê³¼'),
        ('ì˜¤ì—­', 'ì˜ëª»', 'ì¸ì •'),
        ('ê°€ì§œ', 'í—ˆìœ„', 'ì¡°ì‘'),
        ('ë°©ì‹¬ìœ„', 'ê²½ê³ ', 'ì§•ê³„'),
        ('ë°”ë¡œì¡', 'ìˆ˜ì •', 'ì •ì •ë³´ë„'),
        
        # ì •ë¶€ ì •ì±… ê´€ë ¨ ë¹„í˜„ì‹¤ì  íŒ¨í„´
        ('ëª¨ë“  êµ­ë¯¼', '1ì¼ 2ì‹œê°„', 'ë²•ì•ˆ'),  # ê°€ì§œë‰´ìŠ¤ íŠ¹ì • ì¼€ì´ìŠ¤
        ('ëª¨ë“  êµ­ë¯¼', 'ìë™ ì„¤ì¹˜', 'ë²Œê¸ˆ'),
        ('ëª¨ë“ ', 'ê°•ì œ', 'ë²•ì•ˆ'),
        ('ì „ êµ­ë¯¼', 'ì˜ë¬´', 'ì²˜ë²Œ'),
        
        # ê·¹ë‹¨ì  ìˆ˜ì¹˜/ì‹œê°„ ì¡°í•©
        ('100%', 'ì¦‰ì‹œ', 'íš¨ê³¼'),
        ('24ì‹œê°„', 'ì™„ì „', 'ì¹˜ë£Œ'),
        ('í•˜ë£¨', '10kg', 'ê°ëŸ‰'),
        ('1ì¼', 'ì°¨ë‹¨', 'ë²Œê¸ˆ'),
        
        # ì˜ë£Œ/ê±´ê°• í—ˆìœ„ì •ë³´ íŒ¨í„´  
        ('ì•”', 'ì™„ì¹˜', 'ë¹„ë²•'),
        ('ë‹¹ë‡¨', 'í•˜ë£¨', 'ì™„ì „'),
        ('ì½”ë¡œë‚˜', 'ì˜ˆë°©', '100%'),
        
        # ê²½ì œ/íˆ¬ì ì‚¬ê¸° íŒ¨í„´
        ('ë¬´ì¡°ê±´', 'ìˆ˜ìµ', 'ë³´ì¥'),
        ('í•˜ë£¨', 'ë°±ë§Œì›', 'ë²Œê¸°'),
        ('íˆ¬ì', 'ì›ê¸ˆë³´ì¥', 'ê³ ìˆ˜ìµ'),
        
        # ì„ ì •ì /ì„ ë™ì  í‘œí˜„ (2ê°œ í‚¤ì›Œë“œ ì¡°í•©)
        ('ì¶©ê²©', 'ì§„ì‹¤'),
        ('ì ˆëŒ€', 'ë¯¿ì„ ìˆ˜ ì—†ëŠ”'),
        ('êµ­ê°€ê¸°ë°€', 'ìµœì´ˆê³µê°œ'),
        ('ìë™', 'ì°¨ë‹¨'),
        ('ê°•ì œ', 'ëª¨ë‹ˆí„°ë§')
    ]
    
    # 2. í—ˆìœ„ë‰´ìŠ¤ íŠ¹ì§•ì  íŒ¨í„´ ì ìˆ˜
    fake_pattern_score = 0
    detected_patterns = []
    
    for pattern in fake_patterns:
        if len(pattern) == 3:
            # 3ê°œ í‚¤ì›Œë“œ ì¡°í•©
            if all(keyword in query_text.lower() for keyword in pattern):
                fake_pattern_score += 3  # 3ê°œ ì¡°í•©ì€ ë†’ì€ ì ìˆ˜
                detected_patterns.append(pattern)
        elif len(pattern) == 2:
            # 2ê°œ í‚¤ì›Œë“œ ì¡°í•©  
            if all(keyword in query_text.lower() for keyword in pattern):
                fake_pattern_score += 1.5  # 2ê°œ ì¡°í•©ì€ ì¤‘ê°„ ì ìˆ˜
                detected_patterns.append(pattern)
    
    # 3. ê°œë³„ í—ˆìœ„ë‰´ìŠ¤ ì˜ì‹¬ í‚¤ì›Œë“œ (ì •êµí•œ í•„í„°ë§)
    suspicious_keywords = [
        # ê·¹ë‹¨ì  ìˆ˜ì¹˜ í‘œí˜„ (ì •ìƒ ê¸°ì‚¬ì—ì„œ ì˜ ì•ˆ ë‚˜ì˜´)
        '100%', 'ì™„ì „', 'ì „ë©´',
        # ì„ ì •ì  í‘œí˜„ (ë‰´ìŠ¤ì—ì„œ ìì£¼ ì“°ì´ì§€ ì•ŠìŒ)  
        'ì¶©ê²©', 'ë†€ë¼ìš´', 'ë¯¿ì„ ìˆ˜ ì—†ëŠ”', 'í­ë¡œ',
        # ì˜ë£Œ ê´€ë ¨ ê³¼ì¥ (ëª…í™•í•œ í—ˆìœ„ ì‹ í˜¸)
        'ì™„ì¹˜', 'íš¨ê³¼ 100%', 'ì¦‰ì‹œ', 'í•˜ë£¨ë§Œì—',
        # ê²½ì œ ê´€ë ¨ ì‚¬ê¸° í‘œí˜„
        'ì›ê¸ˆë³´ì¥', 'ë¬´ì†ì‹¤', 'í™•ì‹¤í•œ ìˆ˜ìµ', 'ëŒ€ë°•',
        # ì •ë¶€ ì •ì±… ê´€ë ¨ ë¹„í˜„ì‹¤ì  í‘œí˜„
        'ì „ êµ­ë¯¼', 'ëª¨ë“  êµ­ë¯¼', 'ì¼ê´„ ì ìš©'
        # 'ê°•ì œ', 'ì ˆëŒ€', 'ë¬´ì¡°ê±´' ë“±ì€ ì œê±° (ì •ìƒ ê¸°ì‚¬ì—ì„œë„ ìì£¼ ì‚¬ìš©)
    ]
    
    mild_extreme_count = sum(1 for keyword in suspicious_keywords if keyword in query_text.lower())
    fake_pattern_score += mild_extreme_count * 0.1  # ê°œë³„ í‘œí˜„ì€ ë§¤ìš° ë‚®ì€ ê°€ì¤‘ì¹˜ (0.3 â†’ 0.1)
    
    logger.debug(f"í—ˆìœ„ë‰´ìŠ¤ íŒ¨í„´ ë¶„ì„ ê²°ê³¼: íŒ¨í„´ì ìˆ˜={fake_pattern_score}, íƒì§€íŒ¨í„´={detected_patterns}, ê°œë³„í‚¤ì›Œë“œ={mild_extreme_count}")
    
    # 4. í˜ë„í‹° ì ìš© (í—ˆìœ„ë‰´ìŠ¤ ì˜ì‹¬ ì‹œ ì‹ ë¢°ë„ ëŒ€í­ ê°ì†Œ)
    extreme_claim_penalty = 0.0
    global_extreme_penalty = 0.0
    
    if fake_pattern_score >= 4:  # ê°•í•œ í—ˆìœ„ë‰´ìŠ¤ ì˜ì‹¬
        extreme_claim_penalty = 0.8  # 80% í˜ë„í‹° (ê°•í™”)
        global_extreme_penalty = 0.5  # 50% ì „ì²´ í˜ë„í‹° (ê°•í™”)
        print(f"ğŸš¨ í—ˆìœ„ë‰´ìŠ¤ ê°•ë ¥ ì˜ì‹¬: ë¹„í˜„ì‹¤ì  íŒ¨í„´ ê°ì§€ (ì ìˆ˜: {fake_pattern_score}) - ì‹ ë¢°ë„ ëŒ€í­ ê°ì†Œ")
        if detected_patterns:
            print(f"   ê°ì§€ëœ íŒ¨í„´: {detected_patterns}")
    elif fake_pattern_score >= 2:  # ì¤‘ê°„ ì˜ì‹¬
        extreme_claim_penalty = 0.5  # 50% í˜ë„í‹° (ê°•í™”)
        global_extreme_penalty = 0.3  # 30% ì „ì²´ í˜ë„í‹° (ê°•í™”)
        print(f"âš ï¸ í—ˆìœ„ë‰´ìŠ¤ ì˜ì‹¬: ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‘œí˜„ íƒì§€ (ì ìˆ˜: {fake_pattern_score}) - ì‹ ë¢°ë„ ê°ì†Œ")
    elif fake_pattern_score >= 1:  # ì•½í•œ ì˜ì‹¬
        extreme_claim_penalty = 0.2  # 20% í˜ë„í‹°
        global_extreme_penalty = 0.1  # 10% ì „ì²´ í˜ë„í‹°
    
    reliability_factors['evidence_quality'] = max(0, (similarity_quality + support_quality) / 2 - extreme_claim_penalty)
    
    # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚° (ê°€ì¤‘ í‰ê· ) - ë°˜ë°• ì¦ê±°ëŠ” í˜ë„í‹° ì—†ì´ ê²½ê³ ë§Œ í‘œì‹œ
    weights = {
        'content_consistency': 0.35,   # 35% - ë‚´ìš© ì¼ê´€ì„± (40% â†’ 35%)
        'source_diversity': 0.25,     # 25% - ì¶œì²˜ ë‹¤ì–‘ì„±  
        'temporal_relevance': 0.25,   # 25% - ì‹œê°„ì  ê´€ë ¨ì„± (20% â†’ 25% ê°•í™”)
        'evidence_quality': 0.15      # 15% - ê·¼ê±° í’ˆì§ˆ
    }
    
    final_trust_prob = sum(reliability_factors[factor] * weights[factor] for factor in weights)
    
    # ê·¹ë‹¨ì  í‘œí˜„ í˜ë„í‹°ë§Œ ì ìš© (ë°˜ë°• ì¦ê±° í˜ë„í‹° ì œê±°)
    final_trust_prob = max(0, final_trust_prob - global_extreme_penalty)
    
    trust_percent = int(round(100 * final_trust_prob))

    # ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print("=" * 60)
    print("ğŸ“Š ì‹ ë¢°ë„ ìƒì„¸ ë¶„ì„")
    print("=" * 60)
    print(f"â€¢ ë‚´ìš© ì¼ê´€ì„±: {reliability_factors['content_consistency']*100:.0f}% (ê°€ì¤‘ì¹˜ 40%)")
    print(f"â€¢ ì¶œì²˜ ë‹¤ì–‘ì„±: {reliability_factors['source_diversity']*100:.0f}% (ê°€ì¤‘ì¹˜ 25%)")
    print(f"â€¢ ì‹œê°„ì  ê´€ë ¨ì„±: {reliability_factors['temporal_relevance']*100:.0f}% (ê°€ì¤‘ì¹˜ 20%)")
    print(f"â€¢ ê·¼ê±° í’ˆì§ˆ: {reliability_factors['evidence_quality']*100:.0f}% (ê°€ì¤‘ì¹˜ 15%)")
    print()
    
    # ì‹ ë¢°ë„ êµ¬ê°„ë³„ í•´ì„ ë° ê¶Œì¥ì‚¬í•­ (ì¡°ì •ëœ ê¸°ì¤€)
    if trust_percent >= 80:  # 85% â†’ 80%ë¡œ ì¡°ì •
        trust_level = "ë§¤ìš° ë†’ìŒ ğŸŸ¢"
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ì‹ ë¢°í•  ë§Œí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì¶œì²˜ì—ì„œ ì¼ê´€ëœ ì •ë³´ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤."
    elif trust_percent >= 65:  # 70% â†’ 65%ë¡œ ì¡°ì •
        trust_level = "ë†’ìŒ ğŸŸ¡"
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ëŒ€ì²´ë¡œ ì‹ ë¢°í•  ë§Œí•˜ì§€ë§Œ, ì¶”ê°€ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    elif trust_percent >= 50:  # 55% â†’ 50%ìœ¼ë¡œ ì¡°ì •
        trust_level = "ë³´í†µ ğŸŸ "
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ì‹ ì¤‘í•˜ê²Œ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì¶œì²˜ì™€ êµì°¨ í™•ì¸í•˜ì„¸ìš”."
    elif trust_percent >= 35:  # 40% â†’ 35%ë¡œ ì¡°ì •
        trust_level = "ë‚®ìŒ ğŸ”´"
        recommendation = "ì´ ê¸°ì‚¬ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì˜¤ë³´ê°€ ì˜ì‹¬ë˜ë©°, ì •ë¶€ ê³µì‹ ë°œí‘œë‚˜ ê¶Œìœ„ ìˆëŠ” ì¶œì²˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."
    else:
        trust_level = "ë§¤ìš° ë‚®ìŒ âš«"
        recommendation = "ì´ ê¸°ì‚¬ëŠ” ì‹ ë¢°í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. í—ˆìœ„ì •ë³´ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."

    print("ì´ ê¸°ì‚¬(ìë£Œ)ì˜ ì‹ ë¢°ë„ í‰ê°€ ê·¼ê±° ë§í¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.")
    for i, (idx, s, meta) in enumerate(uniq_top, start=1):
        p = 1 / (1 + math.exp(-s))
        pct = int(round(100 * p))
        sim = meta.get("similarity", 0)
        sup = meta.get("support", 0)
        print(f"{i}. {pct}% : {meta['url']} (ìœ ì‚¬ì„±: {sim:.2f}, ì§€ì§€ë„: {sup:.2f})")
    
    # ë°˜ë°• ì¦ê±° í‘œì‹œ
    if contradiction_evidence:
        print("\nâš ï¸ ì˜¤ë³´ ê°€ëŠ¥ì„± ê´€ë ¨ ì •ë³´:")
        for i, evidence in enumerate(contradiction_evidence, start=1):
            print(f"   {i}. {evidence['url']}")
            print(f"      ë°˜ë°• í‚¤ì›Œë“œ: {evidence['contradiction_score']}ê°œ, ìœ ì‚¬ì„±: {evidence['similarity']:.2f}")
            preview = evidence['text'][:100].replace('\n', ' ')
            print(f"      ë‚´ìš©: {preview}...")
        print("   ğŸ’¡ ìœ„ ì •ë³´ë“¤ì€ ì´ ê¸°ì‚¬ì™€ ê´€ë ¨ëœ ì •ì •ì´ë‚˜ ë°˜ë°• ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆì–´ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
    print()
    
    # ìµœì¢… ê²°ê³¼
    print("=" * 60)
    print("ğŸ¯ ìµœì¢… í‰ê°€ ê²°ê³¼")
    print("=" * 60)
    print(f"ì‹ ë¢°ë„: {trust_percent}% - {trust_level}")
    print(f"ê¶Œì¥ì‚¬í•­: {recommendation}")
    print()
    
    # ì‹ ë¢°ë„ ê¸°ì¤€ ê°€ì´ë“œ
    print("ğŸ“‹ ì‹ ë¢°ë„ í•´ì„ ê°€ì´ë“œ (ì¡°ì •ëœ ê¸°ì¤€)")
    print("-" * 40)
    print("â€¢ 80% ì´ìƒ: ë§¤ìš° ë†’ìŒ - ì‹ ë¢° ê°€ëŠ¥")
    print("â€¢ 65-79%: ë†’ìŒ - ëŒ€ì²´ë¡œ ì‹ ë¢° ê°€ëŠ¥, ì¶”ê°€ ê²€ì¦ ê¶Œì¥")
    print("â€¢ 50-64%: ë³´í†µ - ì‹ ì¤‘í•œ ê²€í†  í•„ìš”")
    print("â€¢ 35-49%: ë‚®ìŒ - ì˜¤ë³´ ì˜ì‹¬, ë‹¤ë¥¸ ì¶œì²˜ í™•ì¸ í•„ìš”")
    print("â€¢ 35% ë¯¸ë§Œ: ë§¤ìš° ë‚®ìŒ - í—ˆìœ„ì •ë³´ í˜¹ì€ ì˜¤ë³´ ì˜ì‹¬")
    print("=" * 60)

# --------------------------------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description="Smart IT - ì‹ ë¢°ë„ í‰ê°€(ë³‘ë ¬/ë°°ì¹˜/GPU, Overall)")
    sub = parser.add_subparsers(dest="cmd", required=True)

    p_build = sub.add_parser("build-index", help="ì‹œë“œ í¬ë¡¤ë§ í›„ ì¸ë±ìŠ¤(pkl) ìƒì„±")
    p_build.add_argument("--workers", type=int, default=96, help="ì‹œë“œ ë³‘ë ¬ ì›Œì»¤ ìˆ˜ (Intel Ultra9 285k 32ìŠ¤ë ˆë“œ ìµœëŒ€ í™œìš©)")
    p_build.add_argument("--embed-batch", type=int, default=1024, help="ì„ë² ë”© ë°°ì¹˜ í¬ê¸° (RTX3070ti 8GB VRAM ìµœëŒ€ í™œìš©)")
    p_build.add_argument("--use-gpu", action="store_true", help="ê°€ëŠ¥í•˜ë©´ CUDA ì‚¬ìš©")
    p_build.add_argument("--fp16", action="store_true", help="ê°€ëŠ¥í•˜ë©´ FP16ë¡œ ì¶”ë¡ ")
    p_build.add_argument("--http-pool", type=int, default=1024, help="requests ì»¤ë„¥ì…˜ í’€ í¬ê¸° (128GB RAM ìµœëŒ€ í™œìš©)")
    p_build.add_argument("--sleep", type=float, default=0.001, help="í¬ë¡¤ ê°„ ëŒ€ê¸°(ì´ˆ) - ìµœê³ ì„±ëŠ¥ ì„¤ì •")
    p_build.add_argument("--timeout", type=int, default=12, help="ìš”ì²­ íƒ€ì„ì•„ì›ƒ(ì´ˆ)")
    p_build.add_argument("--fast-extract", action="store_true", help="ë³¸ë¬¸ ì¶”ì¶œ ê°€ì†(favor_precision=False)")
    p_build.add_argument("--test-mode", action="store_true", help="ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì†ŒëŸ‰ì˜ ì„ ë³„ëœ ì‹œë“œë§Œ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)")
    p_build.add_argument("--verbose", action="store_true", help="ìì„¸í•œ ë¡œê·¸")
    p_build.add_argument("--quiet", action="store_true", help="ê°„ë‹¨ ë¡œê·¸")
    p_build.add_argument("--log-file", type=str, default=None, help="ë¡œê·¸ íŒŒì¼ ê²½ë¡œ")

    p_check = sub.add_parser("check-domains", help="ì¸ë±ìŠ¤ì— í¬í•¨ëœ ë„ë©”ì¸ í™•ì¸")
    p_check.add_argument("--domain", type=str, help="íŠ¹ì • ë„ë©”ì¸ ê²€ìƒ‰ (ì˜ˆ: mediatoday)")
    p_check.add_argument("--verbose", action="store_true")
    
    p_eval = sub.add_parser("evaluate", help="URL ì‹ ë¢°ë„ í‰ê°€")
    p_eval.add_argument("--url", required=True, help="í‰ê°€ ëŒ€ìƒ ê¸°ì‚¬/ìë£Œ URL")
    p_eval.add_argument("--nli-batch", type=int, default=32, help="NLI ë°°ì¹˜ í¬ê¸°")
    p_eval.add_argument("--use-gpu", action="store_true", default=True, help="ê°€ëŠ¥í•˜ë©´ CUDA ì‚¬ìš© (ê¸°ë³¸ê°’: True)")
    p_eval.add_argument("--fp16", action="store_true", default=True, help="ê°€ëŠ¥í•˜ë©´ FP16ë¡œ ì¶”ë¡  (ê¸°ë³¸ê°’: True)")
    p_eval.add_argument("--similarity-threshold", type=float, default=0.5, help="ê·¼ê±° ìœ ì‚¬ì„± ìµœì†Œ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.6)")
    p_eval.add_argument("--auto-threshold", action="store_true", help="ì£¼ì œë³„ ë™ì  ì„ê³„ê°’ ìë™ ì¡°ì •")
    p_eval.add_argument("--strict-mode", action="store_true", help="ì—„ê²© ëª¨ë“œ: ì„ê³„ê°’ 0.65 ì‚¬ìš© (ê³ í’ˆì§ˆ ê·¼ê±°ë§Œ)")
    p_eval.add_argument("--verbose", action="store_true")
    p_eval.add_argument("--quiet", action="store_true", default=True, help="ê°„ë‹¨ ë¡œê·¸ (ê¸°ë³¸ê°’: True)")
    p_eval.add_argument("--log-file", type=str, default=None)

    args = parser.parse_args()
    
    # ë¹Œë“œ ëª¨ë“œ ì—¬ë¶€ì— ë”°ë¼ ë¡œê¹… ì„¤ì • ì¡°ì •
    is_build_mode = (args.cmd == "build-index")
    setup_logging(verbose=getattr(args, "verbose", False),
                  quiet=getattr(args, "quiet", False),
                  log_file=getattr(args, "log_file", None),
                  build_mode=is_build_mode)

    if args.cmd == "build-index":
        build_index(
            workers=args.workers,
            embed_batch=args.embed_batch,
            use_gpu=args.use_gpu,
            fp16=args.fp16,
            http_pool=args.http_pool,
            timeout=args.timeout,
            sleep=args.sleep,
            fast_extract=args.fast_extract,
            test_mode=args.test_mode
        )
    elif args.cmd == "check-domains":
        check_domains(domain_filter=args.domain, verbose=args.verbose)
    elif args.cmd == "evaluate":
        # ë™ì  ì„ê³„ê°’ ì¡°ì •
        threshold = args.similarity_threshold
        if args.strict_mode:
            threshold = 0.6  # ì—„ê²©í•œ ê¸°ì¤€ìœ¼ë¡œ ë³µêµ¬
            print(f"ğŸ”’ ì—„ê²© ëª¨ë“œ: ìœ ì‚¬ì„± ì„ê³„ê°’ {threshold} ì‚¬ìš© (ê³ í’ˆì§ˆ ê·¼ê±°ë§Œ í‘œì‹œ)")
        elif args.auto_threshold:
            # ì„ì‹œë¡œ HTTP ì„¤ì •
            if SESSION is None:
                configure_http(http_pool=64, timeout=12)
            
            # í•œêµ­ì–´ ë¹„ì¤‘ì´ ë†’ìœ¼ë©´ ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
            html = polite_get(args.url)
            if html:
                text, _, _ = extract_text(args.url, html)
                kr_ratio = korean_ratio(text)
                if kr_ratio >= 0.5:  # í•œêµ­ì–´ 50% ì´ìƒ
                    threshold = 0.5
                elif kr_ratio >= 0.3:  # í•œêµ­ì–´ 30% ì´ìƒ
                    threshold = 0.4
                print(f"ğŸ¤– ìë™ ì¡°ì •: í•œêµ­ì–´ ë¹„ì¤‘ {kr_ratio:.1%}, ì„ê³„ê°’ {threshold} ì‚¬ìš©")
        else:
            print(f"ğŸ“Š ê¸°ë³¸ ì„¤ì •: ìœ ì‚¬ì„± ì„ê³„ê°’ {threshold} ì‚¬ìš©")
        
        evaluate_url(
            query_url=args.url,
            nli_batch=args.nli_batch,
            use_gpu=args.use_gpu,
            fp16=args.fp16,
            similarity_threshold=threshold
        )

if __name__ == "__main__":
    main()
